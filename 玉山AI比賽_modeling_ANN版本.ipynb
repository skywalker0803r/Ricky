{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "玉山AI比賽_modeling_ANN版本.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skywalker0803r/Ricky/blob/master/%E7%8E%89%E5%B1%B1AI%E6%AF%94%E8%B3%BD_modeling_ANN%E7%89%88%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr44CknfvyTj",
        "colab_type": "code",
        "outputId": "b27ae800-151a-4727-d337-f742f087ce5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import warnings \n",
        "warnings.simplefilter('ignore')\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbAV7ns3v8LW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "中文map = {'bacno':'歸戶帳號','txkey':'交易序號','locdt':'授權日期','loctm':'授權時間','cano':'交易卡號',\n",
        "         'contp':'交易類別','etymd':'交易型態','mchno':'特店代號','acqic':'收單行代碼','mcc':'MCC_CODE',\n",
        "         'conam':'交易金額-台幣(經過轉換)','ecfg':'網路交易註記','insfg':'分期交易註記','iterm':'分期期數',\n",
        "         'stocn':'消費地國別','scity':'消費城市','stscd':'狀態碼','ovrlt':'超額註記碼','flbmk':'Fallback註記',\n",
        "         'hcefg':'支付型態','csmcu':'消費地幣別','flg_3dsmk':'3DS交易註記','fraud_ind':'盜刷註記'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJs7NefWwDKv",
        "colab_type": "text"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Baq7Qt00wBmh",
        "colab_type": "code",
        "outputId": "c46134bb-b28d-40d7-ae13-699bdb658fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"/content/drive/My Drive/玉山人工智慧比賽數據/train_特徵工程完.csv\",index_col=0)\n",
        "print(train.shape)\n",
        "train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1521787, 122)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stscd</th>\n",
              "      <th>ecfg</th>\n",
              "      <th>stocn</th>\n",
              "      <th>acqic</th>\n",
              "      <th>etymd</th>\n",
              "      <th>loctm</th>\n",
              "      <th>scity</th>\n",
              "      <th>hcefg</th>\n",
              "      <th>contp</th>\n",
              "      <th>conam</th>\n",
              "      <th>insfg</th>\n",
              "      <th>ovrlt</th>\n",
              "      <th>flg_3dsmk</th>\n",
              "      <th>iterm</th>\n",
              "      <th>csmcu</th>\n",
              "      <th>mchno</th>\n",
              "      <th>cano</th>\n",
              "      <th>mcc</th>\n",
              "      <th>flbmk</th>\n",
              "      <th>bacno</th>\n",
              "      <th>mchno_stscd_2_norm_count</th>\n",
              "      <th>mchno_stscd_0_norm_count</th>\n",
              "      <th>acqic_stscd_2_norm_count</th>\n",
              "      <th>acqic_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_2_norm_count</th>\n",
              "      <th>cano_stscd_0_norm_count</th>\n",
              "      <th>acqic_scity_nunique</th>\n",
              "      <th>acqic_csmcu_nunique</th>\n",
              "      <th>bacno_stscd_2_norm_count</th>\n",
              "      <th>bacno_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_nunique</th>\n",
              "      <th>acqic_etymd_8_norm_count</th>\n",
              "      <th>mchno_etymd_8_norm_count</th>\n",
              "      <th>acqic_stocn_nunique</th>\n",
              "      <th>acqic_hcefg_nunique</th>\n",
              "      <th>bacno_stscd_nunique</th>\n",
              "      <th>acqic_ovrlt_0_norm_count</th>\n",
              "      <th>acqic_ovrlt_1_norm_count</th>\n",
              "      <th>acqic_etymd_5_norm_count</th>\n",
              "      <th>acqic_bacno_nunique</th>\n",
              "      <th>...</th>\n",
              "      <th>acqic_iterm_3_norm_count</th>\n",
              "      <th>acqic_hcefg_7_norm_count</th>\n",
              "      <th>mchno_ovrlt_nunique</th>\n",
              "      <th>mchno_etymd_nunique</th>\n",
              "      <th>acqic_hcefg_8_norm_count</th>\n",
              "      <th>bacno_scity_nunique</th>\n",
              "      <th>mchno_stocn_nunique</th>\n",
              "      <th>acqic_hcefg_1_norm_count</th>\n",
              "      <th>mchno_etymd_7_norm_count</th>\n",
              "      <th>acqic_hcefg_5_norm_count</th>\n",
              "      <th>bacno_etymd_4_norm_count</th>\n",
              "      <th>cano_mchno_nunique</th>\n",
              "      <th>acqic_etymd_10_norm_count</th>\n",
              "      <th>acqic_etymd_7_norm_count</th>\n",
              "      <th>mchno_etymd_0_norm_count</th>\n",
              "      <th>cano_etymd_0_norm_count</th>\n",
              "      <th>acqic_etymd_0_norm_count</th>\n",
              "      <th>cano_etymd_2_norm_count</th>\n",
              "      <th>mchno_iterm_nunique</th>\n",
              "      <th>mchno_hcefg_5_norm_count</th>\n",
              "      <th>cano_iterm_nunique</th>\n",
              "      <th>acqic_hcefg_2_norm_count</th>\n",
              "      <th>mchno_hcefg_1_norm_count</th>\n",
              "      <th>mchno_contp_6_norm_count</th>\n",
              "      <th>cano_scity_nunique</th>\n",
              "      <th>cano_contp_nunique</th>\n",
              "      <th>mchno_contp_2_norm_count</th>\n",
              "      <th>cano_contp_2_norm_count</th>\n",
              "      <th>mchno_contp_5_norm_count</th>\n",
              "      <th>cano_hcefg_nunique</th>\n",
              "      <th>bacno_etymd_0_norm_count</th>\n",
              "      <th>cano_mcc_nunique</th>\n",
              "      <th>acqic_hcefg_0_norm_count</th>\n",
              "      <th>bacno_acqic_nunique</th>\n",
              "      <th>mchno_iterm_0_norm_count</th>\n",
              "      <th>mchno_hcefg_0_norm_count</th>\n",
              "      <th>cano_hcefg_5_norm_count</th>\n",
              "      <th>cano_etymd_6_norm_count</th>\n",
              "      <th>txkey</th>\n",
              "      <th>fraud_ind</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>61954</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59034</td>\n",
              "      <td>37846</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>112785</td>\n",
              "      <td>0.001044</td>\n",
              "      <td>0.998956</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>33</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.212500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.362500</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989928</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.212500</td>\n",
              "      <td>21</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>18</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>516056</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>38216</td>\n",
              "      <td>5795</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>13693</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>45476</td>\n",
              "      <td>451</td>\n",
              "      <td>0</td>\n",
              "      <td>133951</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.997536</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.994768</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.991720</td>\n",
              "      <td>0.008280</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>19252</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0.003276</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.042083</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006682</td>\n",
              "      <td>0.391304</td>\n",
              "      <td>11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024906</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.008377</td>\n",
              "      <td>0.434783</td>\n",
              "      <td>1</td>\n",
              "      <td>0.034542</td>\n",
              "      <td>2</td>\n",
              "      <td>0.013267</td>\n",
              "      <td>0.039878</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.968283</td>\n",
              "      <td>2</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>8</td>\n",
              "      <td>0.931187</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.911000</td>\n",
              "      <td>0.652174</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4376</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>54640</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59034</td>\n",
              "      <td>187354</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>15350</td>\n",
              "      <td>0.001044</td>\n",
              "      <td>0.998956</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989928</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>7</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.0</td>\n",
              "      <td>483434</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6697</td>\n",
              "      <td>5</td>\n",
              "      <td>62128</td>\n",
              "      <td>3267</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>40413</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>62</td>\n",
              "      <td>50185</td>\n",
              "      <td>29812</td>\n",
              "      <td>247</td>\n",
              "      <td>0</td>\n",
              "      <td>156492</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.999643</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>101</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.142619</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.988238</td>\n",
              "      <td>0.011762</td>\n",
              "      <td>0.312315</td>\n",
              "      <td>74622</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002037</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0.003765</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.024410</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.965999</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.245614</td>\n",
              "      <td>5</td>\n",
              "      <td>0.984111</td>\n",
              "      <td>2</td>\n",
              "      <td>0.001563</td>\n",
              "      <td>0.012910</td>\n",
              "      <td>0.010924</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035088</td>\n",
              "      <td>0.989076</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>8</td>\n",
              "      <td>0.861966</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1407164</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>5959</td>\n",
              "      <td>4</td>\n",
              "      <td>65231</td>\n",
              "      <td>5795</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>25962</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>62</td>\n",
              "      <td>93290</td>\n",
              "      <td>80881</td>\n",
              "      <td>263</td>\n",
              "      <td>0</td>\n",
              "      <td>105534</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.999894</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.077908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.990016</td>\n",
              "      <td>0.009984</td>\n",
              "      <td>0.206081</td>\n",
              "      <td>41913</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009496</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.982903</td>\n",
              "      <td>0.089744</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365079</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079365</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000707</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.079365</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.984127</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1051004</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 122 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   stscd  ecfg  stocn  ...  cano_etymd_6_norm_count    txkey  fraud_ind\n",
              "0      0     0    102  ...                      0.0   516056          0\n",
              "1      0     0    102  ...                      0.0     4376          0\n",
              "2      0     0    102  ...                      0.0   483434          0\n",
              "3      0     0    102  ...                      0.0  1407164          0\n",
              "4      0     0    102  ...                      0.0  1051004          0\n",
              "\n",
              "[5 rows x 122 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmQm34KPwGu_",
        "colab_type": "text"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTm0HGGuwGCm",
        "colab_type": "code",
        "outputId": "3e94413e-3c74-4b2f-832e-c6af05619849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "test = pd.read_csv(\"/content/drive/My Drive/玉山人工智慧比賽數據/test_特徵工程完.csv\",index_col=0)\n",
        "test_txkey = test[\"txkey\"]\n",
        "print(test.shape)\n",
        "test.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(421665, 121)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stscd</th>\n",
              "      <th>ecfg</th>\n",
              "      <th>stocn</th>\n",
              "      <th>acqic</th>\n",
              "      <th>etymd</th>\n",
              "      <th>loctm</th>\n",
              "      <th>scity</th>\n",
              "      <th>hcefg</th>\n",
              "      <th>contp</th>\n",
              "      <th>conam</th>\n",
              "      <th>insfg</th>\n",
              "      <th>ovrlt</th>\n",
              "      <th>flg_3dsmk</th>\n",
              "      <th>iterm</th>\n",
              "      <th>csmcu</th>\n",
              "      <th>mchno</th>\n",
              "      <th>cano</th>\n",
              "      <th>mcc</th>\n",
              "      <th>flbmk</th>\n",
              "      <th>bacno</th>\n",
              "      <th>mchno_stscd_2_norm_count</th>\n",
              "      <th>mchno_stscd_0_norm_count</th>\n",
              "      <th>acqic_stscd_2_norm_count</th>\n",
              "      <th>acqic_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_2_norm_count</th>\n",
              "      <th>cano_stscd_0_norm_count</th>\n",
              "      <th>acqic_scity_nunique</th>\n",
              "      <th>acqic_csmcu_nunique</th>\n",
              "      <th>bacno_stscd_2_norm_count</th>\n",
              "      <th>bacno_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_nunique</th>\n",
              "      <th>acqic_etymd_8_norm_count</th>\n",
              "      <th>mchno_etymd_8_norm_count</th>\n",
              "      <th>acqic_stocn_nunique</th>\n",
              "      <th>acqic_hcefg_nunique</th>\n",
              "      <th>bacno_stscd_nunique</th>\n",
              "      <th>acqic_ovrlt_0_norm_count</th>\n",
              "      <th>acqic_ovrlt_1_norm_count</th>\n",
              "      <th>acqic_etymd_5_norm_count</th>\n",
              "      <th>acqic_bacno_nunique</th>\n",
              "      <th>...</th>\n",
              "      <th>cano_etymd_4_norm_count</th>\n",
              "      <th>acqic_iterm_3_norm_count</th>\n",
              "      <th>acqic_hcefg_7_norm_count</th>\n",
              "      <th>mchno_ovrlt_nunique</th>\n",
              "      <th>mchno_etymd_nunique</th>\n",
              "      <th>acqic_hcefg_8_norm_count</th>\n",
              "      <th>bacno_scity_nunique</th>\n",
              "      <th>mchno_stocn_nunique</th>\n",
              "      <th>acqic_hcefg_1_norm_count</th>\n",
              "      <th>mchno_etymd_7_norm_count</th>\n",
              "      <th>acqic_hcefg_5_norm_count</th>\n",
              "      <th>bacno_etymd_4_norm_count</th>\n",
              "      <th>cano_mchno_nunique</th>\n",
              "      <th>acqic_etymd_10_norm_count</th>\n",
              "      <th>acqic_etymd_7_norm_count</th>\n",
              "      <th>mchno_etymd_0_norm_count</th>\n",
              "      <th>cano_etymd_0_norm_count</th>\n",
              "      <th>acqic_etymd_0_norm_count</th>\n",
              "      <th>cano_etymd_2_norm_count</th>\n",
              "      <th>mchno_iterm_nunique</th>\n",
              "      <th>mchno_hcefg_5_norm_count</th>\n",
              "      <th>cano_iterm_nunique</th>\n",
              "      <th>acqic_hcefg_2_norm_count</th>\n",
              "      <th>mchno_hcefg_1_norm_count</th>\n",
              "      <th>mchno_contp_6_norm_count</th>\n",
              "      <th>cano_scity_nunique</th>\n",
              "      <th>cano_contp_nunique</th>\n",
              "      <th>mchno_contp_2_norm_count</th>\n",
              "      <th>cano_contp_2_norm_count</th>\n",
              "      <th>mchno_contp_5_norm_count</th>\n",
              "      <th>cano_hcefg_nunique</th>\n",
              "      <th>bacno_etymd_0_norm_count</th>\n",
              "      <th>cano_mcc_nunique</th>\n",
              "      <th>acqic_hcefg_0_norm_count</th>\n",
              "      <th>bacno_acqic_nunique</th>\n",
              "      <th>mchno_iterm_0_norm_count</th>\n",
              "      <th>mchno_hcefg_0_norm_count</th>\n",
              "      <th>cano_hcefg_5_norm_count</th>\n",
              "      <th>cano_etymd_6_norm_count</th>\n",
              "      <th>txkey</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1521787</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>77950</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>592489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521788</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>79549</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>592452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521789</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>60355</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>590212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521790</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>60296</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>590209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521791</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>77933</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>592488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 121 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         stscd  ecfg  ...  cano_etymd_6_norm_count   txkey\n",
              "1521787      0     0  ...                      0.0  592489\n",
              "1521788      0     0  ...                      0.0  592452\n",
              "1521789      0     0  ...                      0.0  590212\n",
              "1521790      0     0  ...                      0.0  590209\n",
              "1521791      0     0  ...                      0.0  592488\n",
              "\n",
              "[5 rows x 121 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq9VQAwtwMPr",
        "colab_type": "text"
      },
      "source": [
        "# 定義 features & num_features & target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTyemZPdwJR2",
        "colab_type": "code",
        "outputId": "74239462-8aaf-4b56-efe5-9a14e10b33dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# 全部特徵\n",
        "features = train.drop(['fraud_ind', # just target\n",
        "                       'txkey', # just like index\n",
        "                       ],axis=1).columns.tolist()\n",
        "\n",
        "# 新特徵才是num_features\n",
        "num_features = sorted(list(set(features)^set(中文map.keys())))\n",
        "num_features.remove('fraud_ind')\n",
        "num_features.remove('locdt')\n",
        "num_features.remove('txkey')\n",
        "\n",
        "#只用num_features\n",
        "features = num_features\n",
        "\n",
        "y_name = 'fraud_ind'\n",
        "\n",
        "print(len(features),features)\n",
        "print(len(num_features),num_features)\n",
        "print(len([y_name]),[y_name])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 ['acqic_bacno_nunique', 'acqic_cano_nunique', 'acqic_contp_2_norm_count', 'acqic_contp_5_norm_count', 'acqic_contp_6_norm_count', 'acqic_contp_nunique', 'acqic_csmcu_nunique', 'acqic_etymd_0_norm_count', 'acqic_etymd_10_norm_count', 'acqic_etymd_2_norm_count', 'acqic_etymd_4_norm_count', 'acqic_etymd_5_norm_count', 'acqic_etymd_6_norm_count', 'acqic_etymd_7_norm_count', 'acqic_etymd_8_norm_count', 'acqic_etymd_nunique', 'acqic_hcefg_0_norm_count', 'acqic_hcefg_1_norm_count', 'acqic_hcefg_2_norm_count', 'acqic_hcefg_5_norm_count', 'acqic_hcefg_7_norm_count', 'acqic_hcefg_8_norm_count', 'acqic_hcefg_9_norm_count', 'acqic_hcefg_nunique', 'acqic_iterm_0_norm_count', 'acqic_iterm_1_norm_count', 'acqic_iterm_2_norm_count', 'acqic_iterm_3_norm_count', 'acqic_iterm_4_norm_count', 'acqic_iterm_5_norm_count', 'acqic_iterm_6_norm_count', 'acqic_iterm_7_norm_count', 'acqic_iterm_8_norm_count', 'acqic_iterm_nunique', 'acqic_mcc_nunique', 'acqic_mchno_nunique', 'acqic_ovrlt_0_norm_count', 'acqic_ovrlt_1_norm_count', 'acqic_scity_nunique', 'acqic_stocn_nunique', 'acqic_stscd_0_norm_count', 'acqic_stscd_2_norm_count', 'acqic_stscd_nunique', 'bacno_acqic_nunique', 'bacno_cano_nunique', 'bacno_csmcu_nunique', 'bacno_etymd_0_norm_count', 'bacno_etymd_4_norm_count', 'bacno_etymd_5_norm_count', 'bacno_etymd_8_norm_count', 'bacno_scity_nunique', 'bacno_stocn_nunique', 'bacno_stscd_0_norm_count', 'bacno_stscd_2_norm_count', 'bacno_stscd_nunique', 'cano_contp_2_norm_count', 'cano_contp_nunique', 'cano_csmcu_nunique', 'cano_etymd_0_norm_count', 'cano_etymd_2_norm_count', 'cano_etymd_4_norm_count', 'cano_etymd_5_norm_count', 'cano_etymd_6_norm_count', 'cano_etymd_8_norm_count', 'cano_etymd_nunique', 'cano_hcefg_5_norm_count', 'cano_hcefg_nunique', 'cano_iterm_nunique', 'cano_mcc_nunique', 'cano_mchno_nunique', 'cano_scity_nunique', 'cano_stocn_nunique', 'cano_stscd_0_norm_count', 'cano_stscd_2_norm_count', 'cano_stscd_nunique', 'mchno_contp_2_norm_count', 'mchno_contp_5_norm_count', 'mchno_contp_6_norm_count', 'mchno_contp_nunique', 'mchno_csmcu_nunique', 'mchno_etymd_0_norm_count', 'mchno_etymd_2_norm_count', 'mchno_etymd_4_norm_count', 'mchno_etymd_5_norm_count', 'mchno_etymd_7_norm_count', 'mchno_etymd_8_norm_count', 'mchno_etymd_nunique', 'mchno_hcefg_0_norm_count', 'mchno_hcefg_1_norm_count', 'mchno_hcefg_5_norm_count', 'mchno_hcefg_nunique', 'mchno_iterm_0_norm_count', 'mchno_iterm_nunique', 'mchno_ovrlt_0_norm_count', 'mchno_ovrlt_1_norm_count', 'mchno_ovrlt_nunique', 'mchno_stocn_nunique', 'mchno_stscd_0_norm_count', 'mchno_stscd_2_norm_count', 'mchno_stscd_nunique']\n",
            "100 ['acqic_bacno_nunique', 'acqic_cano_nunique', 'acqic_contp_2_norm_count', 'acqic_contp_5_norm_count', 'acqic_contp_6_norm_count', 'acqic_contp_nunique', 'acqic_csmcu_nunique', 'acqic_etymd_0_norm_count', 'acqic_etymd_10_norm_count', 'acqic_etymd_2_norm_count', 'acqic_etymd_4_norm_count', 'acqic_etymd_5_norm_count', 'acqic_etymd_6_norm_count', 'acqic_etymd_7_norm_count', 'acqic_etymd_8_norm_count', 'acqic_etymd_nunique', 'acqic_hcefg_0_norm_count', 'acqic_hcefg_1_norm_count', 'acqic_hcefg_2_norm_count', 'acqic_hcefg_5_norm_count', 'acqic_hcefg_7_norm_count', 'acqic_hcefg_8_norm_count', 'acqic_hcefg_9_norm_count', 'acqic_hcefg_nunique', 'acqic_iterm_0_norm_count', 'acqic_iterm_1_norm_count', 'acqic_iterm_2_norm_count', 'acqic_iterm_3_norm_count', 'acqic_iterm_4_norm_count', 'acqic_iterm_5_norm_count', 'acqic_iterm_6_norm_count', 'acqic_iterm_7_norm_count', 'acqic_iterm_8_norm_count', 'acqic_iterm_nunique', 'acqic_mcc_nunique', 'acqic_mchno_nunique', 'acqic_ovrlt_0_norm_count', 'acqic_ovrlt_1_norm_count', 'acqic_scity_nunique', 'acqic_stocn_nunique', 'acqic_stscd_0_norm_count', 'acqic_stscd_2_norm_count', 'acqic_stscd_nunique', 'bacno_acqic_nunique', 'bacno_cano_nunique', 'bacno_csmcu_nunique', 'bacno_etymd_0_norm_count', 'bacno_etymd_4_norm_count', 'bacno_etymd_5_norm_count', 'bacno_etymd_8_norm_count', 'bacno_scity_nunique', 'bacno_stocn_nunique', 'bacno_stscd_0_norm_count', 'bacno_stscd_2_norm_count', 'bacno_stscd_nunique', 'cano_contp_2_norm_count', 'cano_contp_nunique', 'cano_csmcu_nunique', 'cano_etymd_0_norm_count', 'cano_etymd_2_norm_count', 'cano_etymd_4_norm_count', 'cano_etymd_5_norm_count', 'cano_etymd_6_norm_count', 'cano_etymd_8_norm_count', 'cano_etymd_nunique', 'cano_hcefg_5_norm_count', 'cano_hcefg_nunique', 'cano_iterm_nunique', 'cano_mcc_nunique', 'cano_mchno_nunique', 'cano_scity_nunique', 'cano_stocn_nunique', 'cano_stscd_0_norm_count', 'cano_stscd_2_norm_count', 'cano_stscd_nunique', 'mchno_contp_2_norm_count', 'mchno_contp_5_norm_count', 'mchno_contp_6_norm_count', 'mchno_contp_nunique', 'mchno_csmcu_nunique', 'mchno_etymd_0_norm_count', 'mchno_etymd_2_norm_count', 'mchno_etymd_4_norm_count', 'mchno_etymd_5_norm_count', 'mchno_etymd_7_norm_count', 'mchno_etymd_8_norm_count', 'mchno_etymd_nunique', 'mchno_hcefg_0_norm_count', 'mchno_hcefg_1_norm_count', 'mchno_hcefg_5_norm_count', 'mchno_hcefg_nunique', 'mchno_iterm_0_norm_count', 'mchno_iterm_nunique', 'mchno_ovrlt_0_norm_count', 'mchno_ovrlt_1_norm_count', 'mchno_ovrlt_nunique', 'mchno_stocn_nunique', 'mchno_stscd_0_norm_count', 'mchno_stscd_2_norm_count', 'mchno_stscd_nunique']\n",
            "1 ['fraud_ind']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3dJla0JWdSj",
        "colab_type": "text"
      },
      "source": [
        "這裡的思路是解決GDBT認BANCO或CANO導致RECALL低的問題,這裡的特徵只用num_features,也就排除了認卡問題,而是去辨識交易行為"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrAlrJU3xqpd",
        "colab_type": "text"
      },
      "source": [
        "# both / test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtKH9MW-wwS7",
        "colab_type": "code",
        "outputId": "6d5e8090-3db3-4550-dbe8-b430d7af9f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "f = {}\n",
        "for col in features:\n",
        "  both_value = set(train[col].unique())&set(test[col].unique())\n",
        "  f[col] = len(both_value)/len(test[col].unique())\n",
        "res = pd.DataFrame(f,index=['both/test']).T.sort_values(by='both/test',ascending=False)\n",
        "res.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>both/test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bacno_stscd_2_norm_count</th>\n",
              "      <td>0.878689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bacno_stscd_0_norm_count</th>\n",
              "      <td>0.873786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cano_stscd_0_norm_count</th>\n",
              "      <td>0.851240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bacno_cano_nunique</th>\n",
              "      <td>0.846154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cano_stscd_2_norm_count</th>\n",
              "      <td>0.843882</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          both/test\n",
              "bacno_stscd_2_norm_count   0.878689\n",
              "bacno_stscd_0_norm_count   0.873786\n",
              "cano_stscd_0_norm_count    0.851240\n",
              "bacno_cano_nunique         0.846154\n",
              "cano_stscd_2_norm_count    0.843882"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abpoOjRCx7jw",
        "colab_type": "text"
      },
      "source": [
        "# 切分 train 跟 val_1 , val_2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtSwUARSxu24",
        "colab_type": "code",
        "outputId": "b510fc78-9fff-4391-9254-eba1d471ecbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train ,val = train_test_split(train[features+[y_name]] ,test_size=0.2 ,random_state=42)\n",
        "val_1 ,val_2 = train_test_split(val[features+[y_name]] ,test_size=0.5 ,random_state=42)\n",
        "print(train.shape)\n",
        "print(val_1.shape)\n",
        "print(val_2.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1217429, 101)\n",
            "(152179, 101)\n",
            "(152179, 101)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDhYtoWFeSV1",
        "colab_type": "text"
      },
      "source": [
        "# 計算樣本權重"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ9uYMOKeSfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "y_org = train[y_name].append(val[y_name])\n",
        "\n",
        "class_weight_dict = dict(zip([0,1],\n",
        "                             compute_class_weight(class_weight ='balanced',\n",
        "                                                  classes = np.unique(y_org),\n",
        "                                                  y = y_org)))\n",
        "class_weight_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5PaF81P9vn6",
        "colab_type": "text"
      },
      "source": [
        "# scaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-uO7lX79vvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# fit on full data\n",
        "scaler =  StandardScaler().fit(train[features].append(val[features]).append(test[features]))\n",
        "\n",
        "# transform each data\n",
        "train[features] = scaler.transform(train[features])\n",
        "val_1[features] = scaler.transform(val_1[features])\n",
        "val_2[features] = scaler.transform(val_2[features])\n",
        "test[features] = scaler.transform(test[features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en_D6NxjXvDd",
        "colab_type": "code",
        "outputId": "77f9c1ea-b8a1-4c54-9745-18c95420d818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec_uR3ocyOqT",
        "colab_type": "text"
      },
      "source": [
        "# 建立ANN模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLiHnkzh2h97",
        "colab_type": "text"
      },
      "source": [
        "評價函數 f1_score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAS1pRSz1iXo",
        "colab_type": "code",
        "outputId": "e20ec9cf-1bc3-457a-d5f4-9cf879d3a973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqSfcw9AGr9-",
        "colab_type": "text"
      },
      "source": [
        "# optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96QqMsI-XnjM",
        "colab_type": "text"
      },
      "source": [
        "adan lr/2 版本"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM0YuK5LGsGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "adam = optimizers.Adam(lr=0.001/2)#除2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BLWkjyyX4ZY",
        "colab_type": "text"
      },
      "source": [
        "# model 架構"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um7BPdAcyJ9R",
        "colab_type": "code",
        "outputId": "50305157-3bf5-4877-c133-b04d1c277967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "source": [
        "from keras import backend\n",
        "from keras.layers import Dropout,Dense,Flatten\n",
        "from keras.models import Sequential\n",
        "import math\n",
        "\n",
        "input_dim = 100\n",
        "out_dim = 1\n",
        "\n",
        "model = Sequential()\n",
        "num_unit = 128\n",
        "num_layers = 6\n",
        "\n",
        "\n",
        "# 輸入層\n",
        "model.add(Dense(num_unit,activation = 'relu',input_dim = input_dim))\n",
        "\n",
        "# 隱藏層\n",
        "for i in range(num_layers-1):\n",
        "  model.add(Dense(num_unit,activation = 'relu'))\n",
        "\n",
        "# 輸出層\n",
        "model.add(Dense(out_dim,activation = 'sigmoid'))\n",
        "\n",
        "# 編譯\n",
        "model.compile(optimizer = adam,loss = 'binary_crossentropy',metrics=[f1_m,recall_m])\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               12928     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 95,617\n",
            "Trainable params: 95,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffdx3Sd_z8aV",
        "colab_type": "text"
      },
      "source": [
        "# Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Kj3IBWzJ-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "es = EarlyStopping(monitor='val_loss',\n",
        "                   min_delta=0,\n",
        "                   patience=40,\n",
        "                   mode='min',\n",
        "                   restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpFowLto0Eyx",
        "colab_type": "code",
        "outputId": "dab94250-5bbe-448d-e89a-87350ab57423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(train[features],train[y_name],\n",
        "                    epochs = 1000,\n",
        "                    batch_size = train.shape[0],\n",
        "                    validation_data = (val_1[features],val_1[y_name]),\n",
        "                    callbacks = [es])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 1217429 samples, validate on 152179 samples\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1217429/1217429 [==============================] - 10s 8us/step - loss: 0.7252 - f1_m: 0.0278 - recall_m: 0.9167 - val_loss: 0.6616 - val_f1_m: 0.0419 - val_recall_m: 0.1676\n",
            "Epoch 2/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.6617 - f1_m: 0.0438 - recall_m: 0.1750 - val_loss: 0.6079 - val_f1_m: 0.0077 - val_recall_m: 0.0044\n",
            "Epoch 3/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.6078 - f1_m: 0.0083 - recall_m: 0.0049 - val_loss: 0.5590 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 4/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.5589 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5120 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 5/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.5117 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4649 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.4645 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4169 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 7/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.4165 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.3683 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.3678 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.3198 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 9/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.3192 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.2730 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 10/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.2724 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.2296 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 11/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.2290 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.1913 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 12/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.1907 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.1593 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 13/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.1587 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.1344 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 14/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.1338 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.1163 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 15/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.1157 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.1046 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 16/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.1038 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0978 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 17/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0970 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0948 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 18/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0939 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0939 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 19/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0930 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0941 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 20/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0932 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0945 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 21/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0936 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0946 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 22/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0936 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0940 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 23/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0931 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0927 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 24/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0918 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0907 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 25/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0898 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0881 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 26/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0872 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0849 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 27/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0841 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0814 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 28/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0806 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0777 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 29/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0770 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0740 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 30/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0733 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0704 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 31/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0697 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0670 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 32/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0663 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0639 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 33/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0632 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0611 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 34/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0605 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0587 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 35/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0580 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0566 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 36/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0560 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0549 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 37/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0543 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0536 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 38/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0530 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0525 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 39/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0519 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0517 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 40/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0511 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0511 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 41/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0504 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0505 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 42/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0499 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0501 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 43/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0495 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0498 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 44/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0492 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0494 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 45/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0488 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0491 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 46/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0485 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0487 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 47/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0481 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0483 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 48/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0477 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0478 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 49/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0473 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0473 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 50/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0468 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0468 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 51/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0463 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0463 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 52/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0458 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0457 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 53/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0452 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0452 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 54/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0447 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0447 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 55/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0443 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0443 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 56/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0438 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0439 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 57/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0434 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0435 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 58/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0431 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0432 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 59/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0428 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0430 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 60/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0426 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0428 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 61/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0424 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0426 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 62/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0423 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0425 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 63/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0421 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0423 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 64/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0420 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0422 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 65/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0419 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0421 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 66/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0417 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0419 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 67/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0415 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0417 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 68/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0414 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0416 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 69/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0412 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0414 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 70/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0410 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0412 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 71/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0408 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0410 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 72/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0406 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0409 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 73/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0405 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0407 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 74/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0403 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0406 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 75/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0402 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0405 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 76/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0400 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0404 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 77/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0399 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0403 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 78/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0398 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0402 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 79/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0397 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0401 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 80/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0396 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0400 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 81/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0395 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0399 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 82/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0394 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0398 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 83/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0393 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0397 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 84/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0392 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0396 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 85/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0391 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0395 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 86/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0390 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0394 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 87/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0388 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0393 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 88/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0387 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0392 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 89/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0386 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0391 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 90/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0385 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0390 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 91/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0384 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0389 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 92/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0383 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0388 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 93/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0382 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0387 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 94/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0381 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0387 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 95/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0380 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0386 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 96/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0379 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0385 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 97/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0378 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0384 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 98/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0377 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0383 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 99/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0377 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0382 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 100/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0376 - f1_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.0382 - val_f1_m: 0.0079 - val_recall_m: 0.0039\n",
            "Epoch 101/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0375 - f1_m: 0.0034 - recall_m: 0.0017 - val_loss: 0.0381 - val_f1_m: 0.0347 - val_recall_m: 0.0177\n",
            "Epoch 102/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0374 - f1_m: 0.0303 - recall_m: 0.0154 - val_loss: 0.0380 - val_f1_m: 0.1058 - val_recall_m: 0.0567\n",
            "Epoch 103/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0373 - f1_m: 0.0978 - recall_m: 0.0521 - val_loss: 0.0379 - val_f1_m: 0.1691 - val_recall_m: 0.0951\n",
            "Epoch 104/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0373 - f1_m: 0.1626 - recall_m: 0.0912 - val_loss: 0.0379 - val_f1_m: 0.2118 - val_recall_m: 0.1237\n",
            "Epoch 105/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0372 - f1_m: 0.2009 - recall_m: 0.1169 - val_loss: 0.0378 - val_f1_m: 0.2383 - val_recall_m: 0.1434\n",
            "Epoch 106/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0371 - f1_m: 0.2287 - recall_m: 0.1371 - val_loss: 0.0377 - val_f1_m: 0.2559 - val_recall_m: 0.1582\n",
            "Epoch 107/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0371 - f1_m: 0.2471 - recall_m: 0.1521 - val_loss: 0.0376 - val_f1_m: 0.2789 - val_recall_m: 0.1764\n",
            "Epoch 108/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0370 - f1_m: 0.2598 - recall_m: 0.1629 - val_loss: 0.0376 - val_f1_m: 0.2923 - val_recall_m: 0.1883\n",
            "Epoch 109/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0369 - f1_m: 0.2751 - recall_m: 0.1758 - val_loss: 0.0375 - val_f1_m: 0.2972 - val_recall_m: 0.1947\n",
            "Epoch 110/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0369 - f1_m: 0.2844 - recall_m: 0.1843 - val_loss: 0.0374 - val_f1_m: 0.3088 - val_recall_m: 0.2045\n",
            "Epoch 111/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0368 - f1_m: 0.2948 - recall_m: 0.1937 - val_loss: 0.0374 - val_f1_m: 0.3216 - val_recall_m: 0.2164\n",
            "Epoch 112/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0367 - f1_m: 0.3030 - recall_m: 0.2017 - val_loss: 0.0373 - val_f1_m: 0.3349 - val_recall_m: 0.2282\n",
            "Epoch 113/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0367 - f1_m: 0.3139 - recall_m: 0.2113 - val_loss: 0.0373 - val_f1_m: 0.3409 - val_recall_m: 0.2341\n",
            "Epoch 114/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0366 - f1_m: 0.3196 - recall_m: 0.2167 - val_loss: 0.0372 - val_f1_m: 0.3477 - val_recall_m: 0.2410\n",
            "Epoch 115/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0366 - f1_m: 0.3256 - recall_m: 0.2228 - val_loss: 0.0371 - val_f1_m: 0.3532 - val_recall_m: 0.2464\n",
            "Epoch 116/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0365 - f1_m: 0.3318 - recall_m: 0.2286 - val_loss: 0.0371 - val_f1_m: 0.3587 - val_recall_m: 0.2518\n",
            "Epoch 117/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0365 - f1_m: 0.3385 - recall_m: 0.2350 - val_loss: 0.0370 - val_f1_m: 0.3673 - val_recall_m: 0.2602\n",
            "Epoch 118/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0364 - f1_m: 0.3462 - recall_m: 0.2425 - val_loss: 0.0370 - val_f1_m: 0.3790 - val_recall_m: 0.2711\n",
            "Epoch 119/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0364 - f1_m: 0.3528 - recall_m: 0.2490 - val_loss: 0.0369 - val_f1_m: 0.3833 - val_recall_m: 0.2760\n",
            "Epoch 120/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0363 - f1_m: 0.3583 - recall_m: 0.2543 - val_loss: 0.0369 - val_f1_m: 0.3843 - val_recall_m: 0.2775\n",
            "Epoch 121/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0363 - f1_m: 0.3616 - recall_m: 0.2577 - val_loss: 0.0368 - val_f1_m: 0.3880 - val_recall_m: 0.2814\n",
            "Epoch 122/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0362 - f1_m: 0.3661 - recall_m: 0.2621 - val_loss: 0.0368 - val_f1_m: 0.3911 - val_recall_m: 0.2849\n",
            "Epoch 123/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0362 - f1_m: 0.3702 - recall_m: 0.2661 - val_loss: 0.0367 - val_f1_m: 0.3930 - val_recall_m: 0.2868\n",
            "Epoch 124/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0361 - f1_m: 0.3732 - recall_m: 0.2686 - val_loss: 0.0367 - val_f1_m: 0.3972 - val_recall_m: 0.2903\n",
            "Epoch 125/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0361 - f1_m: 0.3755 - recall_m: 0.2702 - val_loss: 0.0366 - val_f1_m: 0.3980 - val_recall_m: 0.2903\n",
            "Epoch 126/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0360 - f1_m: 0.3781 - recall_m: 0.2721 - val_loss: 0.0366 - val_f1_m: 0.3989 - val_recall_m: 0.2908\n",
            "Epoch 127/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0360 - f1_m: 0.3792 - recall_m: 0.2727 - val_loss: 0.0365 - val_f1_m: 0.4004 - val_recall_m: 0.2913\n",
            "Epoch 128/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0359 - f1_m: 0.3807 - recall_m: 0.2729 - val_loss: 0.0365 - val_f1_m: 0.3999 - val_recall_m: 0.2898\n",
            "Epoch 129/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0359 - f1_m: 0.3827 - recall_m: 0.2739 - val_loss: 0.0364 - val_f1_m: 0.4004 - val_recall_m: 0.2893\n",
            "Epoch 130/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0358 - f1_m: 0.3841 - recall_m: 0.2743 - val_loss: 0.0364 - val_f1_m: 0.3985 - val_recall_m: 0.2868\n",
            "Epoch 131/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0358 - f1_m: 0.3820 - recall_m: 0.2716 - val_loss: 0.0363 - val_f1_m: 0.3996 - val_recall_m: 0.2868\n",
            "Epoch 132/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0357 - f1_m: 0.3838 - recall_m: 0.2725 - val_loss: 0.0363 - val_f1_m: 0.3977 - val_recall_m: 0.2844\n",
            "Epoch 133/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0357 - f1_m: 0.3860 - recall_m: 0.2737 - val_loss: 0.0362 - val_f1_m: 0.4012 - val_recall_m: 0.2878\n",
            "Epoch 134/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0356 - f1_m: 0.3873 - recall_m: 0.2745 - val_loss: 0.0362 - val_f1_m: 0.4052 - val_recall_m: 0.2908\n",
            "Epoch 135/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0356 - f1_m: 0.3901 - recall_m: 0.2763 - val_loss: 0.0362 - val_f1_m: 0.4060 - val_recall_m: 0.2913\n",
            "Epoch 136/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0355 - f1_m: 0.3917 - recall_m: 0.2775 - val_loss: 0.0361 - val_f1_m: 0.4095 - val_recall_m: 0.2932\n",
            "Epoch 137/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0355 - f1_m: 0.3955 - recall_m: 0.2804 - val_loss: 0.0361 - val_f1_m: 0.4095 - val_recall_m: 0.2932\n",
            "Epoch 138/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0354 - f1_m: 0.3973 - recall_m: 0.2821 - val_loss: 0.0360 - val_f1_m: 0.4144 - val_recall_m: 0.2977\n",
            "Epoch 139/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0354 - f1_m: 0.4029 - recall_m: 0.2870 - val_loss: 0.0360 - val_f1_m: 0.4187 - val_recall_m: 0.3021\n",
            "Epoch 140/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0353 - f1_m: 0.4058 - recall_m: 0.2902 - val_loss: 0.0359 - val_f1_m: 0.4210 - val_recall_m: 0.3046\n",
            "Epoch 141/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0353 - f1_m: 0.4084 - recall_m: 0.2929 - val_loss: 0.0359 - val_f1_m: 0.4244 - val_recall_m: 0.3080\n",
            "Epoch 142/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0352 - f1_m: 0.4120 - recall_m: 0.2966 - val_loss: 0.0358 - val_f1_m: 0.4253 - val_recall_m: 0.3100\n",
            "Epoch 143/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0352 - f1_m: 0.4149 - recall_m: 0.2997 - val_loss: 0.0358 - val_f1_m: 0.4258 - val_recall_m: 0.3110\n",
            "Epoch 144/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0351 - f1_m: 0.4160 - recall_m: 0.3010 - val_loss: 0.0358 - val_f1_m: 0.4279 - val_recall_m: 0.3130\n",
            "Epoch 145/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0351 - f1_m: 0.4186 - recall_m: 0.3034 - val_loss: 0.0357 - val_f1_m: 0.4291 - val_recall_m: 0.3139\n",
            "Epoch 146/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0350 - f1_m: 0.4205 - recall_m: 0.3055 - val_loss: 0.0357 - val_f1_m: 0.4295 - val_recall_m: 0.3144\n",
            "Epoch 147/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0350 - f1_m: 0.4230 - recall_m: 0.3077 - val_loss: 0.0356 - val_f1_m: 0.4320 - val_recall_m: 0.3164\n",
            "Epoch 148/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0349 - f1_m: 0.4246 - recall_m: 0.3091 - val_loss: 0.0356 - val_f1_m: 0.4361 - val_recall_m: 0.3189\n",
            "Epoch 149/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0349 - f1_m: 0.4261 - recall_m: 0.3095 - val_loss: 0.0356 - val_f1_m: 0.4370 - val_recall_m: 0.3194\n",
            "Epoch 150/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0349 - f1_m: 0.4266 - recall_m: 0.3095 - val_loss: 0.0355 - val_f1_m: 0.4364 - val_recall_m: 0.3189\n",
            "Epoch 151/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0348 - f1_m: 0.4272 - recall_m: 0.3096 - val_loss: 0.0355 - val_f1_m: 0.4329 - val_recall_m: 0.3164\n",
            "Epoch 152/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0348 - f1_m: 0.4266 - recall_m: 0.3090 - val_loss: 0.0355 - val_f1_m: 0.4318 - val_recall_m: 0.3154\n",
            "Epoch 153/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0347 - f1_m: 0.4276 - recall_m: 0.3093 - val_loss: 0.0354 - val_f1_m: 0.4289 - val_recall_m: 0.3130\n",
            "Epoch 154/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0347 - f1_m: 0.4281 - recall_m: 0.3098 - val_loss: 0.0354 - val_f1_m: 0.4283 - val_recall_m: 0.3130\n",
            "Epoch 155/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0347 - f1_m: 0.4286 - recall_m: 0.3104 - val_loss: 0.0354 - val_f1_m: 0.4290 - val_recall_m: 0.3135\n",
            "Epoch 156/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0346 - f1_m: 0.4301 - recall_m: 0.3117 - val_loss: 0.0353 - val_f1_m: 0.4323 - val_recall_m: 0.3164\n",
            "Epoch 157/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0346 - f1_m: 0.4317 - recall_m: 0.3133 - val_loss: 0.0353 - val_f1_m: 0.4314 - val_recall_m: 0.3154\n",
            "Epoch 158/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0345 - f1_m: 0.4339 - recall_m: 0.3152 - val_loss: 0.0353 - val_f1_m: 0.4298 - val_recall_m: 0.3139\n",
            "Epoch 159/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0345 - f1_m: 0.4344 - recall_m: 0.3157 - val_loss: 0.0352 - val_f1_m: 0.4293 - val_recall_m: 0.3135\n",
            "Epoch 160/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0345 - f1_m: 0.4342 - recall_m: 0.3155 - val_loss: 0.0352 - val_f1_m: 0.4306 - val_recall_m: 0.3149\n",
            "Epoch 161/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0344 - f1_m: 0.4355 - recall_m: 0.3166 - val_loss: 0.0352 - val_f1_m: 0.4301 - val_recall_m: 0.3144\n",
            "Epoch 162/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0344 - f1_m: 0.4368 - recall_m: 0.3178 - val_loss: 0.0351 - val_f1_m: 0.4306 - val_recall_m: 0.3149\n",
            "Epoch 163/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0343 - f1_m: 0.4367 - recall_m: 0.3178 - val_loss: 0.0351 - val_f1_m: 0.4286 - val_recall_m: 0.3130\n",
            "Epoch 164/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0343 - f1_m: 0.4373 - recall_m: 0.3181 - val_loss: 0.0351 - val_f1_m: 0.4293 - val_recall_m: 0.3135\n",
            "Epoch 165/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0343 - f1_m: 0.4377 - recall_m: 0.3187 - val_loss: 0.0350 - val_f1_m: 0.4295 - val_recall_m: 0.3139\n",
            "Epoch 166/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0342 - f1_m: 0.4380 - recall_m: 0.3190 - val_loss: 0.0350 - val_f1_m: 0.4301 - val_recall_m: 0.3144\n",
            "Epoch 167/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0342 - f1_m: 0.4390 - recall_m: 0.3198 - val_loss: 0.0350 - val_f1_m: 0.4311 - val_recall_m: 0.3154\n",
            "Epoch 168/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0342 - f1_m: 0.4395 - recall_m: 0.3202 - val_loss: 0.0349 - val_f1_m: 0.4308 - val_recall_m: 0.3154\n",
            "Epoch 169/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0341 - f1_m: 0.4398 - recall_m: 0.3204 - val_loss: 0.0349 - val_f1_m: 0.4304 - val_recall_m: 0.3154\n",
            "Epoch 170/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0341 - f1_m: 0.4401 - recall_m: 0.3207 - val_loss: 0.0349 - val_f1_m: 0.4315 - val_recall_m: 0.3164\n",
            "Epoch 171/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0341 - f1_m: 0.4406 - recall_m: 0.3211 - val_loss: 0.0348 - val_f1_m: 0.4303 - val_recall_m: 0.3159\n",
            "Epoch 172/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0340 - f1_m: 0.4410 - recall_m: 0.3216 - val_loss: 0.0348 - val_f1_m: 0.4309 - val_recall_m: 0.3164\n",
            "Epoch 173/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0340 - f1_m: 0.4413 - recall_m: 0.3220 - val_loss: 0.0348 - val_f1_m: 0.4316 - val_recall_m: 0.3174\n",
            "Epoch 174/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0339 - f1_m: 0.4419 - recall_m: 0.3227 - val_loss: 0.0347 - val_f1_m: 0.4320 - val_recall_m: 0.3179\n",
            "Epoch 175/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0339 - f1_m: 0.4420 - recall_m: 0.3230 - val_loss: 0.0347 - val_f1_m: 0.4313 - val_recall_m: 0.3174\n",
            "Epoch 176/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0339 - f1_m: 0.4425 - recall_m: 0.3234 - val_loss: 0.0347 - val_f1_m: 0.4324 - val_recall_m: 0.3184\n",
            "Epoch 177/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0338 - f1_m: 0.4424 - recall_m: 0.3233 - val_loss: 0.0347 - val_f1_m: 0.4324 - val_recall_m: 0.3184\n",
            "Epoch 178/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0338 - f1_m: 0.4428 - recall_m: 0.3236 - val_loss: 0.0346 - val_f1_m: 0.4332 - val_recall_m: 0.3194\n",
            "Epoch 179/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0338 - f1_m: 0.4427 - recall_m: 0.3238 - val_loss: 0.0346 - val_f1_m: 0.4330 - val_recall_m: 0.3194\n",
            "Epoch 180/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0337 - f1_m: 0.4438 - recall_m: 0.3250 - val_loss: 0.0346 - val_f1_m: 0.4325 - val_recall_m: 0.3189\n",
            "Epoch 181/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0337 - f1_m: 0.4433 - recall_m: 0.3247 - val_loss: 0.0345 - val_f1_m: 0.4329 - val_recall_m: 0.3194\n",
            "Epoch 182/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0337 - f1_m: 0.4433 - recall_m: 0.3246 - val_loss: 0.0345 - val_f1_m: 0.4337 - val_recall_m: 0.3199\n",
            "Epoch 183/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0336 - f1_m: 0.4459 - recall_m: 0.3269 - val_loss: 0.0345 - val_f1_m: 0.4342 - val_recall_m: 0.3204\n",
            "Epoch 184/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0336 - f1_m: 0.4461 - recall_m: 0.3272 - val_loss: 0.0344 - val_f1_m: 0.4330 - val_recall_m: 0.3194\n",
            "Epoch 185/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0336 - f1_m: 0.4464 - recall_m: 0.3274 - val_loss: 0.0344 - val_f1_m: 0.4343 - val_recall_m: 0.3204\n",
            "Epoch 186/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0335 - f1_m: 0.4468 - recall_m: 0.3277 - val_loss: 0.0344 - val_f1_m: 0.4361 - val_recall_m: 0.3213\n",
            "Epoch 187/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0335 - f1_m: 0.4472 - recall_m: 0.3280 - val_loss: 0.0344 - val_f1_m: 0.4395 - val_recall_m: 0.3238\n",
            "Epoch 188/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0335 - f1_m: 0.4480 - recall_m: 0.3286 - val_loss: 0.0343 - val_f1_m: 0.4393 - val_recall_m: 0.3238\n",
            "Epoch 189/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0334 - f1_m: 0.4480 - recall_m: 0.3286 - val_loss: 0.0343 - val_f1_m: 0.4396 - val_recall_m: 0.3238\n",
            "Epoch 190/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0334 - f1_m: 0.4482 - recall_m: 0.3286 - val_loss: 0.0343 - val_f1_m: 0.4412 - val_recall_m: 0.3253\n",
            "Epoch 191/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0334 - f1_m: 0.4490 - recall_m: 0.3293 - val_loss: 0.0342 - val_f1_m: 0.4430 - val_recall_m: 0.3268\n",
            "Epoch 192/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0333 - f1_m: 0.4508 - recall_m: 0.3307 - val_loss: 0.0342 - val_f1_m: 0.4439 - val_recall_m: 0.3277\n",
            "Epoch 193/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0333 - f1_m: 0.4514 - recall_m: 0.3316 - val_loss: 0.0342 - val_f1_m: 0.4453 - val_recall_m: 0.3292\n",
            "Epoch 194/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0333 - f1_m: 0.4545 - recall_m: 0.3344 - val_loss: 0.0342 - val_f1_m: 0.4447 - val_recall_m: 0.3282\n",
            "Epoch 195/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0332 - f1_m: 0.4545 - recall_m: 0.3344 - val_loss: 0.0341 - val_f1_m: 0.4455 - val_recall_m: 0.3282\n",
            "Epoch 196/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0332 - f1_m: 0.4554 - recall_m: 0.3344 - val_loss: 0.0341 - val_f1_m: 0.4462 - val_recall_m: 0.3287\n",
            "Epoch 197/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0332 - f1_m: 0.4557 - recall_m: 0.3344 - val_loss: 0.0341 - val_f1_m: 0.4486 - val_recall_m: 0.3312\n",
            "Epoch 198/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0332 - f1_m: 0.4577 - recall_m: 0.3362 - val_loss: 0.0340 - val_f1_m: 0.4481 - val_recall_m: 0.3307\n",
            "Epoch 199/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0331 - f1_m: 0.4583 - recall_m: 0.3366 - val_loss: 0.0340 - val_f1_m: 0.4486 - val_recall_m: 0.3312\n",
            "Epoch 200/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0331 - f1_m: 0.4588 - recall_m: 0.3372 - val_loss: 0.0340 - val_f1_m: 0.4495 - val_recall_m: 0.3322\n",
            "Epoch 201/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0331 - f1_m: 0.4602 - recall_m: 0.3386 - val_loss: 0.0340 - val_f1_m: 0.4491 - val_recall_m: 0.3317\n",
            "Epoch 202/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0330 - f1_m: 0.4599 - recall_m: 0.3381 - val_loss: 0.0339 - val_f1_m: 0.4489 - val_recall_m: 0.3312\n",
            "Epoch 203/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0330 - f1_m: 0.4597 - recall_m: 0.3379 - val_loss: 0.0339 - val_f1_m: 0.4490 - val_recall_m: 0.3312\n",
            "Epoch 204/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0330 - f1_m: 0.4598 - recall_m: 0.3378 - val_loss: 0.0339 - val_f1_m: 0.4508 - val_recall_m: 0.3327\n",
            "Epoch 205/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0329 - f1_m: 0.4600 - recall_m: 0.3380 - val_loss: 0.0339 - val_f1_m: 0.4505 - val_recall_m: 0.3327\n",
            "Epoch 206/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0329 - f1_m: 0.4606 - recall_m: 0.3386 - val_loss: 0.0338 - val_f1_m: 0.4511 - val_recall_m: 0.3332\n",
            "Epoch 207/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0329 - f1_m: 0.4609 - recall_m: 0.3389 - val_loss: 0.0338 - val_f1_m: 0.4518 - val_recall_m: 0.3337\n",
            "Epoch 208/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0328 - f1_m: 0.4619 - recall_m: 0.3397 - val_loss: 0.0338 - val_f1_m: 0.4528 - val_recall_m: 0.3337\n",
            "Epoch 209/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0328 - f1_m: 0.4627 - recall_m: 0.3403 - val_loss: 0.0337 - val_f1_m: 0.4530 - val_recall_m: 0.3337\n",
            "Epoch 210/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0328 - f1_m: 0.4626 - recall_m: 0.3400 - val_loss: 0.0337 - val_f1_m: 0.4542 - val_recall_m: 0.3346\n",
            "Epoch 211/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0327 - f1_m: 0.4627 - recall_m: 0.3400 - val_loss: 0.0337 - val_f1_m: 0.4535 - val_recall_m: 0.3342\n",
            "Epoch 212/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0327 - f1_m: 0.4633 - recall_m: 0.3406 - val_loss: 0.0337 - val_f1_m: 0.4522 - val_recall_m: 0.3332\n",
            "Epoch 213/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0327 - f1_m: 0.4640 - recall_m: 0.3416 - val_loss: 0.0336 - val_f1_m: 0.4531 - val_recall_m: 0.3342\n",
            "Epoch 214/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0326 - f1_m: 0.4648 - recall_m: 0.3422 - val_loss: 0.0336 - val_f1_m: 0.4529 - val_recall_m: 0.3342\n",
            "Epoch 215/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0326 - f1_m: 0.4646 - recall_m: 0.3420 - val_loss: 0.0336 - val_f1_m: 0.4531 - val_recall_m: 0.3342\n",
            "Epoch 216/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0326 - f1_m: 0.4650 - recall_m: 0.3425 - val_loss: 0.0335 - val_f1_m: 0.4532 - val_recall_m: 0.3342\n",
            "Epoch 217/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0325 - f1_m: 0.4652 - recall_m: 0.3426 - val_loss: 0.0335 - val_f1_m: 0.4532 - val_recall_m: 0.3342\n",
            "Epoch 218/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0325 - f1_m: 0.4652 - recall_m: 0.3426 - val_loss: 0.0335 - val_f1_m: 0.4561 - val_recall_m: 0.3366\n",
            "Epoch 219/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0325 - f1_m: 0.4662 - recall_m: 0.3436 - val_loss: 0.0335 - val_f1_m: 0.4583 - val_recall_m: 0.3386\n",
            "Epoch 220/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0324 - f1_m: 0.4668 - recall_m: 0.3443 - val_loss: 0.0334 - val_f1_m: 0.4571 - val_recall_m: 0.3376\n",
            "Epoch 221/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0324 - f1_m: 0.4672 - recall_m: 0.3447 - val_loss: 0.0334 - val_f1_m: 0.4574 - val_recall_m: 0.3376\n",
            "Epoch 222/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0324 - f1_m: 0.4670 - recall_m: 0.3445 - val_loss: 0.0334 - val_f1_m: 0.4579 - val_recall_m: 0.3381\n",
            "Epoch 223/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0323 - f1_m: 0.4675 - recall_m: 0.3449 - val_loss: 0.0333 - val_f1_m: 0.4598 - val_recall_m: 0.3401\n",
            "Epoch 224/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0323 - f1_m: 0.4689 - recall_m: 0.3465 - val_loss: 0.0333 - val_f1_m: 0.4617 - val_recall_m: 0.3420\n",
            "Epoch 225/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0323 - f1_m: 0.4701 - recall_m: 0.3479 - val_loss: 0.0333 - val_f1_m: 0.4628 - val_recall_m: 0.3430\n",
            "Epoch 226/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0322 - f1_m: 0.4709 - recall_m: 0.3487 - val_loss: 0.0333 - val_f1_m: 0.4638 - val_recall_m: 0.3440\n",
            "Epoch 227/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0322 - f1_m: 0.4715 - recall_m: 0.3492 - val_loss: 0.0332 - val_f1_m: 0.4653 - val_recall_m: 0.3455\n",
            "Epoch 228/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0322 - f1_m: 0.4721 - recall_m: 0.3497 - val_loss: 0.0332 - val_f1_m: 0.4660 - val_recall_m: 0.3465\n",
            "Epoch 229/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0321 - f1_m: 0.4722 - recall_m: 0.3499 - val_loss: 0.0332 - val_f1_m: 0.4662 - val_recall_m: 0.3470\n",
            "Epoch 230/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0321 - f1_m: 0.4731 - recall_m: 0.3507 - val_loss: 0.0331 - val_f1_m: 0.4667 - val_recall_m: 0.3475\n",
            "Epoch 231/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0321 - f1_m: 0.4732 - recall_m: 0.3508 - val_loss: 0.0331 - val_f1_m: 0.4659 - val_recall_m: 0.3470\n",
            "Epoch 232/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0320 - f1_m: 0.4741 - recall_m: 0.3516 - val_loss: 0.0331 - val_f1_m: 0.4664 - val_recall_m: 0.3475\n",
            "Epoch 233/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0320 - f1_m: 0.4755 - recall_m: 0.3529 - val_loss: 0.0330 - val_f1_m: 0.4680 - val_recall_m: 0.3494\n",
            "Epoch 234/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0320 - f1_m: 0.4755 - recall_m: 0.3529 - val_loss: 0.0330 - val_f1_m: 0.4675 - val_recall_m: 0.3489\n",
            "Epoch 235/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0319 - f1_m: 0.4761 - recall_m: 0.3535 - val_loss: 0.0330 - val_f1_m: 0.4673 - val_recall_m: 0.3489\n",
            "Epoch 236/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0319 - f1_m: 0.4770 - recall_m: 0.3545 - val_loss: 0.0329 - val_f1_m: 0.4675 - val_recall_m: 0.3494\n",
            "Epoch 237/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0319 - f1_m: 0.4784 - recall_m: 0.3559 - val_loss: 0.0329 - val_f1_m: 0.4681 - val_recall_m: 0.3504\n",
            "Epoch 238/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0318 - f1_m: 0.4784 - recall_m: 0.3559 - val_loss: 0.0329 - val_f1_m: 0.4699 - val_recall_m: 0.3519\n",
            "Epoch 239/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0318 - f1_m: 0.4791 - recall_m: 0.3566 - val_loss: 0.0329 - val_f1_m: 0.4695 - val_recall_m: 0.3514\n",
            "Epoch 240/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0318 - f1_m: 0.4795 - recall_m: 0.3572 - val_loss: 0.0328 - val_f1_m: 0.4699 - val_recall_m: 0.3519\n",
            "Epoch 241/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0317 - f1_m: 0.4802 - recall_m: 0.3580 - val_loss: 0.0328 - val_f1_m: 0.4699 - val_recall_m: 0.3524\n",
            "Epoch 242/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0317 - f1_m: 0.4803 - recall_m: 0.3581 - val_loss: 0.0328 - val_f1_m: 0.4701 - val_recall_m: 0.3529\n",
            "Epoch 243/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0317 - f1_m: 0.4807 - recall_m: 0.3586 - val_loss: 0.0327 - val_f1_m: 0.4701 - val_recall_m: 0.3529\n",
            "Epoch 244/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0316 - f1_m: 0.4811 - recall_m: 0.3588 - val_loss: 0.0327 - val_f1_m: 0.4700 - val_recall_m: 0.3529\n",
            "Epoch 245/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0316 - f1_m: 0.4818 - recall_m: 0.3596 - val_loss: 0.0327 - val_f1_m: 0.4700 - val_recall_m: 0.3529\n",
            "Epoch 246/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0316 - f1_m: 0.4822 - recall_m: 0.3598 - val_loss: 0.0327 - val_f1_m: 0.4699 - val_recall_m: 0.3534\n",
            "Epoch 247/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0315 - f1_m: 0.4829 - recall_m: 0.3604 - val_loss: 0.0326 - val_f1_m: 0.4697 - val_recall_m: 0.3534\n",
            "Epoch 248/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0315 - f1_m: 0.4834 - recall_m: 0.3610 - val_loss: 0.0326 - val_f1_m: 0.4692 - val_recall_m: 0.3534\n",
            "Epoch 249/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0315 - f1_m: 0.4839 - recall_m: 0.3616 - val_loss: 0.0326 - val_f1_m: 0.4696 - val_recall_m: 0.3539\n",
            "Epoch 250/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0314 - f1_m: 0.4840 - recall_m: 0.3617 - val_loss: 0.0325 - val_f1_m: 0.4717 - val_recall_m: 0.3558\n",
            "Epoch 251/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0314 - f1_m: 0.4851 - recall_m: 0.3628 - val_loss: 0.0325 - val_f1_m: 0.4722 - val_recall_m: 0.3563\n",
            "Epoch 252/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0314 - f1_m: 0.4863 - recall_m: 0.3641 - val_loss: 0.0325 - val_f1_m: 0.4699 - val_recall_m: 0.3539\n",
            "Epoch 253/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0313 - f1_m: 0.4858 - recall_m: 0.3632 - val_loss: 0.0325 - val_f1_m: 0.4709 - val_recall_m: 0.3549\n",
            "Epoch 254/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0313 - f1_m: 0.4865 - recall_m: 0.3641 - val_loss: 0.0324 - val_f1_m: 0.4724 - val_recall_m: 0.3563\n",
            "Epoch 255/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0313 - f1_m: 0.4880 - recall_m: 0.3657 - val_loss: 0.0324 - val_f1_m: 0.4722 - val_recall_m: 0.3558\n",
            "Epoch 256/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0312 - f1_m: 0.4886 - recall_m: 0.3657 - val_loss: 0.0324 - val_f1_m: 0.4724 - val_recall_m: 0.3558\n",
            "Epoch 257/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0312 - f1_m: 0.4897 - recall_m: 0.3665 - val_loss: 0.0323 - val_f1_m: 0.4737 - val_recall_m: 0.3573\n",
            "Epoch 258/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0312 - f1_m: 0.4919 - recall_m: 0.3687 - val_loss: 0.0323 - val_f1_m: 0.4729 - val_recall_m: 0.3563\n",
            "Epoch 259/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0311 - f1_m: 0.4923 - recall_m: 0.3690 - val_loss: 0.0323 - val_f1_m: 0.4735 - val_recall_m: 0.3568\n",
            "Epoch 260/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0311 - f1_m: 0.4931 - recall_m: 0.3698 - val_loss: 0.0323 - val_f1_m: 0.4742 - val_recall_m: 0.3573\n",
            "Epoch 261/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0310 - f1_m: 0.4935 - recall_m: 0.3703 - val_loss: 0.0322 - val_f1_m: 0.4733 - val_recall_m: 0.3563\n",
            "Epoch 262/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0310 - f1_m: 0.4934 - recall_m: 0.3700 - val_loss: 0.0322 - val_f1_m: 0.4726 - val_recall_m: 0.3553\n",
            "Epoch 263/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0310 - f1_m: 0.4930 - recall_m: 0.3695 - val_loss: 0.0322 - val_f1_m: 0.4745 - val_recall_m: 0.3573\n",
            "Epoch 264/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0309 - f1_m: 0.4942 - recall_m: 0.3710 - val_loss: 0.0321 - val_f1_m: 0.4753 - val_recall_m: 0.3578\n",
            "Epoch 265/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0309 - f1_m: 0.4950 - recall_m: 0.3714 - val_loss: 0.0321 - val_f1_m: 0.4765 - val_recall_m: 0.3593\n",
            "Epoch 266/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0309 - f1_m: 0.4953 - recall_m: 0.3717 - val_loss: 0.0321 - val_f1_m: 0.4783 - val_recall_m: 0.3608\n",
            "Epoch 267/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0308 - f1_m: 0.4967 - recall_m: 0.3730 - val_loss: 0.0320 - val_f1_m: 0.4788 - val_recall_m: 0.3613\n",
            "Epoch 268/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0308 - f1_m: 0.4974 - recall_m: 0.3733 - val_loss: 0.0320 - val_f1_m: 0.4801 - val_recall_m: 0.3622\n",
            "Epoch 269/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0308 - f1_m: 0.4981 - recall_m: 0.3744 - val_loss: 0.0320 - val_f1_m: 0.4799 - val_recall_m: 0.3618\n",
            "Epoch 270/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0307 - f1_m: 0.4991 - recall_m: 0.3752 - val_loss: 0.0320 - val_f1_m: 0.4810 - val_recall_m: 0.3627\n",
            "Epoch 271/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0307 - f1_m: 0.4990 - recall_m: 0.3750 - val_loss: 0.0319 - val_f1_m: 0.4829 - val_recall_m: 0.3647\n",
            "Epoch 272/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0306 - f1_m: 0.5022 - recall_m: 0.3783 - val_loss: 0.0319 - val_f1_m: 0.4840 - val_recall_m: 0.3657\n",
            "Epoch 273/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0306 - f1_m: 0.5030 - recall_m: 0.3791 - val_loss: 0.0319 - val_f1_m: 0.4833 - val_recall_m: 0.3647\n",
            "Epoch 274/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0306 - f1_m: 0.5034 - recall_m: 0.3795 - val_loss: 0.0318 - val_f1_m: 0.4833 - val_recall_m: 0.3647\n",
            "Epoch 275/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0305 - f1_m: 0.5034 - recall_m: 0.3794 - val_loss: 0.0318 - val_f1_m: 0.4825 - val_recall_m: 0.3637\n",
            "Epoch 276/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0305 - f1_m: 0.5040 - recall_m: 0.3800 - val_loss: 0.0318 - val_f1_m: 0.4853 - val_recall_m: 0.3672\n",
            "Epoch 277/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0304 - f1_m: 0.5065 - recall_m: 0.3830 - val_loss: 0.0317 - val_f1_m: 0.4830 - val_recall_m: 0.3637\n",
            "Epoch 278/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0304 - f1_m: 0.5059 - recall_m: 0.3813 - val_loss: 0.0316 - val_f1_m: 0.4873 - val_recall_m: 0.3696\n",
            "Epoch 279/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0304 - f1_m: 0.5079 - recall_m: 0.3850 - val_loss: 0.0316 - val_f1_m: 0.4831 - val_recall_m: 0.3637\n",
            "Epoch 280/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0303 - f1_m: 0.5057 - recall_m: 0.3803 - val_loss: 0.0315 - val_f1_m: 0.4896 - val_recall_m: 0.3721\n",
            "Epoch 281/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0303 - f1_m: 0.5100 - recall_m: 0.3869 - val_loss: 0.0315 - val_f1_m: 0.4832 - val_recall_m: 0.3642\n",
            "Epoch 282/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0302 - f1_m: 0.5066 - recall_m: 0.3816 - val_loss: 0.0314 - val_f1_m: 0.4886 - val_recall_m: 0.3706\n",
            "Epoch 283/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0302 - f1_m: 0.5102 - recall_m: 0.3866 - val_loss: 0.0313 - val_f1_m: 0.4929 - val_recall_m: 0.3741\n",
            "Epoch 284/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0302 - f1_m: 0.5130 - recall_m: 0.3886 - val_loss: 0.0313 - val_f1_m: 0.4945 - val_recall_m: 0.3756\n",
            "Epoch 285/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0301 - f1_m: 0.5133 - recall_m: 0.3887 - val_loss: 0.0313 - val_f1_m: 0.4979 - val_recall_m: 0.3795\n",
            "Epoch 286/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0301 - f1_m: 0.5152 - recall_m: 0.3917 - val_loss: 0.0312 - val_f1_m: 0.4922 - val_recall_m: 0.3731\n",
            "Epoch 287/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0300 - f1_m: 0.5127 - recall_m: 0.3878 - val_loss: 0.0312 - val_f1_m: 0.4987 - val_recall_m: 0.3810\n",
            "Epoch 288/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0300 - f1_m: 0.5170 - recall_m: 0.3938 - val_loss: 0.0312 - val_f1_m: 0.4904 - val_recall_m: 0.3706\n",
            "Epoch 289/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0300 - f1_m: 0.5127 - recall_m: 0.3873 - val_loss: 0.0311 - val_f1_m: 0.4986 - val_recall_m: 0.3820\n",
            "Epoch 290/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0299 - f1_m: 0.5187 - recall_m: 0.3964 - val_loss: 0.0311 - val_f1_m: 0.4961 - val_recall_m: 0.3760\n",
            "Epoch 291/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0299 - f1_m: 0.5155 - recall_m: 0.3904 - val_loss: 0.0311 - val_f1_m: 0.4984 - val_recall_m: 0.3820\n",
            "Epoch 292/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0298 - f1_m: 0.5194 - recall_m: 0.3965 - val_loss: 0.0311 - val_f1_m: 0.4966 - val_recall_m: 0.3775\n",
            "Epoch 293/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0298 - f1_m: 0.5172 - recall_m: 0.3928 - val_loss: 0.0310 - val_f1_m: 0.4974 - val_recall_m: 0.3790\n",
            "Epoch 294/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0298 - f1_m: 0.5190 - recall_m: 0.3948 - val_loss: 0.0310 - val_f1_m: 0.4994 - val_recall_m: 0.3820\n",
            "Epoch 295/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0297 - f1_m: 0.5213 - recall_m: 0.3979 - val_loss: 0.0310 - val_f1_m: 0.4969 - val_recall_m: 0.3775\n",
            "Epoch 296/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0297 - f1_m: 0.5174 - recall_m: 0.3925 - val_loss: 0.0309 - val_f1_m: 0.5016 - val_recall_m: 0.3849\n",
            "Epoch 297/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0296 - f1_m: 0.5240 - recall_m: 0.4012 - val_loss: 0.0309 - val_f1_m: 0.4948 - val_recall_m: 0.3751\n",
            "Epoch 298/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0296 - f1_m: 0.5167 - recall_m: 0.3910 - val_loss: 0.0309 - val_f1_m: 0.5067 - val_recall_m: 0.3913\n",
            "Epoch 299/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0296 - f1_m: 0.5282 - recall_m: 0.4069 - val_loss: 0.0309 - val_f1_m: 0.4925 - val_recall_m: 0.3726\n",
            "Epoch 300/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0295 - f1_m: 0.5167 - recall_m: 0.3906 - val_loss: 0.0308 - val_f1_m: 0.5075 - val_recall_m: 0.3918\n",
            "Epoch 301/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0295 - f1_m: 0.5286 - recall_m: 0.4072 - val_loss: 0.0308 - val_f1_m: 0.4966 - val_recall_m: 0.3775\n",
            "Epoch 302/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0295 - f1_m: 0.5206 - recall_m: 0.3954 - val_loss: 0.0308 - val_f1_m: 0.5008 - val_recall_m: 0.3834\n",
            "Epoch 303/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0294 - f1_m: 0.5257 - recall_m: 0.4021 - val_loss: 0.0308 - val_f1_m: 0.5034 - val_recall_m: 0.3869\n",
            "Epoch 304/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0294 - f1_m: 0.5264 - recall_m: 0.4036 - val_loss: 0.0307 - val_f1_m: 0.4960 - val_recall_m: 0.3775\n",
            "Epoch 305/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0293 - f1_m: 0.5220 - recall_m: 0.3969 - val_loss: 0.0307 - val_f1_m: 0.5078 - val_recall_m: 0.3928\n",
            "Epoch 306/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0293 - f1_m: 0.5324 - recall_m: 0.4115 - val_loss: 0.0307 - val_f1_m: 0.4943 - val_recall_m: 0.3760\n",
            "Epoch 307/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0293 - f1_m: 0.5206 - recall_m: 0.3949 - val_loss: 0.0306 - val_f1_m: 0.5102 - val_recall_m: 0.3953\n",
            "Epoch 308/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0292 - f1_m: 0.5354 - recall_m: 0.4147 - val_loss: 0.0306 - val_f1_m: 0.5008 - val_recall_m: 0.3834\n",
            "Epoch 309/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0292 - f1_m: 0.5250 - recall_m: 0.4001 - val_loss: 0.0306 - val_f1_m: 0.5067 - val_recall_m: 0.3913\n",
            "Epoch 310/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0292 - f1_m: 0.5323 - recall_m: 0.4104 - val_loss: 0.0306 - val_f1_m: 0.5054 - val_recall_m: 0.3898\n",
            "Epoch 311/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0291 - f1_m: 0.5325 - recall_m: 0.4099 - val_loss: 0.0305 - val_f1_m: 0.5026 - val_recall_m: 0.3859\n",
            "Epoch 312/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0291 - f1_m: 0.5293 - recall_m: 0.4053 - val_loss: 0.0305 - val_f1_m: 0.5117 - val_recall_m: 0.3977\n",
            "Epoch 313/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0291 - f1_m: 0.5381 - recall_m: 0.4173 - val_loss: 0.0305 - val_f1_m: 0.5010 - val_recall_m: 0.3834\n",
            "Epoch 314/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0290 - f1_m: 0.5265 - recall_m: 0.4012 - val_loss: 0.0305 - val_f1_m: 0.5173 - val_recall_m: 0.4046\n",
            "Epoch 315/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0290 - f1_m: 0.5409 - recall_m: 0.4219 - val_loss: 0.0304 - val_f1_m: 0.4994 - val_recall_m: 0.3810\n",
            "Epoch 316/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0290 - f1_m: 0.5271 - recall_m: 0.4011 - val_loss: 0.0304 - val_f1_m: 0.5178 - val_recall_m: 0.4056\n",
            "Epoch 317/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0289 - f1_m: 0.5416 - recall_m: 0.4227 - val_loss: 0.0304 - val_f1_m: 0.5045 - val_recall_m: 0.3869\n",
            "Epoch 318/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0289 - f1_m: 0.5313 - recall_m: 0.4065 - val_loss: 0.0303 - val_f1_m: 0.5159 - val_recall_m: 0.4027\n",
            "Epoch 319/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0288 - f1_m: 0.5399 - recall_m: 0.4195 - val_loss: 0.0303 - val_f1_m: 0.5114 - val_recall_m: 0.3967\n",
            "Epoch 320/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0288 - f1_m: 0.5381 - recall_m: 0.4158 - val_loss: 0.0303 - val_f1_m: 0.5115 - val_recall_m: 0.3958\n",
            "Epoch 321/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0288 - f1_m: 0.5378 - recall_m: 0.4146 - val_loss: 0.0303 - val_f1_m: 0.5153 - val_recall_m: 0.4032\n",
            "Epoch 322/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0287 - f1_m: 0.5422 - recall_m: 0.4222 - val_loss: 0.0303 - val_f1_m: 0.5055 - val_recall_m: 0.3884\n",
            "Epoch 323/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0287 - f1_m: 0.5357 - recall_m: 0.4106 - val_loss: 0.0302 - val_f1_m: 0.5218 - val_recall_m: 0.4120\n",
            "Epoch 324/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0287 - f1_m: 0.5464 - recall_m: 0.4284 - val_loss: 0.0302 - val_f1_m: 0.5036 - val_recall_m: 0.3844\n",
            "Epoch 325/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0287 - f1_m: 0.5327 - recall_m: 0.4062 - val_loss: 0.0302 - val_f1_m: 0.5250 - val_recall_m: 0.4160\n",
            "Epoch 326/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0286 - f1_m: 0.5488 - recall_m: 0.4321 - val_loss: 0.0302 - val_f1_m: 0.5035 - val_recall_m: 0.3849\n",
            "Epoch 327/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0286 - f1_m: 0.5352 - recall_m: 0.4091 - val_loss: 0.0301 - val_f1_m: 0.5242 - val_recall_m: 0.4130\n",
            "Epoch 328/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0285 - f1_m: 0.5481 - recall_m: 0.4289 - val_loss: 0.0301 - val_f1_m: 0.5176 - val_recall_m: 0.4027\n",
            "Epoch 329/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0285 - f1_m: 0.5425 - recall_m: 0.4204 - val_loss: 0.0301 - val_f1_m: 0.5153 - val_recall_m: 0.3992\n",
            "Epoch 330/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0285 - f1_m: 0.5417 - recall_m: 0.4177 - val_loss: 0.0300 - val_f1_m: 0.5254 - val_recall_m: 0.4150\n",
            "Epoch 331/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0284 - f1_m: 0.5505 - recall_m: 0.4323 - val_loss: 0.0300 - val_f1_m: 0.5114 - val_recall_m: 0.3933\n",
            "Epoch 332/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0284 - f1_m: 0.5377 - recall_m: 0.4115 - val_loss: 0.0300 - val_f1_m: 0.5275 - val_recall_m: 0.4184\n",
            "Epoch 333/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0284 - f1_m: 0.5542 - recall_m: 0.4376 - val_loss: 0.0300 - val_f1_m: 0.5128 - val_recall_m: 0.3953\n",
            "Epoch 334/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0283 - f1_m: 0.5394 - recall_m: 0.4139 - val_loss: 0.0299 - val_f1_m: 0.5256 - val_recall_m: 0.4145\n",
            "Epoch 335/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0283 - f1_m: 0.5520 - recall_m: 0.4332 - val_loss: 0.0299 - val_f1_m: 0.5230 - val_recall_m: 0.4091\n",
            "Epoch 336/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0283 - f1_m: 0.5482 - recall_m: 0.4262 - val_loss: 0.0299 - val_f1_m: 0.5207 - val_recall_m: 0.4056\n",
            "Epoch 337/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0282 - f1_m: 0.5467 - recall_m: 0.4237 - val_loss: 0.0299 - val_f1_m: 0.5271 - val_recall_m: 0.4174\n",
            "Epoch 338/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0282 - f1_m: 0.5566 - recall_m: 0.4390 - val_loss: 0.0299 - val_f1_m: 0.5125 - val_recall_m: 0.3948\n",
            "Epoch 339/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0282 - f1_m: 0.5420 - recall_m: 0.4162 - val_loss: 0.0298 - val_f1_m: 0.5299 - val_recall_m: 0.4219\n",
            "Epoch 340/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0281 - f1_m: 0.5595 - recall_m: 0.4432 - val_loss: 0.0298 - val_f1_m: 0.5141 - val_recall_m: 0.3958\n",
            "Epoch 341/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0281 - f1_m: 0.5432 - recall_m: 0.4165 - val_loss: 0.0298 - val_f1_m: 0.5298 - val_recall_m: 0.4209\n",
            "Epoch 342/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0281 - f1_m: 0.5596 - recall_m: 0.4426 - val_loss: 0.0298 - val_f1_m: 0.5184 - val_recall_m: 0.4027\n",
            "Epoch 343/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0280 - f1_m: 0.5493 - recall_m: 0.4252 - val_loss: 0.0297 - val_f1_m: 0.5273 - val_recall_m: 0.4165\n",
            "Epoch 344/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0280 - f1_m: 0.5567 - recall_m: 0.4368 - val_loss: 0.0297 - val_f1_m: 0.5270 - val_recall_m: 0.4155\n",
            "Epoch 345/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0280 - f1_m: 0.5561 - recall_m: 0.4356 - val_loss: 0.0297 - val_f1_m: 0.5228 - val_recall_m: 0.4076\n",
            "Epoch 346/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0279 - f1_m: 0.5525 - recall_m: 0.4292 - val_loss: 0.0297 - val_f1_m: 0.5333 - val_recall_m: 0.4239\n",
            "Epoch 347/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0279 - f1_m: 0.5628 - recall_m: 0.4449 - val_loss: 0.0297 - val_f1_m: 0.5158 - val_recall_m: 0.3982\n",
            "Epoch 348/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0279 - f1_m: 0.5477 - recall_m: 0.4217 - val_loss: 0.0296 - val_f1_m: 0.5388 - val_recall_m: 0.4312\n",
            "Epoch 349/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0279 - f1_m: 0.5679 - recall_m: 0.4537 - val_loss: 0.0296 - val_f1_m: 0.5127 - val_recall_m: 0.3918\n",
            "Epoch 350/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0279 - f1_m: 0.5414 - recall_m: 0.4125 - val_loss: 0.0296 - val_f1_m: 0.5414 - val_recall_m: 0.4367\n",
            "Epoch 351/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0278 - f1_m: 0.5712 - recall_m: 0.4600 - val_loss: 0.0296 - val_f1_m: 0.5114 - val_recall_m: 0.3908\n",
            "Epoch 352/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0278 - f1_m: 0.5427 - recall_m: 0.4137 - val_loss: 0.0296 - val_f1_m: 0.5388 - val_recall_m: 0.4312\n",
            "Epoch 353/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0277 - f1_m: 0.5696 - recall_m: 0.4546 - val_loss: 0.0295 - val_f1_m: 0.5289 - val_recall_m: 0.4155\n",
            "Epoch 354/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0277 - f1_m: 0.5603 - recall_m: 0.4384 - val_loss: 0.0295 - val_f1_m: 0.5251 - val_recall_m: 0.4105\n",
            "Epoch 355/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0276 - f1_m: 0.5552 - recall_m: 0.4310 - val_loss: 0.0295 - val_f1_m: 0.5413 - val_recall_m: 0.4342\n",
            "Epoch 356/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0276 - f1_m: 0.5710 - recall_m: 0.4570 - val_loss: 0.0295 - val_f1_m: 0.5176 - val_recall_m: 0.3987\n",
            "Epoch 357/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0276 - f1_m: 0.5475 - recall_m: 0.4198 - val_loss: 0.0295 - val_f1_m: 0.5438 - val_recall_m: 0.4377\n",
            "Epoch 358/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0276 - f1_m: 0.5723 - recall_m: 0.4583 - val_loss: 0.0294 - val_f1_m: 0.5277 - val_recall_m: 0.4130\n",
            "Epoch 359/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0275 - f1_m: 0.5588 - recall_m: 0.4355 - val_loss: 0.0294 - val_f1_m: 0.5320 - val_recall_m: 0.4199\n",
            "Epoch 360/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0275 - f1_m: 0.5638 - recall_m: 0.4427 - val_loss: 0.0294 - val_f1_m: 0.5425 - val_recall_m: 0.4337\n",
            "Epoch 361/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0275 - f1_m: 0.5717 - recall_m: 0.4553 - val_loss: 0.0294 - val_f1_m: 0.5245 - val_recall_m: 0.4081\n",
            "Epoch 362/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0274 - f1_m: 0.5537 - recall_m: 0.4278 - val_loss: 0.0294 - val_f1_m: 0.5481 - val_recall_m: 0.4421\n",
            "Epoch 363/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0274 - f1_m: 0.5761 - recall_m: 0.4622 - val_loss: 0.0293 - val_f1_m: 0.5266 - val_recall_m: 0.4115\n",
            "Epoch 364/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0274 - f1_m: 0.5563 - recall_m: 0.4313 - val_loss: 0.0293 - val_f1_m: 0.5454 - val_recall_m: 0.4367\n",
            "Epoch 365/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0273 - f1_m: 0.5738 - recall_m: 0.4570 - val_loss: 0.0293 - val_f1_m: 0.5355 - val_recall_m: 0.4239\n",
            "Epoch 366/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0273 - f1_m: 0.5676 - recall_m: 0.4463 - val_loss: 0.0292 - val_f1_m: 0.5325 - val_recall_m: 0.4204\n",
            "Epoch 367/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0273 - f1_m: 0.5666 - recall_m: 0.4447 - val_loss: 0.0292 - val_f1_m: 0.5467 - val_recall_m: 0.4386\n",
            "Epoch 368/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0272 - f1_m: 0.5754 - recall_m: 0.4592 - val_loss: 0.0292 - val_f1_m: 0.5266 - val_recall_m: 0.4115\n",
            "Epoch 369/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0272 - f1_m: 0.5598 - recall_m: 0.4349 - val_loss: 0.0292 - val_f1_m: 0.5506 - val_recall_m: 0.4450\n",
            "Epoch 370/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0272 - f1_m: 0.5793 - recall_m: 0.4654 - val_loss: 0.0292 - val_f1_m: 0.5231 - val_recall_m: 0.4071\n",
            "Epoch 371/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0272 - f1_m: 0.5584 - recall_m: 0.4326 - val_loss: 0.0292 - val_f1_m: 0.5502 - val_recall_m: 0.4446\n",
            "Epoch 372/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0271 - f1_m: 0.5791 - recall_m: 0.4649 - val_loss: 0.0291 - val_f1_m: 0.5346 - val_recall_m: 0.4209\n",
            "Epoch 373/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0271 - f1_m: 0.5648 - recall_m: 0.4412 - val_loss: 0.0291 - val_f1_m: 0.5444 - val_recall_m: 0.4352\n",
            "Epoch 374/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0270 - f1_m: 0.5750 - recall_m: 0.4568 - val_loss: 0.0291 - val_f1_m: 0.5420 - val_recall_m: 0.4312\n",
            "Epoch 375/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0270 - f1_m: 0.5735 - recall_m: 0.4538 - val_loss: 0.0290 - val_f1_m: 0.5398 - val_recall_m: 0.4278\n",
            "Epoch 376/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0270 - f1_m: 0.5719 - recall_m: 0.4512 - val_loss: 0.0290 - val_f1_m: 0.5504 - val_recall_m: 0.4441\n",
            "Epoch 377/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0270 - f1_m: 0.5796 - recall_m: 0.4636 - val_loss: 0.0290 - val_f1_m: 0.5345 - val_recall_m: 0.4199\n",
            "Epoch 378/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0269 - f1_m: 0.5663 - recall_m: 0.4418 - val_loss: 0.0290 - val_f1_m: 0.5546 - val_recall_m: 0.4519\n",
            "Epoch 379/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0269 - f1_m: 0.5850 - recall_m: 0.4726 - val_loss: 0.0290 - val_f1_m: 0.5249 - val_recall_m: 0.4076\n",
            "Epoch 380/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0269 - f1_m: 0.5630 - recall_m: 0.4357 - val_loss: 0.0290 - val_f1_m: 0.5579 - val_recall_m: 0.4569\n",
            "Epoch 381/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0269 - f1_m: 0.5892 - recall_m: 0.4793 - val_loss: 0.0290 - val_f1_m: 0.5242 - val_recall_m: 0.4061\n",
            "Epoch 382/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0269 - f1_m: 0.5617 - recall_m: 0.4334 - val_loss: 0.0289 - val_f1_m: 0.5563 - val_recall_m: 0.4539\n",
            "Epoch 383/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0268 - f1_m: 0.5885 - recall_m: 0.4771 - val_loss: 0.0289 - val_f1_m: 0.5372 - val_recall_m: 0.4239\n",
            "Epoch 384/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0267 - f1_m: 0.5725 - recall_m: 0.4491 - val_loss: 0.0289 - val_f1_m: 0.5487 - val_recall_m: 0.4411\n",
            "Epoch 385/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0267 - f1_m: 0.5817 - recall_m: 0.4644 - val_loss: 0.0288 - val_f1_m: 0.5493 - val_recall_m: 0.4421\n",
            "Epoch 386/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0267 - f1_m: 0.5823 - recall_m: 0.4654 - val_loss: 0.0288 - val_f1_m: 0.5409 - val_recall_m: 0.4273\n",
            "Epoch 387/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0267 - f1_m: 0.5735 - recall_m: 0.4499 - val_loss: 0.0288 - val_f1_m: 0.5577 - val_recall_m: 0.4559\n",
            "Epoch 388/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0266 - f1_m: 0.5894 - recall_m: 0.4782 - val_loss: 0.0288 - val_f1_m: 0.5305 - val_recall_m: 0.4135\n",
            "Epoch 389/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0266 - f1_m: 0.5667 - recall_m: 0.4396 - val_loss: 0.0289 - val_f1_m: 0.5630 - val_recall_m: 0.4648\n",
            "Epoch 390/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0266 - f1_m: 0.5952 - recall_m: 0.4881 - val_loss: 0.0288 - val_f1_m: 0.5286 - val_recall_m: 0.4115\n",
            "Epoch 391/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0266 - f1_m: 0.5665 - recall_m: 0.4387 - val_loss: 0.0288 - val_f1_m: 0.5623 - val_recall_m: 0.4628\n",
            "Epoch 392/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0265 - f1_m: 0.5938 - recall_m: 0.4854 - val_loss: 0.0287 - val_f1_m: 0.5400 - val_recall_m: 0.4263\n",
            "Epoch 393/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0265 - f1_m: 0.5743 - recall_m: 0.4501 - val_loss: 0.0287 - val_f1_m: 0.5521 - val_recall_m: 0.4450\n",
            "Epoch 394/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0264 - f1_m: 0.5868 - recall_m: 0.4707 - val_loss: 0.0287 - val_f1_m: 0.5521 - val_recall_m: 0.4450\n",
            "Epoch 395/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0264 - f1_m: 0.5869 - recall_m: 0.4707 - val_loss: 0.0287 - val_f1_m: 0.5414 - val_recall_m: 0.4288\n",
            "Epoch 396/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0264 - f1_m: 0.5768 - recall_m: 0.4538 - val_loss: 0.0287 - val_f1_m: 0.5635 - val_recall_m: 0.4633\n",
            "Epoch 397/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0264 - f1_m: 0.5948 - recall_m: 0.4852 - val_loss: 0.0287 - val_f1_m: 0.5375 - val_recall_m: 0.4219\n",
            "Epoch 398/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0264 - f1_m: 0.5734 - recall_m: 0.4471 - val_loss: 0.0287 - val_f1_m: 0.5679 - val_recall_m: 0.4702\n",
            "Epoch 399/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0264 - f1_m: 0.5981 - recall_m: 0.4924 - val_loss: 0.0286 - val_f1_m: 0.5363 - val_recall_m: 0.4204\n",
            "Epoch 400/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0263 - f1_m: 0.5713 - recall_m: 0.4442 - val_loss: 0.0286 - val_f1_m: 0.5636 - val_recall_m: 0.4643\n",
            "Epoch 401/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0263 - f1_m: 0.5955 - recall_m: 0.4869 - val_loss: 0.0285 - val_f1_m: 0.5445 - val_recall_m: 0.4327\n",
            "Epoch 402/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0262 - f1_m: 0.5793 - recall_m: 0.4573 - val_loss: 0.0285 - val_f1_m: 0.5556 - val_recall_m: 0.4480\n",
            "Epoch 403/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0262 - f1_m: 0.5885 - recall_m: 0.4717 - val_loss: 0.0285 - val_f1_m: 0.5621 - val_recall_m: 0.4593\n",
            "Epoch 404/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0261 - f1_m: 0.5927 - recall_m: 0.4798 - val_loss: 0.0285 - val_f1_m: 0.5444 - val_recall_m: 0.4317\n",
            "Epoch 405/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0261 - f1_m: 0.5794 - recall_m: 0.4560 - val_loss: 0.0285 - val_f1_m: 0.5678 - val_recall_m: 0.4707\n",
            "Epoch 406/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0261 - f1_m: 0.6003 - recall_m: 0.4936 - val_loss: 0.0285 - val_f1_m: 0.5387 - val_recall_m: 0.4224\n",
            "Epoch 407/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0261 - f1_m: 0.5753 - recall_m: 0.4485 - val_loss: 0.0285 - val_f1_m: 0.5725 - val_recall_m: 0.4776\n",
            "Epoch 408/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0261 - f1_m: 0.6039 - recall_m: 0.4996 - val_loss: 0.0285 - val_f1_m: 0.5426 - val_recall_m: 0.4273\n",
            "Epoch 409/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0261 - f1_m: 0.5769 - recall_m: 0.4511 - val_loss: 0.0284 - val_f1_m: 0.5675 - val_recall_m: 0.4702\n",
            "Epoch 410/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0260 - f1_m: 0.6006 - recall_m: 0.4929 - val_loss: 0.0284 - val_f1_m: 0.5517 - val_recall_m: 0.4421\n",
            "Epoch 411/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0259 - f1_m: 0.5874 - recall_m: 0.4675 - val_loss: 0.0284 - val_f1_m: 0.5622 - val_recall_m: 0.4579\n",
            "Epoch 412/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0259 - f1_m: 0.5943 - recall_m: 0.4799 - val_loss: 0.0283 - val_f1_m: 0.5639 - val_recall_m: 0.4618\n",
            "Epoch 413/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0259 - f1_m: 0.5961 - recall_m: 0.4836 - val_loss: 0.0283 - val_f1_m: 0.5516 - val_recall_m: 0.4411\n",
            "Epoch 414/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0259 - f1_m: 0.5875 - recall_m: 0.4667 - val_loss: 0.0284 - val_f1_m: 0.5693 - val_recall_m: 0.4726\n",
            "Epoch 415/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0259 - f1_m: 0.6035 - recall_m: 0.4967 - val_loss: 0.0284 - val_f1_m: 0.5474 - val_recall_m: 0.4312\n",
            "Epoch 416/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0259 - f1_m: 0.5798 - recall_m: 0.4541 - val_loss: 0.0284 - val_f1_m: 0.5783 - val_recall_m: 0.4850\n",
            "Epoch 417/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0259 - f1_m: 0.6093 - recall_m: 0.5073 - val_loss: 0.0284 - val_f1_m: 0.5448 - val_recall_m: 0.4268\n",
            "Epoch 418/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0259 - f1_m: 0.5764 - recall_m: 0.4482 - val_loss: 0.0284 - val_f1_m: 0.5777 - val_recall_m: 0.4845\n",
            "Epoch 419/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0258 - f1_m: 0.6106 - recall_m: 0.5079 - val_loss: 0.0283 - val_f1_m: 0.5518 - val_recall_m: 0.4386\n",
            "Epoch 420/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0257 - f1_m: 0.5851 - recall_m: 0.4613 - val_loss: 0.0282 - val_f1_m: 0.5684 - val_recall_m: 0.4697\n",
            "Epoch 421/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0256 - f1_m: 0.6038 - recall_m: 0.4945 - val_loss: 0.0282 - val_f1_m: 0.5672 - val_recall_m: 0.4657\n",
            "Epoch 422/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0256 - f1_m: 0.6010 - recall_m: 0.4884 - val_loss: 0.0282 - val_f1_m: 0.5578 - val_recall_m: 0.4480\n",
            "Epoch 423/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0256 - f1_m: 0.5909 - recall_m: 0.4704 - val_loss: 0.0282 - val_f1_m: 0.5727 - val_recall_m: 0.4776\n",
            "Epoch 424/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0256 - f1_m: 0.6085 - recall_m: 0.5034 - val_loss: 0.0282 - val_f1_m: 0.5513 - val_recall_m: 0.4342\n",
            "Epoch 425/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0256 - f1_m: 0.5827 - recall_m: 0.4562 - val_loss: 0.0282 - val_f1_m: 0.5772 - val_recall_m: 0.4845\n",
            "Epoch 426/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0256 - f1_m: 0.6134 - recall_m: 0.5114 - val_loss: 0.0282 - val_f1_m: 0.5527 - val_recall_m: 0.4357\n",
            "Epoch 427/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0256 - f1_m: 0.5846 - recall_m: 0.4586 - val_loss: 0.0281 - val_f1_m: 0.5716 - val_recall_m: 0.4761\n",
            "Epoch 428/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0255 - f1_m: 0.6092 - recall_m: 0.5030 - val_loss: 0.0281 - val_f1_m: 0.5641 - val_recall_m: 0.4588\n",
            "Epoch 429/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0254 - f1_m: 0.5974 - recall_m: 0.4804 - val_loss: 0.0280 - val_f1_m: 0.5672 - val_recall_m: 0.4638\n",
            "Epoch 430/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0254 - f1_m: 0.6012 - recall_m: 0.4860 - val_loss: 0.0281 - val_f1_m: 0.5696 - val_recall_m: 0.4722\n",
            "Epoch 431/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0254 - f1_m: 0.6101 - recall_m: 0.5021 - val_loss: 0.0280 - val_f1_m: 0.5602 - val_recall_m: 0.4480\n",
            "Epoch 432/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0254 - f1_m: 0.5931 - recall_m: 0.4705 - val_loss: 0.0281 - val_f1_m: 0.5757 - val_recall_m: 0.4825\n",
            "Epoch 433/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0254 - f1_m: 0.6158 - recall_m: 0.5135 - val_loss: 0.0281 - val_f1_m: 0.5527 - val_recall_m: 0.4367\n",
            "Epoch 434/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0254 - f1_m: 0.5860 - recall_m: 0.4597 - val_loss: 0.0281 - val_f1_m: 0.5769 - val_recall_m: 0.4845\n",
            "Epoch 435/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0253 - f1_m: 0.6167 - recall_m: 0.5151 - val_loss: 0.0280 - val_f1_m: 0.5612 - val_recall_m: 0.4475\n",
            "Epoch 436/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0253 - f1_m: 0.5927 - recall_m: 0.4694 - val_loss: 0.0280 - val_f1_m: 0.5720 - val_recall_m: 0.4756\n",
            "Epoch 437/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0252 - f1_m: 0.6125 - recall_m: 0.5061 - val_loss: 0.0279 - val_f1_m: 0.5673 - val_recall_m: 0.4633\n",
            "Epoch 438/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0252 - f1_m: 0.6043 - recall_m: 0.4891 - val_loss: 0.0279 - val_f1_m: 0.5669 - val_recall_m: 0.4633\n",
            "Epoch 439/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0251 - f1_m: 0.6047 - recall_m: 0.4898 - val_loss: 0.0279 - val_f1_m: 0.5712 - val_recall_m: 0.4736\n",
            "Epoch 440/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0251 - f1_m: 0.6133 - recall_m: 0.5053 - val_loss: 0.0279 - val_f1_m: 0.5626 - val_recall_m: 0.4519\n",
            "Epoch 441/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0251 - f1_m: 0.5961 - recall_m: 0.4745 - val_loss: 0.0279 - val_f1_m: 0.5784 - val_recall_m: 0.4855\n",
            "Epoch 442/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0251 - f1_m: 0.6192 - recall_m: 0.5174 - val_loss: 0.0279 - val_f1_m: 0.5556 - val_recall_m: 0.4396\n",
            "Epoch 443/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0251 - f1_m: 0.5887 - recall_m: 0.4619 - val_loss: 0.0280 - val_f1_m: 0.5811 - val_recall_m: 0.4919\n",
            "Epoch 444/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0251 - f1_m: 0.6217 - recall_m: 0.5236 - val_loss: 0.0279 - val_f1_m: 0.5539 - val_recall_m: 0.4367\n",
            "Epoch 445/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0251 - f1_m: 0.5884 - recall_m: 0.4602 - val_loss: 0.0279 - val_f1_m: 0.5801 - val_recall_m: 0.4879\n",
            "Epoch 446/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0250 - f1_m: 0.6205 - recall_m: 0.5194 - val_loss: 0.0278 - val_f1_m: 0.5660 - val_recall_m: 0.4564\n",
            "Epoch 447/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0249 - f1_m: 0.6033 - recall_m: 0.4833 - val_loss: 0.0277 - val_f1_m: 0.5701 - val_recall_m: 0.4697\n",
            "Epoch 448/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0249 - f1_m: 0.6121 - recall_m: 0.5007 - val_loss: 0.0277 - val_f1_m: 0.5703 - val_recall_m: 0.4707\n",
            "Epoch 449/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0249 - f1_m: 0.6137 - recall_m: 0.5028 - val_loss: 0.0277 - val_f1_m: 0.5673 - val_recall_m: 0.4588\n",
            "Epoch 450/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0249 - f1_m: 0.6045 - recall_m: 0.4853 - val_loss: 0.0278 - val_f1_m: 0.5805 - val_recall_m: 0.4879\n",
            "Epoch 451/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0249 - f1_m: 0.6229 - recall_m: 0.5212 - val_loss: 0.0278 - val_f1_m: 0.5593 - val_recall_m: 0.4441\n",
            "Epoch 452/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0249 - f1_m: 0.5950 - recall_m: 0.4701 - val_loss: 0.0278 - val_f1_m: 0.5839 - val_recall_m: 0.4938\n",
            "Epoch 453/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0249 - f1_m: 0.6259 - recall_m: 0.5285 - val_loss: 0.0278 - val_f1_m: 0.5528 - val_recall_m: 0.4347\n",
            "Epoch 454/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0249 - f1_m: 0.5910 - recall_m: 0.4628 - val_loss: 0.0278 - val_f1_m: 0.5815 - val_recall_m: 0.4899\n",
            "Epoch 455/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0248 - f1_m: 0.6252 - recall_m: 0.5263 - val_loss: 0.0277 - val_f1_m: 0.5664 - val_recall_m: 0.4559\n",
            "Epoch 456/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0247 - f1_m: 0.6039 - recall_m: 0.4824 - val_loss: 0.0276 - val_f1_m: 0.5784 - val_recall_m: 0.4800\n",
            "Epoch 457/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0246 - f1_m: 0.6216 - recall_m: 0.5136 - val_loss: 0.0276 - val_f1_m: 0.5765 - val_recall_m: 0.4766\n",
            "Epoch 458/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0246 - f1_m: 0.6198 - recall_m: 0.5094 - val_loss: 0.0276 - val_f1_m: 0.5727 - val_recall_m: 0.4657\n",
            "Epoch 459/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0246 - f1_m: 0.6102 - recall_m: 0.4930 - val_loss: 0.0276 - val_f1_m: 0.5830 - val_recall_m: 0.4899\n",
            "Epoch 460/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0246 - f1_m: 0.6271 - recall_m: 0.5253 - val_loss: 0.0276 - val_f1_m: 0.5591 - val_recall_m: 0.4441\n",
            "Epoch 461/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0246 - f1_m: 0.5996 - recall_m: 0.4747 - val_loss: 0.0277 - val_f1_m: 0.5852 - val_recall_m: 0.4968\n",
            "Epoch 462/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0246 - f1_m: 0.6296 - recall_m: 0.5332 - val_loss: 0.0276 - val_f1_m: 0.5535 - val_recall_m: 0.4362\n",
            "Epoch 463/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0246 - f1_m: 0.5948 - recall_m: 0.4671 - val_loss: 0.0276 - val_f1_m: 0.5879 - val_recall_m: 0.4988\n",
            "Epoch 464/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0246 - f1_m: 0.6304 - recall_m: 0.5325 - val_loss: 0.0275 - val_f1_m: 0.5681 - val_recall_m: 0.4574\n",
            "Epoch 465/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0245 - f1_m: 0.6086 - recall_m: 0.4874 - val_loss: 0.0275 - val_f1_m: 0.5811 - val_recall_m: 0.4845\n",
            "Epoch 466/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0244 - f1_m: 0.6262 - recall_m: 0.5205 - val_loss: 0.0274 - val_f1_m: 0.5785 - val_recall_m: 0.4766\n",
            "Epoch 467/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0244 - f1_m: 0.6228 - recall_m: 0.5113 - val_loss: 0.0274 - val_f1_m: 0.5778 - val_recall_m: 0.4731\n",
            "Epoch 468/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0244 - f1_m: 0.6186 - recall_m: 0.5038 - val_loss: 0.0275 - val_f1_m: 0.5858 - val_recall_m: 0.4924\n",
            "Epoch 469/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0244 - f1_m: 0.6300 - recall_m: 0.5274 - val_loss: 0.0275 - val_f1_m: 0.5691 - val_recall_m: 0.4579\n",
            "Epoch 470/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0244 - f1_m: 0.6101 - recall_m: 0.4884 - val_loss: 0.0276 - val_f1_m: 0.5914 - val_recall_m: 0.5037\n",
            "Epoch 471/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0244 - f1_m: 0.6339 - recall_m: 0.5372 - val_loss: 0.0275 - val_f1_m: 0.5609 - val_recall_m: 0.4450\n",
            "Epoch 472/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0244 - f1_m: 0.6026 - recall_m: 0.4764 - val_loss: 0.0276 - val_f1_m: 0.5929 - val_recall_m: 0.5081\n",
            "Epoch 473/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0244 - f1_m: 0.6375 - recall_m: 0.5440 - val_loss: 0.0275 - val_f1_m: 0.5623 - val_recall_m: 0.4470\n",
            "Epoch 474/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0243 - f1_m: 0.6060 - recall_m: 0.4809 - val_loss: 0.0274 - val_f1_m: 0.5895 - val_recall_m: 0.4998\n",
            "Epoch 475/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0243 - f1_m: 0.6341 - recall_m: 0.5350 - val_loss: 0.0273 - val_f1_m: 0.5782 - val_recall_m: 0.4726\n",
            "Epoch 476/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0242 - f1_m: 0.6209 - recall_m: 0.5049 - val_loss: 0.0273 - val_f1_m: 0.5857 - val_recall_m: 0.4860\n",
            "Epoch 477/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0241 - f1_m: 0.6292 - recall_m: 0.5202 - val_loss: 0.0273 - val_f1_m: 0.5876 - val_recall_m: 0.4899\n",
            "Epoch 478/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0241 - f1_m: 0.6320 - recall_m: 0.5262 - val_loss: 0.0273 - val_f1_m: 0.5775 - val_recall_m: 0.4692\n",
            "Epoch 479/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0241 - f1_m: 0.6191 - recall_m: 0.5014 - val_loss: 0.0274 - val_f1_m: 0.5932 - val_recall_m: 0.5057\n",
            "Epoch 480/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0241 - f1_m: 0.6391 - recall_m: 0.5422 - val_loss: 0.0274 - val_f1_m: 0.5669 - val_recall_m: 0.4529\n",
            "Epoch 481/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0242 - f1_m: 0.6095 - recall_m: 0.4852 - val_loss: 0.0274 - val_f1_m: 0.5973 - val_recall_m: 0.5136\n",
            "Epoch 482/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0242 - f1_m: 0.6417 - recall_m: 0.5490 - val_loss: 0.0274 - val_f1_m: 0.5620 - val_recall_m: 0.4465\n",
            "Epoch 483/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0242 - f1_m: 0.6046 - recall_m: 0.4775 - val_loss: 0.0274 - val_f1_m: 0.5975 - val_recall_m: 0.5136\n",
            "Epoch 484/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0241 - f1_m: 0.6429 - recall_m: 0.5503 - val_loss: 0.0273 - val_f1_m: 0.5740 - val_recall_m: 0.4628\n",
            "Epoch 485/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0240 - f1_m: 0.6146 - recall_m: 0.4931 - val_loss: 0.0273 - val_f1_m: 0.5953 - val_recall_m: 0.5042\n",
            "Epoch 486/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0240 - f1_m: 0.6399 - recall_m: 0.5401 - val_loss: 0.0272 - val_f1_m: 0.5860 - val_recall_m: 0.4845\n",
            "Epoch 487/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0239 - f1_m: 0.6311 - recall_m: 0.5205 - val_loss: 0.0272 - val_f1_m: 0.5854 - val_recall_m: 0.4830\n",
            "Epoch 488/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0239 - f1_m: 0.6307 - recall_m: 0.5195 - val_loss: 0.0272 - val_f1_m: 0.5936 - val_recall_m: 0.5017\n",
            "Epoch 489/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0239 - f1_m: 0.6400 - recall_m: 0.5400 - val_loss: 0.0272 - val_f1_m: 0.5715 - val_recall_m: 0.4598\n",
            "Epoch 490/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0239 - f1_m: 0.6162 - recall_m: 0.4945 - val_loss: 0.0273 - val_f1_m: 0.5981 - val_recall_m: 0.5145\n",
            "Epoch 491/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0239 - f1_m: 0.6467 - recall_m: 0.5536 - val_loss: 0.0273 - val_f1_m: 0.5690 - val_recall_m: 0.4539\n",
            "Epoch 492/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0240 - f1_m: 0.6112 - recall_m: 0.4856 - val_loss: 0.0273 - val_f1_m: 0.6032 - val_recall_m: 0.5234\n",
            "Epoch 493/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0239 - f1_m: 0.6536 - recall_m: 0.5643 - val_loss: 0.0272 - val_f1_m: 0.5683 - val_recall_m: 0.4539\n",
            "Epoch 494/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0239 - f1_m: 0.6108 - recall_m: 0.4856 - val_loss: 0.0272 - val_f1_m: 0.5976 - val_recall_m: 0.5116\n",
            "Epoch 495/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0238 - f1_m: 0.6479 - recall_m: 0.5534 - val_loss: 0.0271 - val_f1_m: 0.5850 - val_recall_m: 0.4791\n",
            "Epoch 496/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0237 - f1_m: 0.6275 - recall_m: 0.5111 - val_loss: 0.0271 - val_f1_m: 0.5922 - val_recall_m: 0.4948\n",
            "Epoch 497/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0237 - f1_m: 0.6407 - recall_m: 0.5358 - val_loss: 0.0271 - val_f1_m: 0.5944 - val_recall_m: 0.4983\n",
            "Epoch 498/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0236 - f1_m: 0.6419 - recall_m: 0.5382 - val_loss: 0.0271 - val_f1_m: 0.5826 - val_recall_m: 0.4761\n",
            "Epoch 499/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0236 - f1_m: 0.6300 - recall_m: 0.5138 - val_loss: 0.0271 - val_f1_m: 0.5995 - val_recall_m: 0.5136\n",
            "Epoch 500/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0237 - f1_m: 0.6510 - recall_m: 0.5564 - val_loss: 0.0271 - val_f1_m: 0.5727 - val_recall_m: 0.4608\n",
            "Epoch 501/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0237 - f1_m: 0.6176 - recall_m: 0.4947 - val_loss: 0.0272 - val_f1_m: 0.6059 - val_recall_m: 0.5244\n",
            "Epoch 502/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0237 - f1_m: 0.6567 - recall_m: 0.5670 - val_loss: 0.0271 - val_f1_m: 0.5706 - val_recall_m: 0.4559\n",
            "Epoch 503/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0237 - f1_m: 0.6130 - recall_m: 0.4872 - val_loss: 0.0272 - val_f1_m: 0.6068 - val_recall_m: 0.5249\n",
            "Epoch 504/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0236 - f1_m: 0.6578 - recall_m: 0.5682 - val_loss: 0.0270 - val_f1_m: 0.5767 - val_recall_m: 0.4667\n",
            "Epoch 505/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0236 - f1_m: 0.6237 - recall_m: 0.5023 - val_loss: 0.0270 - val_f1_m: 0.5976 - val_recall_m: 0.5086\n",
            "Epoch 506/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0235 - f1_m: 0.6501 - recall_m: 0.5526 - val_loss: 0.0270 - val_f1_m: 0.5909 - val_recall_m: 0.4904\n",
            "Epoch 507/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0234 - f1_m: 0.6410 - recall_m: 0.5314 - val_loss: 0.0269 - val_f1_m: 0.5922 - val_recall_m: 0.4929\n",
            "Epoch 508/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0234 - f1_m: 0.6429 - recall_m: 0.5343 - val_loss: 0.0270 - val_f1_m: 0.5983 - val_recall_m: 0.5076\n",
            "Epoch 509/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0234 - f1_m: 0.6513 - recall_m: 0.5528 - val_loss: 0.0270 - val_f1_m: 0.5828 - val_recall_m: 0.4761\n",
            "Epoch 510/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0234 - f1_m: 0.6318 - recall_m: 0.5142 - val_loss: 0.0271 - val_f1_m: 0.6060 - val_recall_m: 0.5234\n",
            "Epoch 511/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0235 - f1_m: 0.6603 - recall_m: 0.5708 - val_loss: 0.0271 - val_f1_m: 0.5732 - val_recall_m: 0.4613\n",
            "Epoch 512/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0235 - f1_m: 0.6189 - recall_m: 0.4943 - val_loss: 0.0271 - val_f1_m: 0.6084 - val_recall_m: 0.5303\n",
            "Epoch 513/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0235 - f1_m: 0.6640 - recall_m: 0.5796 - val_loss: 0.0271 - val_f1_m: 0.5711 - val_recall_m: 0.4574\n",
            "Epoch 514/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0235 - f1_m: 0.6179 - recall_m: 0.4919 - val_loss: 0.0271 - val_f1_m: 0.6073 - val_recall_m: 0.5274\n",
            "Epoch 515/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0234 - f1_m: 0.6622 - recall_m: 0.5747 - val_loss: 0.0269 - val_f1_m: 0.5809 - val_recall_m: 0.4741\n",
            "Epoch 516/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0233 - f1_m: 0.6332 - recall_m: 0.5148 - val_loss: 0.0269 - val_f1_m: 0.5995 - val_recall_m: 0.5076\n",
            "Epoch 517/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0232 - f1_m: 0.6530 - recall_m: 0.5534 - val_loss: 0.0269 - val_f1_m: 0.5982 - val_recall_m: 0.5042\n",
            "Epoch 518/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0232 - f1_m: 0.6521 - recall_m: 0.5497 - val_loss: 0.0269 - val_f1_m: 0.5902 - val_recall_m: 0.4879\n",
            "Epoch 519/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0232 - f1_m: 0.6414 - recall_m: 0.5287 - val_loss: 0.0270 - val_f1_m: 0.6062 - val_recall_m: 0.5224\n",
            "Epoch 520/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0232 - f1_m: 0.6621 - recall_m: 0.5718 - val_loss: 0.0270 - val_f1_m: 0.5767 - val_recall_m: 0.4662\n",
            "Epoch 521/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0233 - f1_m: 0.6273 - recall_m: 0.5047 - val_loss: 0.0271 - val_f1_m: 0.6126 - val_recall_m: 0.5382\n",
            "Epoch 522/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0233 - f1_m: 0.6684 - recall_m: 0.5867 - val_loss: 0.0270 - val_f1_m: 0.5741 - val_recall_m: 0.4623\n",
            "Epoch 523/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0233 - f1_m: 0.6229 - recall_m: 0.4974 - val_loss: 0.0270 - val_f1_m: 0.6126 - val_recall_m: 0.5343\n",
            "Epoch 524/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0232 - f1_m: 0.6675 - recall_m: 0.5827 - val_loss: 0.0269 - val_f1_m: 0.5831 - val_recall_m: 0.4766\n",
            "Epoch 525/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0231 - f1_m: 0.6342 - recall_m: 0.5155 - val_loss: 0.0268 - val_f1_m: 0.6016 - val_recall_m: 0.5131\n",
            "Epoch 526/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0230 - f1_m: 0.6584 - recall_m: 0.5615 - val_loss: 0.0268 - val_f1_m: 0.5993 - val_recall_m: 0.5057\n",
            "Epoch 527/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0230 - f1_m: 0.6551 - recall_m: 0.5525 - val_loss: 0.0268 - val_f1_m: 0.5898 - val_recall_m: 0.4889\n",
            "Epoch 528/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0230 - f1_m: 0.6447 - recall_m: 0.5327 - val_loss: 0.0269 - val_f1_m: 0.6079 - val_recall_m: 0.5254\n",
            "Epoch 529/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0230 - f1_m: 0.6651 - recall_m: 0.5752 - val_loss: 0.0269 - val_f1_m: 0.5780 - val_recall_m: 0.4692\n",
            "Epoch 530/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0231 - f1_m: 0.6315 - recall_m: 0.5100 - val_loss: 0.0270 - val_f1_m: 0.6138 - val_recall_m: 0.5402\n",
            "Epoch 531/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0231 - f1_m: 0.6714 - recall_m: 0.5911 - val_loss: 0.0269 - val_f1_m: 0.5737 - val_recall_m: 0.4623\n",
            "Epoch 532/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0231 - f1_m: 0.6276 - recall_m: 0.5030 - val_loss: 0.0269 - val_f1_m: 0.6145 - val_recall_m: 0.5382\n",
            "Epoch 533/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0230 - f1_m: 0.6707 - recall_m: 0.5881 - val_loss: 0.0268 - val_f1_m: 0.5818 - val_recall_m: 0.4751\n",
            "Epoch 534/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0229 - f1_m: 0.6368 - recall_m: 0.5180 - val_loss: 0.0268 - val_f1_m: 0.6033 - val_recall_m: 0.5160\n",
            "Epoch 535/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0229 - f1_m: 0.6639 - recall_m: 0.5687 - val_loss: 0.0267 - val_f1_m: 0.5974 - val_recall_m: 0.5042\n",
            "Epoch 536/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0228 - f1_m: 0.6577 - recall_m: 0.5550 - val_loss: 0.0267 - val_f1_m: 0.5929 - val_recall_m: 0.4953\n",
            "Epoch 537/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0228 - f1_m: 0.6524 - recall_m: 0.5445 - val_loss: 0.0268 - val_f1_m: 0.6093 - val_recall_m: 0.5269\n",
            "Epoch 538/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0228 - f1_m: 0.6689 - recall_m: 0.5794 - val_loss: 0.0268 - val_f1_m: 0.5818 - val_recall_m: 0.4741\n",
            "Epoch 539/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0229 - f1_m: 0.6365 - recall_m: 0.5163 - val_loss: 0.0269 - val_f1_m: 0.6186 - val_recall_m: 0.5461\n",
            "Epoch 540/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0229 - f1_m: 0.6738 - recall_m: 0.5932 - val_loss: 0.0268 - val_f1_m: 0.5748 - val_recall_m: 0.4638\n",
            "Epoch 541/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0229 - f1_m: 0.6300 - recall_m: 0.5054 - val_loss: 0.0269 - val_f1_m: 0.6200 - val_recall_m: 0.5500\n",
            "Epoch 542/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0229 - f1_m: 0.6768 - recall_m: 0.5991 - val_loss: 0.0268 - val_f1_m: 0.5784 - val_recall_m: 0.4702\n",
            "Epoch 543/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0228 - f1_m: 0.6359 - recall_m: 0.5141 - val_loss: 0.0268 - val_f1_m: 0.6136 - val_recall_m: 0.5357\n",
            "Epoch 544/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0227 - f1_m: 0.6723 - recall_m: 0.5869 - val_loss: 0.0266 - val_f1_m: 0.5909 - val_recall_m: 0.4919\n",
            "Epoch 545/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0227 - f1_m: 0.6518 - recall_m: 0.5416 - val_loss: 0.0266 - val_f1_m: 0.6039 - val_recall_m: 0.5155\n",
            "Epoch 546/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0226 - f1_m: 0.6661 - recall_m: 0.5696 - val_loss: 0.0266 - val_f1_m: 0.6053 - val_recall_m: 0.5170\n",
            "Epoch 547/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0226 - f1_m: 0.6664 - recall_m: 0.5701 - val_loss: 0.0266 - val_f1_m: 0.5932 - val_recall_m: 0.4958\n",
            "Epoch 548/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0226 - f1_m: 0.6552 - recall_m: 0.5467 - val_loss: 0.0267 - val_f1_m: 0.6169 - val_recall_m: 0.5377\n",
            "Epoch 549/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0226 - f1_m: 0.6760 - recall_m: 0.5896 - val_loss: 0.0267 - val_f1_m: 0.5841 - val_recall_m: 0.4776\n",
            "Epoch 550/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0227 - f1_m: 0.6426 - recall_m: 0.5231 - val_loss: 0.0268 - val_f1_m: 0.6230 - val_recall_m: 0.5530\n",
            "Epoch 551/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0227 - f1_m: 0.6803 - recall_m: 0.6034 - val_loss: 0.0268 - val_f1_m: 0.5761 - val_recall_m: 0.4638\n",
            "Epoch 552/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0228 - f1_m: 0.6326 - recall_m: 0.5065 - val_loss: 0.0269 - val_f1_m: 0.6227 - val_recall_m: 0.5554\n",
            "Epoch 553/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0227 - f1_m: 0.6836 - recall_m: 0.6109 - val_loss: 0.0267 - val_f1_m: 0.5785 - val_recall_m: 0.4687\n",
            "Epoch 554/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0227 - f1_m: 0.6369 - recall_m: 0.5133 - val_loss: 0.0267 - val_f1_m: 0.6208 - val_recall_m: 0.5466\n",
            "Epoch 555/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0225 - f1_m: 0.6789 - recall_m: 0.5966 - val_loss: 0.0265 - val_f1_m: 0.5952 - val_recall_m: 0.4983\n",
            "Epoch 556/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0224 - f1_m: 0.6601 - recall_m: 0.5534 - val_loss: 0.0265 - val_f1_m: 0.6052 - val_recall_m: 0.5145\n",
            "Epoch 557/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0224 - f1_m: 0.6676 - recall_m: 0.5682 - val_loss: 0.0266 - val_f1_m: 0.6154 - val_recall_m: 0.5343\n",
            "Epoch 558/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0224 - f1_m: 0.6758 - recall_m: 0.5858 - val_loss: 0.0266 - val_f1_m: 0.5854 - val_recall_m: 0.4815\n",
            "Epoch 559/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0224 - f1_m: 0.6494 - recall_m: 0.5326 - val_loss: 0.0267 - val_f1_m: 0.6219 - val_recall_m: 0.5520\n",
            "Epoch 560/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0225 - f1_m: 0.6832 - recall_m: 0.6058 - val_loss: 0.0267 - val_f1_m: 0.5822 - val_recall_m: 0.4722\n",
            "Epoch 561/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0226 - f1_m: 0.6400 - recall_m: 0.5164 - val_loss: 0.0268 - val_f1_m: 0.6235 - val_recall_m: 0.5559\n",
            "Epoch 562/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0225 - f1_m: 0.6853 - recall_m: 0.6111 - val_loss: 0.0266 - val_f1_m: 0.5829 - val_recall_m: 0.4746\n",
            "Epoch 563/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0225 - f1_m: 0.6449 - recall_m: 0.5236 - val_loss: 0.0266 - val_f1_m: 0.6193 - val_recall_m: 0.5436\n",
            "Epoch 564/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0223 - f1_m: 0.6813 - recall_m: 0.5976 - val_loss: 0.0264 - val_f1_m: 0.6046 - val_recall_m: 0.5106\n",
            "Epoch 565/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0222 - f1_m: 0.6666 - recall_m: 0.5629 - val_loss: 0.0264 - val_f1_m: 0.6062 - val_recall_m: 0.5140\n",
            "Epoch 566/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0222 - f1_m: 0.6696 - recall_m: 0.5687 - val_loss: 0.0265 - val_f1_m: 0.6161 - val_recall_m: 0.5367\n",
            "Epoch 567/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0222 - f1_m: 0.6788 - recall_m: 0.5904 - val_loss: 0.0265 - val_f1_m: 0.5883 - val_recall_m: 0.4860\n",
            "Epoch 568/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0223 - f1_m: 0.6532 - recall_m: 0.5378 - val_loss: 0.0266 - val_f1_m: 0.6255 - val_recall_m: 0.5559\n",
            "Epoch 569/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0223 - f1_m: 0.6866 - recall_m: 0.6094 - val_loss: 0.0266 - val_f1_m: 0.5825 - val_recall_m: 0.4731\n",
            "Epoch 570/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0224 - f1_m: 0.6445 - recall_m: 0.5222 - val_loss: 0.0267 - val_f1_m: 0.6263 - val_recall_m: 0.5614\n",
            "Epoch 571/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0223 - f1_m: 0.6901 - recall_m: 0.6179 - val_loss: 0.0265 - val_f1_m: 0.5854 - val_recall_m: 0.4781\n",
            "Epoch 572/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0223 - f1_m: 0.6477 - recall_m: 0.5269 - val_loss: 0.0265 - val_f1_m: 0.6230 - val_recall_m: 0.5505\n",
            "Epoch 573/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0222 - f1_m: 0.6870 - recall_m: 0.6059 - val_loss: 0.0264 - val_f1_m: 0.5994 - val_recall_m: 0.5017\n",
            "Epoch 574/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0221 - f1_m: 0.6642 - recall_m: 0.5556 - val_loss: 0.0264 - val_f1_m: 0.6126 - val_recall_m: 0.5264\n",
            "Epoch 575/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0220 - f1_m: 0.6777 - recall_m: 0.5818 - val_loss: 0.0264 - val_f1_m: 0.6157 - val_recall_m: 0.5343\n",
            "Epoch 576/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0220 - f1_m: 0.6801 - recall_m: 0.5892 - val_loss: 0.0264 - val_f1_m: 0.5978 - val_recall_m: 0.4993\n",
            "Epoch 577/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0221 - f1_m: 0.6631 - recall_m: 0.5528 - val_loss: 0.0265 - val_f1_m: 0.6241 - val_recall_m: 0.5540\n",
            "Epoch 578/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0221 - f1_m: 0.6882 - recall_m: 0.6086 - val_loss: 0.0265 - val_f1_m: 0.5883 - val_recall_m: 0.4820\n",
            "Epoch 579/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0222 - f1_m: 0.6519 - recall_m: 0.5327 - val_loss: 0.0266 - val_f1_m: 0.6288 - val_recall_m: 0.5653\n",
            "Epoch 580/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0222 - f1_m: 0.6934 - recall_m: 0.6214 - val_loss: 0.0265 - val_f1_m: 0.5859 - val_recall_m: 0.4781\n",
            "Epoch 581/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0222 - f1_m: 0.6496 - recall_m: 0.5285 - val_loss: 0.0265 - val_f1_m: 0.6269 - val_recall_m: 0.5614\n",
            "Epoch 582/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0221 - f1_m: 0.6928 - recall_m: 0.6183 - val_loss: 0.0264 - val_f1_m: 0.5966 - val_recall_m: 0.4963\n",
            "Epoch 583/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0220 - f1_m: 0.6613 - recall_m: 0.5478 - val_loss: 0.0264 - val_f1_m: 0.6188 - val_recall_m: 0.5421\n",
            "Epoch 584/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0219 - f1_m: 0.6852 - recall_m: 0.5975 - val_loss: 0.0263 - val_f1_m: 0.6138 - val_recall_m: 0.5269\n",
            "Epoch 585/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0219 - f1_m: 0.6795 - recall_m: 0.5826 - val_loss: 0.0263 - val_f1_m: 0.6076 - val_recall_m: 0.5155\n",
            "Epoch 586/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0218 - f1_m: 0.6747 - recall_m: 0.5719 - val_loss: 0.0264 - val_f1_m: 0.6209 - val_recall_m: 0.5466\n",
            "Epoch 587/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0219 - f1_m: 0.6874 - recall_m: 0.6027 - val_loss: 0.0263 - val_f1_m: 0.5981 - val_recall_m: 0.4973\n",
            "Epoch 588/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0219 - f1_m: 0.6611 - recall_m: 0.5472 - val_loss: 0.0265 - val_f1_m: 0.6277 - val_recall_m: 0.5633\n",
            "Epoch 589/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0219 - f1_m: 0.6949 - recall_m: 0.6204 - val_loss: 0.0264 - val_f1_m: 0.5883 - val_recall_m: 0.4810\n",
            "Epoch 590/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0220 - f1_m: 0.6527 - recall_m: 0.5319 - val_loss: 0.0266 - val_f1_m: 0.6319 - val_recall_m: 0.5727\n",
            "Epoch 591/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0220 - f1_m: 0.6968 - recall_m: 0.6279 - val_loss: 0.0264 - val_f1_m: 0.5881 - val_recall_m: 0.4795\n",
            "Epoch 592/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0220 - f1_m: 0.6526 - recall_m: 0.5306 - val_loss: 0.0265 - val_f1_m: 0.6296 - val_recall_m: 0.5663\n",
            "Epoch 593/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0219 - f1_m: 0.6958 - recall_m: 0.6221 - val_loss: 0.0263 - val_f1_m: 0.6014 - val_recall_m: 0.5027\n",
            "Epoch 594/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0218 - f1_m: 0.6678 - recall_m: 0.5568 - val_loss: 0.0263 - val_f1_m: 0.6191 - val_recall_m: 0.5407\n",
            "Epoch 595/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0217 - f1_m: 0.6886 - recall_m: 0.5999 - val_loss: 0.0262 - val_f1_m: 0.6150 - val_recall_m: 0.5298\n",
            "Epoch 596/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0217 - f1_m: 0.6849 - recall_m: 0.5903 - val_loss: 0.0262 - val_f1_m: 0.6088 - val_recall_m: 0.5155\n",
            "Epoch 597/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0217 - f1_m: 0.6763 - recall_m: 0.5728 - val_loss: 0.0263 - val_f1_m: 0.6239 - val_recall_m: 0.5535\n",
            "Epoch 598/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0217 - f1_m: 0.6940 - recall_m: 0.6128 - val_loss: 0.0263 - val_f1_m: 0.5985 - val_recall_m: 0.4963\n",
            "Epoch 599/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0217 - f1_m: 0.6637 - recall_m: 0.5485 - val_loss: 0.0265 - val_f1_m: 0.6324 - val_recall_m: 0.5727\n",
            "Epoch 600/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0218 - f1_m: 0.6978 - recall_m: 0.6267 - val_loss: 0.0264 - val_f1_m: 0.5887 - val_recall_m: 0.4800\n",
            "Epoch 601/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0219 - f1_m: 0.6546 - recall_m: 0.5325 - val_loss: 0.0266 - val_f1_m: 0.6351 - val_recall_m: 0.5786\n",
            "Epoch 602/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0218 - f1_m: 0.6997 - recall_m: 0.6331 - val_loss: 0.0264 - val_f1_m: 0.5913 - val_recall_m: 0.4845\n",
            "Epoch 603/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0218 - f1_m: 0.6574 - recall_m: 0.5370 - val_loss: 0.0264 - val_f1_m: 0.6291 - val_recall_m: 0.5663\n",
            "Epoch 604/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0216 - f1_m: 0.6979 - recall_m: 0.6237 - val_loss: 0.0262 - val_f1_m: 0.6067 - val_recall_m: 0.5116\n",
            "Epoch 605/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0215 - f1_m: 0.6748 - recall_m: 0.5681 - val_loss: 0.0262 - val_f1_m: 0.6195 - val_recall_m: 0.5377\n",
            "Epoch 606/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0215 - f1_m: 0.6884 - recall_m: 0.5964 - val_loss: 0.0262 - val_f1_m: 0.6197 - val_recall_m: 0.5402\n",
            "Epoch 607/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0215 - f1_m: 0.6902 - recall_m: 0.6007 - val_loss: 0.0262 - val_f1_m: 0.6075 - val_recall_m: 0.5126\n",
            "Epoch 608/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0215 - f1_m: 0.6753 - recall_m: 0.5678 - val_loss: 0.0264 - val_f1_m: 0.6286 - val_recall_m: 0.5648\n",
            "Epoch 609/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0216 - f1_m: 0.6977 - recall_m: 0.6233 - val_loss: 0.0263 - val_f1_m: 0.5947 - val_recall_m: 0.4889\n",
            "Epoch 610/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0216 - f1_m: 0.6604 - recall_m: 0.5410 - val_loss: 0.0265 - val_f1_m: 0.6368 - val_recall_m: 0.5821\n",
            "Epoch 611/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0216 - f1_m: 0.7018 - recall_m: 0.6360 - val_loss: 0.0264 - val_f1_m: 0.5888 - val_recall_m: 0.4810\n",
            "Epoch 612/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0217 - f1_m: 0.6573 - recall_m: 0.5352 - val_loss: 0.0265 - val_f1_m: 0.6373 - val_recall_m: 0.5806\n",
            "Epoch 613/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0216 - f1_m: 0.7023 - recall_m: 0.6341 - val_loss: 0.0262 - val_f1_m: 0.6034 - val_recall_m: 0.5027\n",
            "Epoch 614/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0215 - f1_m: 0.6692 - recall_m: 0.5548 - val_loss: 0.0262 - val_f1_m: 0.6259 - val_recall_m: 0.5545\n",
            "Epoch 615/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0214 - f1_m: 0.6976 - recall_m: 0.6154 - val_loss: 0.0261 - val_f1_m: 0.6151 - val_recall_m: 0.5269\n",
            "Epoch 616/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0213 - f1_m: 0.6865 - recall_m: 0.5895 - val_loss: 0.0261 - val_f1_m: 0.6131 - val_recall_m: 0.5224\n",
            "Epoch 617/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0213 - f1_m: 0.6848 - recall_m: 0.5835 - val_loss: 0.0262 - val_f1_m: 0.6282 - val_recall_m: 0.5584\n",
            "Epoch 618/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0213 - f1_m: 0.6986 - recall_m: 0.6182 - val_loss: 0.0262 - val_f1_m: 0.6045 - val_recall_m: 0.5062\n",
            "Epoch 619/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0214 - f1_m: 0.6739 - recall_m: 0.5615 - val_loss: 0.0264 - val_f1_m: 0.6367 - val_recall_m: 0.5796\n",
            "Epoch 620/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0214 - f1_m: 0.7034 - recall_m: 0.6345 - val_loss: 0.0263 - val_f1_m: 0.5935 - val_recall_m: 0.4874\n",
            "Epoch 621/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0215 - f1_m: 0.6611 - recall_m: 0.5403 - val_loss: 0.0265 - val_f1_m: 0.6398 - val_recall_m: 0.5870\n",
            "Epoch 622/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0215 - f1_m: 0.7059 - recall_m: 0.6412 - val_loss: 0.0263 - val_f1_m: 0.5959 - val_recall_m: 0.4914\n",
            "Epoch 623/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0214 - f1_m: 0.6638 - recall_m: 0.5448 - val_loss: 0.0263 - val_f1_m: 0.6317 - val_recall_m: 0.5697\n",
            "Epoch 624/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0213 - f1_m: 0.7026 - recall_m: 0.6295 - val_loss: 0.0261 - val_f1_m: 0.6099 - val_recall_m: 0.5155\n",
            "Epoch 625/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0212 - f1_m: 0.6804 - recall_m: 0.5742 - val_loss: 0.0261 - val_f1_m: 0.6254 - val_recall_m: 0.5476\n",
            "Epoch 626/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0211 - f1_m: 0.6953 - recall_m: 0.6069 - val_loss: 0.0261 - val_f1_m: 0.6260 - val_recall_m: 0.5490\n",
            "Epoch 627/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0211 - f1_m: 0.6965 - recall_m: 0.6085 - val_loss: 0.0261 - val_f1_m: 0.6106 - val_recall_m: 0.5175\n",
            "Epoch 628/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0211 - f1_m: 0.6836 - recall_m: 0.5785 - val_loss: 0.0263 - val_f1_m: 0.6307 - val_recall_m: 0.5673\n",
            "Epoch 629/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0212 - f1_m: 0.7034 - recall_m: 0.6289 - val_loss: 0.0262 - val_f1_m: 0.6047 - val_recall_m: 0.5037\n",
            "Epoch 630/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0213 - f1_m: 0.6727 - recall_m: 0.5577 - val_loss: 0.0264 - val_f1_m: 0.6397 - val_recall_m: 0.5845\n",
            "Epoch 631/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0213 - f1_m: 0.7078 - recall_m: 0.6424 - val_loss: 0.0263 - val_f1_m: 0.5918 - val_recall_m: 0.4860\n",
            "Epoch 632/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0214 - f1_m: 0.6629 - recall_m: 0.5416 - val_loss: 0.0264 - val_f1_m: 0.6405 - val_recall_m: 0.5870\n",
            "Epoch 633/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0213 - f1_m: 0.7086 - recall_m: 0.6451 - val_loss: 0.0262 - val_f1_m: 0.6033 - val_recall_m: 0.5017\n",
            "Epoch 634/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0212 - f1_m: 0.6728 - recall_m: 0.5570 - val_loss: 0.0262 - val_f1_m: 0.6310 - val_recall_m: 0.5668\n",
            "Epoch 635/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0211 - f1_m: 0.7050 - recall_m: 0.6297 - val_loss: 0.0261 - val_f1_m: 0.6124 - val_recall_m: 0.5214\n",
            "Epoch 636/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0210 - f1_m: 0.6878 - recall_m: 0.5850 - val_loss: 0.0261 - val_f1_m: 0.6261 - val_recall_m: 0.5481\n",
            "Epoch 637/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0210 - f1_m: 0.6974 - recall_m: 0.6074 - val_loss: 0.0261 - val_f1_m: 0.6267 - val_recall_m: 0.5515\n",
            "Epoch 638/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0209 - f1_m: 0.6995 - recall_m: 0.6131 - val_loss: 0.0261 - val_f1_m: 0.6109 - val_recall_m: 0.5185\n",
            "Epoch 639/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0210 - f1_m: 0.6863 - recall_m: 0.5812 - val_loss: 0.0262 - val_f1_m: 0.6361 - val_recall_m: 0.5747\n",
            "Epoch 640/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0210 - f1_m: 0.7077 - recall_m: 0.6345 - val_loss: 0.0262 - val_f1_m: 0.6046 - val_recall_m: 0.5037\n",
            "Epoch 641/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0211 - f1_m: 0.6757 - recall_m: 0.5614 - val_loss: 0.0264 - val_f1_m: 0.6399 - val_recall_m: 0.5850\n",
            "Epoch 642/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0211 - f1_m: 0.7109 - recall_m: 0.6462 - val_loss: 0.0263 - val_f1_m: 0.5926 - val_recall_m: 0.4874\n",
            "Epoch 643/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0212 - f1_m: 0.6685 - recall_m: 0.5476 - val_loss: 0.0264 - val_f1_m: 0.6438 - val_recall_m: 0.5929\n",
            "Epoch 644/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0211 - f1_m: 0.7120 - recall_m: 0.6508 - val_loss: 0.0262 - val_f1_m: 0.5996 - val_recall_m: 0.4963\n",
            "Epoch 645/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0211 - f1_m: 0.6736 - recall_m: 0.5560 - val_loss: 0.0263 - val_f1_m: 0.6378 - val_recall_m: 0.5801\n",
            "Epoch 646/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0210 - f1_m: 0.7097 - recall_m: 0.6401 - val_loss: 0.0260 - val_f1_m: 0.6114 - val_recall_m: 0.5180\n",
            "Epoch 647/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0209 - f1_m: 0.6873 - recall_m: 0.5811 - val_loss: 0.0261 - val_f1_m: 0.6281 - val_recall_m: 0.5545\n",
            "Epoch 648/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0208 - f1_m: 0.7021 - recall_m: 0.6171 - val_loss: 0.0260 - val_f1_m: 0.6268 - val_recall_m: 0.5495\n",
            "Epoch 649/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0208 - f1_m: 0.7008 - recall_m: 0.6121 - val_loss: 0.0260 - val_f1_m: 0.6187 - val_recall_m: 0.5298\n",
            "Epoch 650/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0208 - f1_m: 0.6935 - recall_m: 0.5930 - val_loss: 0.0262 - val_f1_m: 0.6380 - val_recall_m: 0.5747\n",
            "Epoch 651/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0208 - f1_m: 0.7096 - recall_m: 0.6354 - val_loss: 0.0261 - val_f1_m: 0.6083 - val_recall_m: 0.5101\n",
            "Epoch 652/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0209 - f1_m: 0.6818 - recall_m: 0.5705 - val_loss: 0.0263 - val_f1_m: 0.6428 - val_recall_m: 0.5895\n",
            "Epoch 653/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0209 - f1_m: 0.7129 - recall_m: 0.6482 - val_loss: 0.0262 - val_f1_m: 0.5970 - val_recall_m: 0.4914\n",
            "Epoch 654/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0210 - f1_m: 0.6730 - recall_m: 0.5527 - val_loss: 0.0264 - val_f1_m: 0.6468 - val_recall_m: 0.5988\n",
            "Epoch 655/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0210 - f1_m: 0.7152 - recall_m: 0.6568 - val_loss: 0.0262 - val_f1_m: 0.5955 - val_recall_m: 0.4894\n",
            "Epoch 656/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0210 - f1_m: 0.6730 - recall_m: 0.5523 - val_loss: 0.0263 - val_f1_m: 0.6432 - val_recall_m: 0.5895\n",
            "Epoch 657/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0208 - f1_m: 0.7139 - recall_m: 0.6485 - val_loss: 0.0260 - val_f1_m: 0.6124 - val_recall_m: 0.5175\n",
            "Epoch 658/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0207 - f1_m: 0.6864 - recall_m: 0.5782 - val_loss: 0.0260 - val_f1_m: 0.6322 - val_recall_m: 0.5628\n",
            "Epoch 659/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0206 - f1_m: 0.7064 - recall_m: 0.6252 - val_loss: 0.0260 - val_f1_m: 0.6266 - val_recall_m: 0.5476\n",
            "Epoch 660/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0206 - f1_m: 0.7021 - recall_m: 0.6124 - val_loss: 0.0260 - val_f1_m: 0.6215 - val_recall_m: 0.5338\n",
            "Epoch 661/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0206 - f1_m: 0.6961 - recall_m: 0.5961 - val_loss: 0.0261 - val_f1_m: 0.6409 - val_recall_m: 0.5791\n",
            "Epoch 662/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0206 - f1_m: 0.7127 - recall_m: 0.6394 - val_loss: 0.0261 - val_f1_m: 0.6116 - val_recall_m: 0.5145\n",
            "Epoch 663/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0207 - f1_m: 0.6847 - recall_m: 0.5743 - val_loss: 0.0263 - val_f1_m: 0.6447 - val_recall_m: 0.5924\n",
            "Epoch 664/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0207 - f1_m: 0.7156 - recall_m: 0.6514 - val_loss: 0.0262 - val_f1_m: 0.5998 - val_recall_m: 0.4948\n",
            "Epoch 665/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0208 - f1_m: 0.6764 - recall_m: 0.5568 - val_loss: 0.0264 - val_f1_m: 0.6499 - val_recall_m: 0.6037\n",
            "Epoch 666/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0208 - f1_m: 0.7182 - recall_m: 0.6608 - val_loss: 0.0262 - val_f1_m: 0.5991 - val_recall_m: 0.4938\n",
            "Epoch 667/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0208 - f1_m: 0.6775 - recall_m: 0.5585 - val_loss: 0.0262 - val_f1_m: 0.6445 - val_recall_m: 0.5909\n",
            "Epoch 668/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0206 - f1_m: 0.7161 - recall_m: 0.6500 - val_loss: 0.0260 - val_f1_m: 0.6170 - val_recall_m: 0.5249\n",
            "Epoch 669/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0205 - f1_m: 0.6931 - recall_m: 0.5878 - val_loss: 0.0260 - val_f1_m: 0.6314 - val_recall_m: 0.5619\n",
            "Epoch 670/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0204 - f1_m: 0.7093 - recall_m: 0.6288 - val_loss: 0.0260 - val_f1_m: 0.6293 - val_recall_m: 0.5530\n",
            "Epoch 671/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0204 - f1_m: 0.7055 - recall_m: 0.6173 - val_loss: 0.0260 - val_f1_m: 0.6229 - val_recall_m: 0.5357\n",
            "Epoch 672/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0204 - f1_m: 0.6993 - recall_m: 0.5997 - val_loss: 0.0261 - val_f1_m: 0.6424 - val_recall_m: 0.5816\n",
            "Epoch 673/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0205 - f1_m: 0.7145 - recall_m: 0.6424 - val_loss: 0.0260 - val_f1_m: 0.6120 - val_recall_m: 0.5145\n",
            "Epoch 674/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0205 - f1_m: 0.6881 - recall_m: 0.5769 - val_loss: 0.0263 - val_f1_m: 0.6451 - val_recall_m: 0.5954\n",
            "Epoch 675/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0206 - f1_m: 0.7186 - recall_m: 0.6568 - val_loss: 0.0262 - val_f1_m: 0.6002 - val_recall_m: 0.4953\n",
            "Epoch 676/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0207 - f1_m: 0.6799 - recall_m: 0.5601 - val_loss: 0.0264 - val_f1_m: 0.6513 - val_recall_m: 0.6077\n",
            "Epoch 677/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0206 - f1_m: 0.7203 - recall_m: 0.6648 - val_loss: 0.0261 - val_f1_m: 0.6023 - val_recall_m: 0.4978\n",
            "Epoch 678/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0206 - f1_m: 0.6812 - recall_m: 0.5627 - val_loss: 0.0262 - val_f1_m: 0.6448 - val_recall_m: 0.5904\n",
            "Epoch 679/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0204 - f1_m: 0.7181 - recall_m: 0.6512 - val_loss: 0.0260 - val_f1_m: 0.6224 - val_recall_m: 0.5318\n",
            "Epoch 680/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0203 - f1_m: 0.6984 - recall_m: 0.5958 - val_loss: 0.0260 - val_f1_m: 0.6307 - val_recall_m: 0.5584\n",
            "Epoch 681/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0203 - f1_m: 0.7096 - recall_m: 0.6267 - val_loss: 0.0260 - val_f1_m: 0.6295 - val_recall_m: 0.5564\n",
            "Epoch 682/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0202 - f1_m: 0.7088 - recall_m: 0.6237 - val_loss: 0.0259 - val_f1_m: 0.6209 - val_recall_m: 0.5333\n",
            "Epoch 683/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0203 - f1_m: 0.7009 - recall_m: 0.6002 - val_loss: 0.0261 - val_f1_m: 0.6481 - val_recall_m: 0.5904\n",
            "Epoch 684/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0203 - f1_m: 0.7179 - recall_m: 0.6489 - val_loss: 0.0261 - val_f1_m: 0.6099 - val_recall_m: 0.5101\n",
            "Epoch 685/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0204 - f1_m: 0.6891 - recall_m: 0.5755 - val_loss: 0.0263 - val_f1_m: 0.6466 - val_recall_m: 0.5988\n",
            "Epoch 686/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0204 - f1_m: 0.7217 - recall_m: 0.6624 - val_loss: 0.0262 - val_f1_m: 0.6041 - val_recall_m: 0.4993\n",
            "Epoch 687/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0206 - f1_m: 0.6820 - recall_m: 0.5624 - val_loss: 0.0264 - val_f1_m: 0.6518 - val_recall_m: 0.6102\n",
            "Epoch 688/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0205 - f1_m: 0.7232 - recall_m: 0.6693 - val_loss: 0.0261 - val_f1_m: 0.6052 - val_recall_m: 0.5027\n",
            "Epoch 689/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0204 - f1_m: 0.6853 - recall_m: 0.5678 - val_loss: 0.0262 - val_f1_m: 0.6471 - val_recall_m: 0.5929\n",
            "Epoch 690/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0203 - f1_m: 0.7206 - recall_m: 0.6538 - val_loss: 0.0259 - val_f1_m: 0.6215 - val_recall_m: 0.5338\n",
            "Epoch 691/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0202 - f1_m: 0.7026 - recall_m: 0.6030 - val_loss: 0.0259 - val_f1_m: 0.6337 - val_recall_m: 0.5614\n",
            "Epoch 692/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0201 - f1_m: 0.7105 - recall_m: 0.6260 - val_loss: 0.0260 - val_f1_m: 0.6360 - val_recall_m: 0.5658\n",
            "Epoch 693/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0201 - f1_m: 0.7130 - recall_m: 0.6306 - val_loss: 0.0260 - val_f1_m: 0.6202 - val_recall_m: 0.5303\n",
            "Epoch 694/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0201 - f1_m: 0.7019 - recall_m: 0.6002 - val_loss: 0.0262 - val_f1_m: 0.6484 - val_recall_m: 0.5944\n",
            "Epoch 695/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0202 - f1_m: 0.7208 - recall_m: 0.6550 - val_loss: 0.0261 - val_f1_m: 0.6054 - val_recall_m: 0.5047\n",
            "Epoch 696/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0203 - f1_m: 0.6890 - recall_m: 0.5729 - val_loss: 0.0264 - val_f1_m: 0.6492 - val_recall_m: 0.6052\n",
            "Epoch 697/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0203 - f1_m: 0.7251 - recall_m: 0.6687 - val_loss: 0.0262 - val_f1_m: 0.6026 - val_recall_m: 0.4978\n",
            "Epoch 698/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0204 - f1_m: 0.6826 - recall_m: 0.5620 - val_loss: 0.0263 - val_f1_m: 0.6510 - val_recall_m: 0.6082\n",
            "Epoch 699/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0203 - f1_m: 0.7246 - recall_m: 0.6699 - val_loss: 0.0260 - val_f1_m: 0.6098 - val_recall_m: 0.5111\n",
            "Epoch 700/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0202 - f1_m: 0.6919 - recall_m: 0.5781 - val_loss: 0.0261 - val_f1_m: 0.6452 - val_recall_m: 0.5880\n",
            "Epoch 701/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0200 - f1_m: 0.7211 - recall_m: 0.6517 - val_loss: 0.0259 - val_f1_m: 0.6254 - val_recall_m: 0.5426\n",
            "Epoch 702/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0200 - f1_m: 0.7076 - recall_m: 0.6137 - val_loss: 0.0259 - val_f1_m: 0.6263 - val_recall_m: 0.5456\n",
            "Epoch 703/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0199 - f1_m: 0.7083 - recall_m: 0.6158 - val_loss: 0.0260 - val_f1_m: 0.6384 - val_recall_m: 0.5747\n",
            "Epoch 704/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0200 - f1_m: 0.7180 - recall_m: 0.6418 - val_loss: 0.0260 - val_f1_m: 0.6179 - val_recall_m: 0.5249\n",
            "Epoch 705/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0200 - f1_m: 0.7002 - recall_m: 0.5943 - val_loss: 0.0262 - val_f1_m: 0.6494 - val_recall_m: 0.5993\n",
            "Epoch 706/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0201 - f1_m: 0.7255 - recall_m: 0.6644 - val_loss: 0.0261 - val_f1_m: 0.6047 - val_recall_m: 0.5017\n",
            "Epoch 707/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0202 - f1_m: 0.6875 - recall_m: 0.5688 - val_loss: 0.0264 - val_f1_m: 0.6507 - val_recall_m: 0.6106\n",
            "Epoch 708/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0202 - f1_m: 0.7278 - recall_m: 0.6755 - val_loss: 0.0261 - val_f1_m: 0.6075 - val_recall_m: 0.5047\n",
            "Epoch 709/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0202 - f1_m: 0.6880 - recall_m: 0.5695 - val_loss: 0.0262 - val_f1_m: 0.6492 - val_recall_m: 0.6033\n",
            "Epoch 710/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0201 - f1_m: 0.7269 - recall_m: 0.6681 - val_loss: 0.0259 - val_f1_m: 0.6198 - val_recall_m: 0.5264\n",
            "Epoch 711/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0199 - f1_m: 0.7022 - recall_m: 0.5951 - val_loss: 0.0260 - val_f1_m: 0.6421 - val_recall_m: 0.5786\n",
            "Epoch 712/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0198 - f1_m: 0.7204 - recall_m: 0.6442 - val_loss: 0.0259 - val_f1_m: 0.6297 - val_recall_m: 0.5540\n",
            "Epoch 713/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0198 - f1_m: 0.7132 - recall_m: 0.6256 - val_loss: 0.0259 - val_f1_m: 0.6243 - val_recall_m: 0.5402\n",
            "Epoch 714/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0198 - f1_m: 0.7079 - recall_m: 0.6120 - val_loss: 0.0261 - val_f1_m: 0.6468 - val_recall_m: 0.5885\n",
            "Epoch 715/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0198 - f1_m: 0.7237 - recall_m: 0.6540 - val_loss: 0.0260 - val_f1_m: 0.6157 - val_recall_m: 0.5185\n",
            "Epoch 716/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0199 - f1_m: 0.6990 - recall_m: 0.5886 - val_loss: 0.0263 - val_f1_m: 0.6520 - val_recall_m: 0.6082\n",
            "Epoch 717/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0200 - f1_m: 0.7304 - recall_m: 0.6751 - val_loss: 0.0262 - val_f1_m: 0.6071 - val_recall_m: 0.5022\n",
            "Epoch 718/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0202 - f1_m: 0.6874 - recall_m: 0.5673 - val_loss: 0.0264 - val_f1_m: 0.6526 - val_recall_m: 0.6161\n",
            "Epoch 719/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0201 - f1_m: 0.7314 - recall_m: 0.6832 - val_loss: 0.0261 - val_f1_m: 0.6096 - val_recall_m: 0.5071\n",
            "Epoch 720/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0201 - f1_m: 0.6924 - recall_m: 0.5750 - val_loss: 0.0262 - val_f1_m: 0.6503 - val_recall_m: 0.6013\n",
            "Epoch 721/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0198 - f1_m: 0.7280 - recall_m: 0.6671 - val_loss: 0.0259 - val_f1_m: 0.6246 - val_recall_m: 0.5362\n",
            "Epoch 722/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0197 - f1_m: 0.7073 - recall_m: 0.6067 - val_loss: 0.0259 - val_f1_m: 0.6345 - val_recall_m: 0.5643\n",
            "Epoch 723/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0196 - f1_m: 0.7182 - recall_m: 0.6355 - val_loss: 0.0259 - val_f1_m: 0.6376 - val_recall_m: 0.5688\n",
            "Epoch 724/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0196 - f1_m: 0.7194 - recall_m: 0.6385 - val_loss: 0.0259 - val_f1_m: 0.6234 - val_recall_m: 0.5347\n",
            "Epoch 725/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0197 - f1_m: 0.7079 - recall_m: 0.6075 - val_loss: 0.0261 - val_f1_m: 0.6489 - val_recall_m: 0.5988\n",
            "Epoch 726/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0198 - f1_m: 0.7289 - recall_m: 0.6675 - val_loss: 0.0260 - val_f1_m: 0.6127 - val_recall_m: 0.5126\n",
            "Epoch 727/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0199 - f1_m: 0.6968 - recall_m: 0.5821 - val_loss: 0.0263 - val_f1_m: 0.6541 - val_recall_m: 0.6161\n",
            "Epoch 728/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0199 - f1_m: 0.7337 - recall_m: 0.6835 - val_loss: 0.0261 - val_f1_m: 0.6105 - val_recall_m: 0.5067\n",
            "Epoch 729/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0200 - f1_m: 0.6902 - recall_m: 0.5705 - val_loss: 0.0263 - val_f1_m: 0.6551 - val_recall_m: 0.6141\n",
            "Epoch 730/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0198 - f1_m: 0.7332 - recall_m: 0.6813 - val_loss: 0.0259 - val_f1_m: 0.6163 - val_recall_m: 0.5205\n",
            "Epoch 731/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0197 - f1_m: 0.7026 - recall_m: 0.5936 - val_loss: 0.0260 - val_f1_m: 0.6450 - val_recall_m: 0.5855\n",
            "Epoch 732/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0196 - f1_m: 0.7259 - recall_m: 0.6541 - val_loss: 0.0258 - val_f1_m: 0.6307 - val_recall_m: 0.5535\n",
            "Epoch 733/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0195 - f1_m: 0.7161 - recall_m: 0.6250 - val_loss: 0.0258 - val_f1_m: 0.6292 - val_recall_m: 0.5510\n",
            "Epoch 734/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0195 - f1_m: 0.7162 - recall_m: 0.6253 - val_loss: 0.0260 - val_f1_m: 0.6453 - val_recall_m: 0.5850\n",
            "Epoch 735/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0195 - f1_m: 0.7273 - recall_m: 0.6556 - val_loss: 0.0259 - val_f1_m: 0.6188 - val_recall_m: 0.5244\n",
            "Epoch 736/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0196 - f1_m: 0.7060 - recall_m: 0.5983 - val_loss: 0.0262 - val_f1_m: 0.6531 - val_recall_m: 0.6097\n",
            "Epoch 737/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0197 - f1_m: 0.7344 - recall_m: 0.6804 - val_loss: 0.0261 - val_f1_m: 0.6091 - val_recall_m: 0.5057\n",
            "Epoch 738/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0199 - f1_m: 0.6934 - recall_m: 0.5743 - val_loss: 0.0264 - val_f1_m: 0.6561 - val_recall_m: 0.6195\n",
            "Epoch 739/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0198 - f1_m: 0.7359 - recall_m: 0.6892 - val_loss: 0.0261 - val_f1_m: 0.6090 - val_recall_m: 0.5052\n",
            "Epoch 740/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0199 - f1_m: 0.6926 - recall_m: 0.5734 - val_loss: 0.0262 - val_f1_m: 0.6540 - val_recall_m: 0.6111\n",
            "Epoch 741/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0196 - f1_m: 0.7346 - recall_m: 0.6803 - val_loss: 0.0259 - val_f1_m: 0.6240 - val_recall_m: 0.5338\n",
            "Epoch 742/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0195 - f1_m: 0.7097 - recall_m: 0.6070 - val_loss: 0.0259 - val_f1_m: 0.6412 - val_recall_m: 0.5747\n",
            "Epoch 743/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0194 - f1_m: 0.7247 - recall_m: 0.6466 - val_loss: 0.0259 - val_f1_m: 0.6374 - val_recall_m: 0.5688\n",
            "Epoch 744/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0194 - f1_m: 0.7236 - recall_m: 0.6431 - val_loss: 0.0258 - val_f1_m: 0.6263 - val_recall_m: 0.5402\n",
            "Epoch 745/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0194 - f1_m: 0.7130 - recall_m: 0.6151 - val_loss: 0.0261 - val_f1_m: 0.6501 - val_recall_m: 0.6003\n",
            "Epoch 746/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0195 - f1_m: 0.7348 - recall_m: 0.6738 - val_loss: 0.0260 - val_f1_m: 0.6143 - val_recall_m: 0.5145\n",
            "Epoch 747/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0196 - f1_m: 0.7005 - recall_m: 0.5862 - val_loss: 0.0263 - val_f1_m: 0.6567 - val_recall_m: 0.6200\n",
            "Epoch 748/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0196 - f1_m: 0.7376 - recall_m: 0.6911 - val_loss: 0.0261 - val_f1_m: 0.6080 - val_recall_m: 0.5042\n",
            "Epoch 749/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0198 - f1_m: 0.6933 - recall_m: 0.5735 - val_loss: 0.0263 - val_f1_m: 0.6567 - val_recall_m: 0.6166\n",
            "Epoch 750/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0196 - f1_m: 0.7379 - recall_m: 0.6880 - val_loss: 0.0259 - val_f1_m: 0.6215 - val_recall_m: 0.5269\n",
            "Epoch 751/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0195 - f1_m: 0.7045 - recall_m: 0.5953 - val_loss: 0.0260 - val_f1_m: 0.6467 - val_recall_m: 0.5895\n",
            "Epoch 752/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0193 - f1_m: 0.7320 - recall_m: 0.6637 - val_loss: 0.0258 - val_f1_m: 0.6326 - val_recall_m: 0.5574\n",
            "Epoch 753/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0192 - f1_m: 0.7191 - recall_m: 0.6300 - val_loss: 0.0258 - val_f1_m: 0.6324 - val_recall_m: 0.5525\n",
            "Epoch 754/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0192 - f1_m: 0.7180 - recall_m: 0.6254 - val_loss: 0.0260 - val_f1_m: 0.6505 - val_recall_m: 0.5949\n",
            "Epoch 755/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0193 - f1_m: 0.7335 - recall_m: 0.6671 - val_loss: 0.0259 - val_f1_m: 0.6196 - val_recall_m: 0.5234\n",
            "Epoch 756/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0194 - f1_m: 0.7076 - recall_m: 0.5980 - val_loss: 0.0263 - val_f1_m: 0.6541 - val_recall_m: 0.6136\n",
            "Epoch 757/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0195 - f1_m: 0.7394 - recall_m: 0.6886 - val_loss: 0.0261 - val_f1_m: 0.6082 - val_recall_m: 0.5042\n",
            "Epoch 758/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0197 - f1_m: 0.6939 - recall_m: 0.5746 - val_loss: 0.0263 - val_f1_m: 0.6592 - val_recall_m: 0.6240\n",
            "Epoch 759/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0195 - f1_m: 0.7399 - recall_m: 0.6958 - val_loss: 0.0260 - val_f1_m: 0.6157 - val_recall_m: 0.5155\n",
            "Epoch 760/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0195 - f1_m: 0.7024 - recall_m: 0.5883 - val_loss: 0.0260 - val_f1_m: 0.6499 - val_recall_m: 0.5983\n",
            "Epoch 761/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0193 - f1_m: 0.7372 - recall_m: 0.6754 - val_loss: 0.0258 - val_f1_m: 0.6318 - val_recall_m: 0.5515\n",
            "Epoch 762/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0191 - f1_m: 0.7191 - recall_m: 0.6257 - val_loss: 0.0258 - val_f1_m: 0.6367 - val_recall_m: 0.5648\n",
            "Epoch 763/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0191 - f1_m: 0.7239 - recall_m: 0.6387 - val_loss: 0.0259 - val_f1_m: 0.6436 - val_recall_m: 0.5830\n",
            "Epoch 764/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0191 - f1_m: 0.7334 - recall_m: 0.6612 - val_loss: 0.0258 - val_f1_m: 0.6246 - val_recall_m: 0.5338\n",
            "Epoch 765/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0192 - f1_m: 0.7128 - recall_m: 0.6093 - val_loss: 0.0262 - val_f1_m: 0.6580 - val_recall_m: 0.6141\n",
            "Epoch 766/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0193 - f1_m: 0.7406 - recall_m: 0.6874 - val_loss: 0.0260 - val_f1_m: 0.6125 - val_recall_m: 0.5111\n",
            "Epoch 767/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0194 - f1_m: 0.7010 - recall_m: 0.5855 - val_loss: 0.0263 - val_f1_m: 0.6585 - val_recall_m: 0.6210\n",
            "Epoch 768/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0194 - f1_m: 0.7426 - recall_m: 0.6961 - val_loss: 0.0260 - val_f1_m: 0.6106 - val_recall_m: 0.5081\n",
            "Epoch 769/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0195 - f1_m: 0.6988 - recall_m: 0.5813 - val_loss: 0.0262 - val_f1_m: 0.6590 - val_recall_m: 0.6175\n",
            "Epoch 770/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0193 - f1_m: 0.7418 - recall_m: 0.6914 - val_loss: 0.0258 - val_f1_m: 0.6229 - val_recall_m: 0.5313\n",
            "Epoch 771/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0191 - f1_m: 0.7146 - recall_m: 0.6101 - val_loss: 0.0258 - val_f1_m: 0.6457 - val_recall_m: 0.5855\n",
            "Epoch 772/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0190 - f1_m: 0.7343 - recall_m: 0.6627 - val_loss: 0.0258 - val_f1_m: 0.6416 - val_recall_m: 0.5727\n",
            "Epoch 773/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0190 - f1_m: 0.7290 - recall_m: 0.6477 - val_loss: 0.0258 - val_f1_m: 0.6315 - val_recall_m: 0.5485\n",
            "Epoch 774/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0190 - f1_m: 0.7210 - recall_m: 0.6267 - val_loss: 0.0260 - val_f1_m: 0.6515 - val_recall_m: 0.5988\n",
            "Epoch 775/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0191 - f1_m: 0.7397 - recall_m: 0.6770 - val_loss: 0.0259 - val_f1_m: 0.6180 - val_recall_m: 0.5219\n",
            "Epoch 776/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0192 - f1_m: 0.7093 - recall_m: 0.5997 - val_loss: 0.0263 - val_f1_m: 0.6618 - val_recall_m: 0.6249\n",
            "Epoch 777/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0192 - f1_m: 0.7441 - recall_m: 0.6980 - val_loss: 0.0261 - val_f1_m: 0.6101 - val_recall_m: 0.5071\n",
            "Epoch 778/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0194 - f1_m: 0.6999 - recall_m: 0.5815 - val_loss: 0.0263 - val_f1_m: 0.6613 - val_recall_m: 0.6244\n",
            "Epoch 779/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0192 - f1_m: 0.7444 - recall_m: 0.6990 - val_loss: 0.0260 - val_f1_m: 0.6175 - val_recall_m: 0.5200\n",
            "Epoch 780/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0192 - f1_m: 0.7075 - recall_m: 0.5955 - val_loss: 0.0260 - val_f1_m: 0.6543 - val_recall_m: 0.6062\n",
            "Epoch 781/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0190 - f1_m: 0.7418 - recall_m: 0.6837 - val_loss: 0.0258 - val_f1_m: 0.6325 - val_recall_m: 0.5481\n",
            "Epoch 782/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0189 - f1_m: 0.7213 - recall_m: 0.6254 - val_loss: 0.0258 - val_f1_m: 0.6408 - val_recall_m: 0.5732\n",
            "Epoch 783/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0188 - f1_m: 0.7317 - recall_m: 0.6524 - val_loss: 0.0258 - val_f1_m: 0.6471 - val_recall_m: 0.5855\n",
            "Epoch 784/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0188 - f1_m: 0.7364 - recall_m: 0.6634 - val_loss: 0.0258 - val_f1_m: 0.6307 - val_recall_m: 0.5441\n",
            "Epoch 785/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0189 - f1_m: 0.7220 - recall_m: 0.6244 - val_loss: 0.0260 - val_f1_m: 0.6541 - val_recall_m: 0.6057\n",
            "Epoch 786/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0189 - f1_m: 0.7430 - recall_m: 0.6843 - val_loss: 0.0259 - val_f1_m: 0.6191 - val_recall_m: 0.5219\n",
            "Epoch 787/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0191 - f1_m: 0.7107 - recall_m: 0.6001 - val_loss: 0.0263 - val_f1_m: 0.6628 - val_recall_m: 0.6269\n",
            "Epoch 788/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0191 - f1_m: 0.7465 - recall_m: 0.7012 - val_loss: 0.0260 - val_f1_m: 0.6109 - val_recall_m: 0.5076\n",
            "Epoch 789/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0193 - f1_m: 0.7021 - recall_m: 0.5841 - val_loss: 0.0263 - val_f1_m: 0.6629 - val_recall_m: 0.6289\n",
            "Epoch 790/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0191 - f1_m: 0.7464 - recall_m: 0.7028 - val_loss: 0.0259 - val_f1_m: 0.6178 - val_recall_m: 0.5195\n",
            "Epoch 791/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0191 - f1_m: 0.7113 - recall_m: 0.5999 - val_loss: 0.0260 - val_f1_m: 0.6541 - val_recall_m: 0.6057\n",
            "Epoch 792/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0189 - f1_m: 0.7436 - recall_m: 0.6846 - val_loss: 0.0257 - val_f1_m: 0.6307 - val_recall_m: 0.5481\n",
            "Epoch 793/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0188 - f1_m: 0.7250 - recall_m: 0.6307 - val_loss: 0.0258 - val_f1_m: 0.6417 - val_recall_m: 0.5747\n",
            "Epoch 794/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0187 - f1_m: 0.7344 - recall_m: 0.6556 - val_loss: 0.0258 - val_f1_m: 0.6469 - val_recall_m: 0.5860\n",
            "Epoch 795/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0187 - f1_m: 0.7380 - recall_m: 0.6652 - val_loss: 0.0258 - val_f1_m: 0.6285 - val_recall_m: 0.5441\n",
            "Epoch 796/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0187 - f1_m: 0.7248 - recall_m: 0.6287 - val_loss: 0.0260 - val_f1_m: 0.6555 - val_recall_m: 0.6077\n",
            "Epoch 797/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0188 - f1_m: 0.7453 - recall_m: 0.6877 - val_loss: 0.0259 - val_f1_m: 0.6201 - val_recall_m: 0.5254\n",
            "Epoch 798/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0189 - f1_m: 0.7142 - recall_m: 0.6058 - val_loss: 0.0262 - val_f1_m: 0.6614 - val_recall_m: 0.6244\n",
            "Epoch 799/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0189 - f1_m: 0.7489 - recall_m: 0.7022 - val_loss: 0.0260 - val_f1_m: 0.6082 - val_recall_m: 0.5042\n",
            "Epoch 800/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0191 - f1_m: 0.7029 - recall_m: 0.5844 - val_loss: 0.0263 - val_f1_m: 0.6672 - val_recall_m: 0.6363\n",
            "Epoch 801/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0190 - f1_m: 0.7494 - recall_m: 0.7092 - val_loss: 0.0260 - val_f1_m: 0.6154 - val_recall_m: 0.5150\n",
            "Epoch 802/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0190 - f1_m: 0.7087 - recall_m: 0.5943 - val_loss: 0.0261 - val_f1_m: 0.6603 - val_recall_m: 0.6166\n",
            "Epoch 803/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0188 - f1_m: 0.7481 - recall_m: 0.6951 - val_loss: 0.0257 - val_f1_m: 0.6277 - val_recall_m: 0.5426\n",
            "Epoch 804/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0187 - f1_m: 0.7238 - recall_m: 0.6260 - val_loss: 0.0258 - val_f1_m: 0.6487 - val_recall_m: 0.5885\n",
            "Epoch 805/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0186 - f1_m: 0.7403 - recall_m: 0.6689 - val_loss: 0.0258 - val_f1_m: 0.6432 - val_recall_m: 0.5766\n",
            "Epoch 806/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0185 - f1_m: 0.7372 - recall_m: 0.6588 - val_loss: 0.0257 - val_f1_m: 0.6323 - val_recall_m: 0.5505\n",
            "Epoch 807/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0186 - f1_m: 0.7282 - recall_m: 0.6351 - val_loss: 0.0260 - val_f1_m: 0.6569 - val_recall_m: 0.6092\n",
            "Epoch 808/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0186 - f1_m: 0.7480 - recall_m: 0.6909 - val_loss: 0.0259 - val_f1_m: 0.6187 - val_recall_m: 0.5234\n",
            "Epoch 809/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0188 - f1_m: 0.7146 - recall_m: 0.6046 - val_loss: 0.0263 - val_f1_m: 0.6618 - val_recall_m: 0.6274\n",
            "Epoch 810/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0188 - f1_m: 0.7508 - recall_m: 0.7067 - val_loss: 0.0261 - val_f1_m: 0.6080 - val_recall_m: 0.5037\n",
            "Epoch 811/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0191 - f1_m: 0.7032 - recall_m: 0.5844 - val_loss: 0.0263 - val_f1_m: 0.6696 - val_recall_m: 0.6417\n",
            "Epoch 812/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0189 - f1_m: 0.7523 - recall_m: 0.7138 - val_loss: 0.0260 - val_f1_m: 0.6148 - val_recall_m: 0.5155\n",
            "Epoch 813/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0189 - f1_m: 0.7112 - recall_m: 0.5973 - val_loss: 0.0260 - val_f1_m: 0.6612 - val_recall_m: 0.6180\n",
            "Epoch 814/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0186 - f1_m: 0.7493 - recall_m: 0.6946 - val_loss: 0.0257 - val_f1_m: 0.6336 - val_recall_m: 0.5545\n",
            "Epoch 815/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0185 - f1_m: 0.7302 - recall_m: 0.6383 - val_loss: 0.0257 - val_f1_m: 0.6474 - val_recall_m: 0.5840\n",
            "Epoch 816/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0184 - f1_m: 0.7403 - recall_m: 0.6643 - val_loss: 0.0258 - val_f1_m: 0.6531 - val_recall_m: 0.5949\n",
            "Epoch 817/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0184 - f1_m: 0.7440 - recall_m: 0.6752 - val_loss: 0.0258 - val_f1_m: 0.6301 - val_recall_m: 0.5441\n",
            "Epoch 818/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0185 - f1_m: 0.7267 - recall_m: 0.6283 - val_loss: 0.0261 - val_f1_m: 0.6620 - val_recall_m: 0.6230\n",
            "Epoch 819/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0186 - f1_m: 0.7518 - recall_m: 0.7025 - val_loss: 0.0259 - val_f1_m: 0.6163 - val_recall_m: 0.5165\n",
            "Epoch 820/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0188 - f1_m: 0.7119 - recall_m: 0.5980 - val_loss: 0.0263 - val_f1_m: 0.6665 - val_recall_m: 0.6363\n",
            "Epoch 821/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0188 - f1_m: 0.7531 - recall_m: 0.7126 - val_loss: 0.0260 - val_f1_m: 0.6113 - val_recall_m: 0.5101\n",
            "Epoch 822/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0189 - f1_m: 0.7072 - recall_m: 0.5901 - val_loss: 0.0262 - val_f1_m: 0.6668 - val_recall_m: 0.6333\n",
            "Epoch 823/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0187 - f1_m: 0.7536 - recall_m: 0.7098 - val_loss: 0.0258 - val_f1_m: 0.6269 - val_recall_m: 0.5357\n",
            "Epoch 824/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0185 - f1_m: 0.7227 - recall_m: 0.6173 - val_loss: 0.0258 - val_f1_m: 0.6547 - val_recall_m: 0.6003\n",
            "Epoch 825/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0184 - f1_m: 0.7475 - recall_m: 0.6836 - val_loss: 0.0257 - val_f1_m: 0.6450 - val_recall_m: 0.5766\n",
            "Epoch 826/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0183 - f1_m: 0.7389 - recall_m: 0.6588 - val_loss: 0.0257 - val_f1_m: 0.6362 - val_recall_m: 0.5594\n",
            "Epoch 827/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0183 - f1_m: 0.7334 - recall_m: 0.6446 - val_loss: 0.0259 - val_f1_m: 0.6564 - val_recall_m: 0.6062\n",
            "Epoch 828/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0184 - f1_m: 0.7493 - recall_m: 0.6890 - val_loss: 0.0258 - val_f1_m: 0.6242 - val_recall_m: 0.5338\n",
            "Epoch 829/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0185 - f1_m: 0.7229 - recall_m: 0.6181 - val_loss: 0.0262 - val_f1_m: 0.6660 - val_recall_m: 0.6323\n",
            "Epoch 830/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0185 - f1_m: 0.7545 - recall_m: 0.7100 - val_loss: 0.0260 - val_f1_m: 0.6135 - val_recall_m: 0.5116\n",
            "Epoch 831/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0187 - f1_m: 0.7090 - recall_m: 0.5928 - val_loss: 0.0263 - val_f1_m: 0.6686 - val_recall_m: 0.6397\n",
            "Epoch 832/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0186 - f1_m: 0.7553 - recall_m: 0.7166 - val_loss: 0.0260 - val_f1_m: 0.6135 - val_recall_m: 0.5140\n",
            "Epoch 833/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0187 - f1_m: 0.7116 - recall_m: 0.5963 - val_loss: 0.0261 - val_f1_m: 0.6675 - val_recall_m: 0.6299\n",
            "Epoch 834/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0184 - f1_m: 0.7553 - recall_m: 0.7066 - val_loss: 0.0257 - val_f1_m: 0.6313 - val_recall_m: 0.5485\n",
            "Epoch 835/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0183 - f1_m: 0.7301 - recall_m: 0.6350 - val_loss: 0.0258 - val_f1_m: 0.6490 - val_recall_m: 0.5855\n",
            "Epoch 836/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0182 - f1_m: 0.7448 - recall_m: 0.6707 - val_loss: 0.0258 - val_f1_m: 0.6515 - val_recall_m: 0.5914\n",
            "Epoch 837/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0182 - f1_m: 0.7466 - recall_m: 0.6766 - val_loss: 0.0257 - val_f1_m: 0.6346 - val_recall_m: 0.5535\n",
            "Epoch 838/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0182 - f1_m: 0.7324 - recall_m: 0.6374 - val_loss: 0.0260 - val_f1_m: 0.6640 - val_recall_m: 0.6215\n",
            "Epoch 839/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0183 - f1_m: 0.7547 - recall_m: 0.7032 - val_loss: 0.0259 - val_f1_m: 0.6208 - val_recall_m: 0.5249\n",
            "Epoch 840/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0185 - f1_m: 0.7178 - recall_m: 0.6072 - val_loss: 0.0263 - val_f1_m: 0.6674 - val_recall_m: 0.6373\n",
            "Epoch 841/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0185 - f1_m: 0.7569 - recall_m: 0.7168 - val_loss: 0.0260 - val_f1_m: 0.6091 - val_recall_m: 0.5057\n",
            "Epoch 842/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0187 - f1_m: 0.7068 - recall_m: 0.5882 - val_loss: 0.0263 - val_f1_m: 0.6701 - val_recall_m: 0.6402\n",
            "Epoch 843/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0185 - f1_m: 0.7573 - recall_m: 0.7182 - val_loss: 0.0258 - val_f1_m: 0.6239 - val_recall_m: 0.5323\n",
            "Epoch 844/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0184 - f1_m: 0.7238 - recall_m: 0.6173 - val_loss: 0.0259 - val_f1_m: 0.6599 - val_recall_m: 0.6116\n",
            "Epoch 845/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0182 - f1_m: 0.7530 - recall_m: 0.6938 - val_loss: 0.0257 - val_f1_m: 0.6455 - val_recall_m: 0.5757\n",
            "Epoch 846/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0181 - f1_m: 0.7416 - recall_m: 0.6606 - val_loss: 0.0257 - val_f1_m: 0.6423 - val_recall_m: 0.5712\n",
            "Epoch 847/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0181 - f1_m: 0.7418 - recall_m: 0.6590 - val_loss: 0.0259 - val_f1_m: 0.6588 - val_recall_m: 0.6087\n",
            "Epoch 848/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0181 - f1_m: 0.7535 - recall_m: 0.6932 - val_loss: 0.0258 - val_f1_m: 0.6293 - val_recall_m: 0.5402\n",
            "Epoch 849/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0182 - f1_m: 0.7277 - recall_m: 0.6241 - val_loss: 0.0262 - val_f1_m: 0.6686 - val_recall_m: 0.6353\n",
            "Epoch 850/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0183 - f1_m: 0.7579 - recall_m: 0.7146 - val_loss: 0.0260 - val_f1_m: 0.6129 - val_recall_m: 0.5111\n",
            "Epoch 851/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0185 - f1_m: 0.7115 - recall_m: 0.5947 - val_loss: 0.0263 - val_f1_m: 0.6668 - val_recall_m: 0.6397\n",
            "Epoch 852/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0184 - f1_m: 0.7588 - recall_m: 0.7224 - val_loss: 0.0260 - val_f1_m: 0.6139 - val_recall_m: 0.5140\n",
            "Epoch 853/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0185 - f1_m: 0.7145 - recall_m: 0.5993 - val_loss: 0.0261 - val_f1_m: 0.6693 - val_recall_m: 0.6338\n",
            "Epoch 854/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0182 - f1_m: 0.7581 - recall_m: 0.7130 - val_loss: 0.0257 - val_f1_m: 0.6317 - val_recall_m: 0.5490\n",
            "Epoch 855/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0181 - f1_m: 0.7330 - recall_m: 0.6359 - val_loss: 0.0258 - val_f1_m: 0.6526 - val_recall_m: 0.5939\n",
            "Epoch 856/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0180 - f1_m: 0.7496 - recall_m: 0.6804 - val_loss: 0.0258 - val_f1_m: 0.6561 - val_recall_m: 0.5998\n",
            "Epoch 857/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0179 - f1_m: 0.7513 - recall_m: 0.6845 - val_loss: 0.0257 - val_f1_m: 0.6345 - val_recall_m: 0.5545\n",
            "Epoch 858/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0180 - f1_m: 0.7353 - recall_m: 0.6413 - val_loss: 0.0261 - val_f1_m: 0.6656 - val_recall_m: 0.6249\n",
            "Epoch 859/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0181 - f1_m: 0.7575 - recall_m: 0.7066 - val_loss: 0.0259 - val_f1_m: 0.6193 - val_recall_m: 0.5224\n",
            "Epoch 860/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0183 - f1_m: 0.7199 - recall_m: 0.6084 - val_loss: 0.0263 - val_f1_m: 0.6696 - val_recall_m: 0.6417\n",
            "Epoch 861/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0183 - f1_m: 0.7597 - recall_m: 0.7224 - val_loss: 0.0260 - val_f1_m: 0.6167 - val_recall_m: 0.5150\n",
            "Epoch 862/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0184 - f1_m: 0.7138 - recall_m: 0.5970 - val_loss: 0.0262 - val_f1_m: 0.6693 - val_recall_m: 0.6368\n",
            "Epoch 863/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0182 - f1_m: 0.7608 - recall_m: 0.7202 - val_loss: 0.0258 - val_f1_m: 0.6284 - val_recall_m: 0.5397\n",
            "Epoch 864/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0181 - f1_m: 0.7299 - recall_m: 0.6263 - val_loss: 0.0259 - val_f1_m: 0.6617 - val_recall_m: 0.6136\n",
            "Epoch 865/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0179 - f1_m: 0.7572 - recall_m: 0.6984 - val_loss: 0.0257 - val_f1_m: 0.6502 - val_recall_m: 0.5830\n",
            "Epoch 866/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0178 - f1_m: 0.7467 - recall_m: 0.6688 - val_loss: 0.0257 - val_f1_m: 0.6441 - val_recall_m: 0.5727\n",
            "Epoch 867/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0178 - f1_m: 0.7457 - recall_m: 0.6636 - val_loss: 0.0259 - val_f1_m: 0.6629 - val_recall_m: 0.6146\n",
            "Epoch 868/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0179 - f1_m: 0.7565 - recall_m: 0.6980 - val_loss: 0.0258 - val_f1_m: 0.6280 - val_recall_m: 0.5397\n",
            "Epoch 869/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0180 - f1_m: 0.7314 - recall_m: 0.6283 - val_loss: 0.0262 - val_f1_m: 0.6691 - val_recall_m: 0.6353\n",
            "Epoch 870/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0181 - f1_m: 0.7610 - recall_m: 0.7185 - val_loss: 0.0260 - val_f1_m: 0.6168 - val_recall_m: 0.5180\n",
            "Epoch 871/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0183 - f1_m: 0.7174 - recall_m: 0.6027 - val_loss: 0.0263 - val_f1_m: 0.6711 - val_recall_m: 0.6447\n",
            "Epoch 872/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0182 - f1_m: 0.7621 - recall_m: 0.7263 - val_loss: 0.0260 - val_f1_m: 0.6160 - val_recall_m: 0.5155\n",
            "Epoch 873/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0183 - f1_m: 0.7178 - recall_m: 0.6029 - val_loss: 0.0261 - val_f1_m: 0.6712 - val_recall_m: 0.6378\n",
            "Epoch 874/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0180 - f1_m: 0.7613 - recall_m: 0.7186 - val_loss: 0.0257 - val_f1_m: 0.6354 - val_recall_m: 0.5515\n",
            "Epoch 875/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0179 - f1_m: 0.7359 - recall_m: 0.6391 - val_loss: 0.0258 - val_f1_m: 0.6593 - val_recall_m: 0.6047\n",
            "Epoch 876/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0177 - f1_m: 0.7547 - recall_m: 0.6897 - val_loss: 0.0257 - val_f1_m: 0.6512 - val_recall_m: 0.5865\n",
            "Epoch 877/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0177 - f1_m: 0.7500 - recall_m: 0.6757 - val_loss: 0.0257 - val_f1_m: 0.6447 - val_recall_m: 0.5732\n",
            "Epoch 878/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0177 - f1_m: 0.7455 - recall_m: 0.6625 - val_loss: 0.0259 - val_f1_m: 0.6642 - val_recall_m: 0.6175\n",
            "Epoch 879/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0178 - f1_m: 0.7581 - recall_m: 0.7005 - val_loss: 0.0258 - val_f1_m: 0.6318 - val_recall_m: 0.5456\n",
            "Epoch 880/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0179 - f1_m: 0.7352 - recall_m: 0.6345 - val_loss: 0.0262 - val_f1_m: 0.6714 - val_recall_m: 0.6353\n",
            "Epoch 881/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0179 - f1_m: 0.7619 - recall_m: 0.7175 - val_loss: 0.0260 - val_f1_m: 0.6229 - val_recall_m: 0.5259\n",
            "Epoch 882/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0181 - f1_m: 0.7231 - recall_m: 0.6101 - val_loss: 0.0264 - val_f1_m: 0.6711 - val_recall_m: 0.6461\n",
            "Epoch 883/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0181 - f1_m: 0.7643 - recall_m: 0.7293 - val_loss: 0.0260 - val_f1_m: 0.6173 - val_recall_m: 0.5175\n",
            "Epoch 884/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0182 - f1_m: 0.7190 - recall_m: 0.6038 - val_loss: 0.0263 - val_f1_m: 0.6730 - val_recall_m: 0.6442\n",
            "Epoch 885/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0180 - f1_m: 0.7639 - recall_m: 0.7260 - val_loss: 0.0258 - val_f1_m: 0.6281 - val_recall_m: 0.5377\n",
            "Epoch 886/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0179 - f1_m: 0.7315 - recall_m: 0.6266 - val_loss: 0.0259 - val_f1_m: 0.6653 - val_recall_m: 0.6175\n",
            "Epoch 887/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0177 - f1_m: 0.7595 - recall_m: 0.7031 - val_loss: 0.0257 - val_f1_m: 0.6517 - val_recall_m: 0.5850\n",
            "Epoch 888/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7503 - recall_m: 0.6720 - val_loss: 0.0257 - val_f1_m: 0.6539 - val_recall_m: 0.5904\n",
            "Epoch 889/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7522 - recall_m: 0.6775 - val_loss: 0.0258 - val_f1_m: 0.6590 - val_recall_m: 0.6047\n",
            "Epoch 890/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7575 - recall_m: 0.6931 - val_loss: 0.0257 - val_f1_m: 0.6402 - val_recall_m: 0.5604\n",
            "Epoch 891/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7412 - recall_m: 0.6482 - val_loss: 0.0260 - val_f1_m: 0.6707 - val_recall_m: 0.6318\n",
            "Epoch 892/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0177 - f1_m: 0.7623 - recall_m: 0.7137 - val_loss: 0.0259 - val_f1_m: 0.6271 - val_recall_m: 0.5338\n",
            "Epoch 893/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0179 - f1_m: 0.7295 - recall_m: 0.6214 - val_loss: 0.0263 - val_f1_m: 0.6730 - val_recall_m: 0.6461\n",
            "Epoch 894/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0179 - f1_m: 0.7654 - recall_m: 0.7295 - val_loss: 0.0261 - val_f1_m: 0.6134 - val_recall_m: 0.5131\n",
            "Epoch 895/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0181 - f1_m: 0.7169 - recall_m: 0.5995 - val_loss: 0.0264 - val_f1_m: 0.6730 - val_recall_m: 0.6501\n",
            "Epoch 896/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0180 - f1_m: 0.7665 - recall_m: 0.7346 - val_loss: 0.0259 - val_f1_m: 0.6231 - val_recall_m: 0.5264\n",
            "Epoch 897/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0179 - f1_m: 0.7250 - recall_m: 0.6122 - val_loss: 0.0261 - val_f1_m: 0.6712 - val_recall_m: 0.6353\n",
            "Epoch 898/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0177 - f1_m: 0.7639 - recall_m: 0.7199 - val_loss: 0.0257 - val_f1_m: 0.6367 - val_recall_m: 0.5554\n",
            "Epoch 899/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7406 - recall_m: 0.6473 - val_loss: 0.0258 - val_f1_m: 0.6627 - val_recall_m: 0.6097\n",
            "Epoch 900/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0175 - f1_m: 0.7588 - recall_m: 0.6945 - val_loss: 0.0257 - val_f1_m: 0.6560 - val_recall_m: 0.5939\n",
            "Epoch 901/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0174 - f1_m: 0.7569 - recall_m: 0.6846 - val_loss: 0.0257 - val_f1_m: 0.6453 - val_recall_m: 0.5717\n",
            "Epoch 902/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0175 - f1_m: 0.7486 - recall_m: 0.6632 - val_loss: 0.0259 - val_f1_m: 0.6693 - val_recall_m: 0.6249\n",
            "Epoch 903/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0175 - f1_m: 0.7624 - recall_m: 0.7089 - val_loss: 0.0258 - val_f1_m: 0.6305 - val_recall_m: 0.5416\n",
            "Epoch 904/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7352 - recall_m: 0.6316 - val_loss: 0.0262 - val_f1_m: 0.6736 - val_recall_m: 0.6427\n",
            "Epoch 905/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0177 - f1_m: 0.7670 - recall_m: 0.7284 - val_loss: 0.0260 - val_f1_m: 0.6209 - val_recall_m: 0.5234\n",
            "Epoch 906/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0179 - f1_m: 0.7240 - recall_m: 0.6104 - val_loss: 0.0264 - val_f1_m: 0.6728 - val_recall_m: 0.6506\n",
            "Epoch 907/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0178 - f1_m: 0.7677 - recall_m: 0.7375 - val_loss: 0.0260 - val_f1_m: 0.6183 - val_recall_m: 0.5190\n",
            "Epoch 908/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0180 - f1_m: 0.7215 - recall_m: 0.6056 - val_loss: 0.0262 - val_f1_m: 0.6730 - val_recall_m: 0.6447\n",
            "Epoch 909/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0177 - f1_m: 0.7675 - recall_m: 0.7311 - val_loss: 0.0257 - val_f1_m: 0.6334 - val_recall_m: 0.5481\n",
            "Epoch 910/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0175 - f1_m: 0.7387 - recall_m: 0.6385 - val_loss: 0.0258 - val_f1_m: 0.6652 - val_recall_m: 0.6141\n",
            "Epoch 911/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0174 - f1_m: 0.7613 - recall_m: 0.7013 - val_loss: 0.0257 - val_f1_m: 0.6548 - val_recall_m: 0.5914\n",
            "Epoch 912/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0173 - f1_m: 0.7567 - recall_m: 0.6824 - val_loss: 0.0257 - val_f1_m: 0.6527 - val_recall_m: 0.5840\n",
            "Epoch 913/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0173 - f1_m: 0.7528 - recall_m: 0.6728 - val_loss: 0.0259 - val_f1_m: 0.6668 - val_recall_m: 0.6205\n",
            "Epoch 914/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0174 - f1_m: 0.7629 - recall_m: 0.7068 - val_loss: 0.0258 - val_f1_m: 0.6352 - val_recall_m: 0.5481\n",
            "Epoch 915/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0175 - f1_m: 0.7395 - recall_m: 0.6394 - val_loss: 0.0262 - val_f1_m: 0.6737 - val_recall_m: 0.6427\n",
            "Epoch 916/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7685 - recall_m: 0.7293 - val_loss: 0.0260 - val_f1_m: 0.6245 - val_recall_m: 0.5274\n",
            "Epoch 917/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0178 - f1_m: 0.7263 - recall_m: 0.6135 - val_loss: 0.0264 - val_f1_m: 0.6724 - val_recall_m: 0.6516\n",
            "Epoch 918/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0177 - f1_m: 0.7693 - recall_m: 0.7414 - val_loss: 0.0261 - val_f1_m: 0.6184 - val_recall_m: 0.5180\n",
            "Epoch 919/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0179 - f1_m: 0.7225 - recall_m: 0.6064 - val_loss: 0.0263 - val_f1_m: 0.6735 - val_recall_m: 0.6481\n",
            "Epoch 920/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7696 - recall_m: 0.7363 - val_loss: 0.0258 - val_f1_m: 0.6315 - val_recall_m: 0.5421\n",
            "Epoch 921/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0175 - f1_m: 0.7365 - recall_m: 0.6323 - val_loss: 0.0259 - val_f1_m: 0.6683 - val_recall_m: 0.6235\n",
            "Epoch 922/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0173 - f1_m: 0.7644 - recall_m: 0.7089 - val_loss: 0.0257 - val_f1_m: 0.6590 - val_recall_m: 0.5968\n",
            "Epoch 923/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0172 - f1_m: 0.7596 - recall_m: 0.6864 - val_loss: 0.0257 - val_f1_m: 0.6527 - val_recall_m: 0.5840\n",
            "Epoch 924/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0172 - f1_m: 0.7548 - recall_m: 0.6737 - val_loss: 0.0259 - val_f1_m: 0.6693 - val_recall_m: 0.6249\n",
            "Epoch 925/1000\n",
            "1217429/1217429 [==============================] - 4s 4us/step - loss: 0.0173 - f1_m: 0.7652 - recall_m: 0.7124 - val_loss: 0.0258 - val_f1_m: 0.6337 - val_recall_m: 0.5461\n",
            "Epoch 926/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0174 - f1_m: 0.7381 - recall_m: 0.6364 - val_loss: 0.0263 - val_f1_m: 0.6749 - val_recall_m: 0.6471\n",
            "Epoch 927/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0175 - f1_m: 0.7702 - recall_m: 0.7350 - val_loss: 0.0260 - val_f1_m: 0.6220 - val_recall_m: 0.5239\n",
            "Epoch 928/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0177 - f1_m: 0.7246 - recall_m: 0.6098 - val_loss: 0.0264 - val_f1_m: 0.6736 - val_recall_m: 0.6530\n",
            "Epoch 929/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7706 - recall_m: 0.7431 - val_loss: 0.0260 - val_f1_m: 0.6239 - val_recall_m: 0.5278\n",
            "Epoch 930/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7302 - recall_m: 0.6191 - val_loss: 0.0261 - val_f1_m: 0.6710 - val_recall_m: 0.6363\n",
            "Epoch 931/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0174 - f1_m: 0.7706 - recall_m: 0.7277 - val_loss: 0.0257 - val_f1_m: 0.6409 - val_recall_m: 0.5604\n",
            "Epoch 932/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0172 - f1_m: 0.7472 - recall_m: 0.6538 - val_loss: 0.0258 - val_f1_m: 0.6674 - val_recall_m: 0.6156\n",
            "Epoch 933/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0171 - f1_m: 0.7652 - recall_m: 0.7036 - val_loss: 0.0257 - val_f1_m: 0.6594 - val_recall_m: 0.5998\n",
            "Epoch 934/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0171 - f1_m: 0.7618 - recall_m: 0.6925 - val_loss: 0.0257 - val_f1_m: 0.6525 - val_recall_m: 0.5821\n",
            "Epoch 935/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0171 - f1_m: 0.7549 - recall_m: 0.6719 - val_loss: 0.0259 - val_f1_m: 0.6714 - val_recall_m: 0.6289\n",
            "Epoch 936/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0172 - f1_m: 0.7681 - recall_m: 0.7174 - val_loss: 0.0258 - val_f1_m: 0.6347 - val_recall_m: 0.5476\n",
            "Epoch 937/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0173 - f1_m: 0.7399 - recall_m: 0.6388 - val_loss: 0.0262 - val_f1_m: 0.6741 - val_recall_m: 0.6447\n",
            "Epoch 938/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0174 - f1_m: 0.7717 - recall_m: 0.7351 - val_loss: 0.0260 - val_f1_m: 0.6219 - val_recall_m: 0.5244\n",
            "Epoch 939/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7263 - recall_m: 0.6123 - val_loss: 0.0265 - val_f1_m: 0.6749 - val_recall_m: 0.6560\n",
            "Epoch 940/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0175 - f1_m: 0.7723 - recall_m: 0.7472 - val_loss: 0.0261 - val_f1_m: 0.6197 - val_recall_m: 0.5205\n",
            "Epoch 941/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7260 - recall_m: 0.6101 - val_loss: 0.0263 - val_f1_m: 0.6745 - val_recall_m: 0.6471\n",
            "Epoch 942/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0173 - f1_m: 0.7727 - recall_m: 0.7381 - val_loss: 0.0258 - val_f1_m: 0.6378 - val_recall_m: 0.5525\n",
            "Epoch 943/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0172 - f1_m: 0.7419 - recall_m: 0.6416 - val_loss: 0.0259 - val_f1_m: 0.6705 - val_recall_m: 0.6244\n",
            "Epoch 944/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0170 - f1_m: 0.7684 - recall_m: 0.7130 - val_loss: 0.0257 - val_f1_m: 0.6535 - val_recall_m: 0.5860\n",
            "Epoch 945/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0170 - f1_m: 0.7596 - recall_m: 0.6805 - val_loss: 0.0257 - val_f1_m: 0.6597 - val_recall_m: 0.5944\n",
            "Epoch 946/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0170 - f1_m: 0.7604 - recall_m: 0.6841 - val_loss: 0.0259 - val_f1_m: 0.6711 - val_recall_m: 0.6254\n",
            "Epoch 947/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0170 - f1_m: 0.7684 - recall_m: 0.7142 - val_loss: 0.0258 - val_f1_m: 0.6431 - val_recall_m: 0.5604\n",
            "Epoch 948/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0171 - f1_m: 0.7468 - recall_m: 0.6502 - val_loss: 0.0262 - val_f1_m: 0.6734 - val_recall_m: 0.6417\n",
            "Epoch 949/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0172 - f1_m: 0.7727 - recall_m: 0.7342 - val_loss: 0.0260 - val_f1_m: 0.6214 - val_recall_m: 0.5259\n",
            "Epoch 950/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0175 - f1_m: 0.7291 - recall_m: 0.6166 - val_loss: 0.0265 - val_f1_m: 0.6760 - val_recall_m: 0.6570\n",
            "Epoch 951/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0174 - f1_m: 0.7732 - recall_m: 0.7479 - val_loss: 0.0261 - val_f1_m: 0.6205 - val_recall_m: 0.5209\n",
            "Epoch 952/1000\n",
            "1217429/1217429 [==============================] - 5s 4us/step - loss: 0.0176 - f1_m: 0.7250 - recall_m: 0.6083 - val_loss: 0.0264 - val_f1_m: 0.6743 - val_recall_m: 0.6506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9gB_dwZ3jkI",
        "colab_type": "text"
      },
      "source": [
        "# 訓練過程評估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S970Vc8k0cp3",
        "colab_type": "code",
        "outputId": "6cc3215f-71e9-4fa0-ef7d-2295cc2574f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n=1\n",
        "\n",
        "fig, ax = plt.subplots(1,1)\n",
        "ax.plot(history.history['loss'][n:], color='b', label=\"Training loss\")\n",
        "ax.plot(history.history['val_loss'][n:], color='r', label=\"validation loss\")\n",
        "legend = ax.legend(loc='best', shadow=True)\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(1,1)\n",
        "ax.plot(history.history['f1_m'][n:], color='b', label=\"Training f1\")\n",
        "ax.plot(history.history['val_f1_m'][n:], color='r', label=\"validation f1\")\n",
        "legend = ax.legend(loc='best', shadow=True)\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAUjhLiIyAP1//TcuyquOB5wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCxMeMPw4Zkq",
        "colab_type": "text"
      },
      "source": [
        "# val_2's F1 report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kASFgsao4aF_",
        "colab_type": "code",
        "outputId": "1340285e-df65-4733-9ff3-15deeece8a4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "from sklearn.metrics import classification_report \n",
        "\n",
        "val_2_pred_01 = pd.Series(model.predict(val_2[features]).ravel()).apply(round)\n",
        "\n",
        "print(classification_report(val_2[y_name],val_2_pred_01,target_names=['0','1']))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00    150117\n",
            "           1       0.75      0.57      0.65      2062\n",
            "\n",
            "    accuracy                           0.99    152179\n",
            "   macro avg       0.87      0.78      0.82    152179\n",
            "weighted avg       0.99      0.99      0.99    152179\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6PFh2lCBwBG",
        "colab_type": "text"
      },
      "source": [
        "# 產生submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNZ2Eing7kUS",
        "colab_type": "code",
        "outputId": "55513da6-9261-46d7-fbde-3b953004da14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "submission = pd.DataFrame({\"txkey\":test_txkey,\n",
        "                           \"fraud_ind\":pd.Series(model.predict(test[features]).ravel()).apply(round).values})\n",
        "\n",
        "# value_counts\n",
        "print(submission[\"fraud_ind\"].value_counts())\n",
        "submission.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    416219\n",
            "1      5446\n",
            "Name: fraud_ind, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>txkey</th>\n",
              "      <th>fraud_ind</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1521787</th>\n",
              "      <td>592489</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521788</th>\n",
              "      <td>592452</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521789</th>\n",
              "      <td>590212</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521790</th>\n",
              "      <td>590209</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521791</th>\n",
              "      <td>592488</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          txkey  fraud_ind\n",
              "1521787  592489          0\n",
              "1521788  592452          0\n",
              "1521789  590212          0\n",
              "1521790  590209          0\n",
              "1521791  592488          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1duNRbRlDgAG",
        "colab_type": "text"
      },
      "source": [
        "# 這裡有個想法如果test上盜刷的比例 跟train上面盜刷的比例 愈相近愈好"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNz1_v5aB33k",
        "colab_type": "code",
        "outputId": "1c1520d9-e052-4d1b-d069-516138a83173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "train_p = round((train['fraud_ind'].value_counts()[1] / #盜刷樣本數\n",
        "                len(train['fraud_ind']))*100 ,5) #總數\n",
        "\n",
        "val_1_p = round((val_1['fraud_ind'].value_counts()[1] / #盜刷樣本數\n",
        "                len(val_1['fraud_ind']))*100 ,5) #總數\n",
        "\n",
        "val_2_p = round((val_2['fraud_ind'].value_counts()[1] / #盜刷樣本數\n",
        "                len(val_2['fraud_ind']))*100 ,5) #總數\n",
        "\n",
        "test_p = round((submission['fraud_ind'].value_counts()[1] / #盜刷樣本數\n",
        "                len(submission['fraud_ind']))*100 ,5) #總數\n",
        "pd.DataFrame({'train_p':train_p,\n",
        "              'val_1_p':val_1_p,\n",
        "              'val_2_p':val_2_p,\n",
        "              'test_p':test_p},\n",
        "               index=['盜刷比例%'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_p</th>\n",
              "      <th>val_1_p</th>\n",
              "      <th>val_2_p</th>\n",
              "      <th>test_p</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>盜刷比例%</th>\n",
              "      <td>1.33593</td>\n",
              "      <td>1.3333</td>\n",
              "      <td>1.35498</td>\n",
              "      <td>1.29155</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       train_p  val_1_p  val_2_p   test_p\n",
              "盜刷比例%  1.33593   1.3333  1.35498  1.29155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNMCpHJ8Dkqc",
        "colab_type": "text"
      },
      "source": [
        "# 保存預測結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSoM9qNeDhyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv(\"./submission_ANN_1_29155.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg3PQVLODmUt",
        "colab_type": "code",
        "outputId": "ffe3d952-4caa-41b0-83b2-9840da28097e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(submission[\"fraud_ind\"].value_counts())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    416219\n",
            "1      5446\n",
            "Name: fraud_ind, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix3KqTveDr_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}