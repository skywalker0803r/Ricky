{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "玉山AI比賽_modeling_ANN版本.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skywalker0803r/Ricky/blob/master/%E7%8E%89%E5%B1%B1AI%E6%AF%94%E8%B3%BD_modeling_ANN%E7%89%88%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr44CknfvyTj",
        "colab_type": "code",
        "outputId": "baff2456-116c-4f66-e8dc-f299b5275e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import warnings \n",
        "warnings.simplefilter('ignore')\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbAV7ns3v8LW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "中文map = {'bacno':'歸戶帳號','txkey':'交易序號','locdt':'授權日期','loctm':'授權時間','cano':'交易卡號',\n",
        "         'contp':'交易類別','etymd':'交易型態','mchno':'特店代號','acqic':'收單行代碼','mcc':'MCC_CODE',\n",
        "         'conam':'交易金額-台幣(經過轉換)','ecfg':'網路交易註記','insfg':'分期交易註記','iterm':'分期期數',\n",
        "         'stocn':'消費地國別','scity':'消費城市','stscd':'狀態碼','ovrlt':'超額註記碼','flbmk':'Fallback註記',\n",
        "         'hcefg':'支付型態','csmcu':'消費地幣別','flg_3dsmk':'3DS交易註記','fraud_ind':'盜刷註記'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJs7NefWwDKv",
        "colab_type": "text"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Baq7Qt00wBmh",
        "colab_type": "code",
        "outputId": "3ab55d4b-3530-4b20-9c44-98c47d897e2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"/content/drive/My Drive/玉山人工智慧比賽數據/train_特徵工程完.csv\",index_col=0)\n",
        "print(train.shape)\n",
        "train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1521787, 122)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stscd</th>\n",
              "      <th>ecfg</th>\n",
              "      <th>stocn</th>\n",
              "      <th>acqic</th>\n",
              "      <th>etymd</th>\n",
              "      <th>loctm</th>\n",
              "      <th>scity</th>\n",
              "      <th>hcefg</th>\n",
              "      <th>contp</th>\n",
              "      <th>conam</th>\n",
              "      <th>insfg</th>\n",
              "      <th>ovrlt</th>\n",
              "      <th>flg_3dsmk</th>\n",
              "      <th>iterm</th>\n",
              "      <th>csmcu</th>\n",
              "      <th>mchno</th>\n",
              "      <th>cano</th>\n",
              "      <th>mcc</th>\n",
              "      <th>flbmk</th>\n",
              "      <th>bacno</th>\n",
              "      <th>mchno_stscd_2_norm_count</th>\n",
              "      <th>mchno_stscd_0_norm_count</th>\n",
              "      <th>acqic_stscd_2_norm_count</th>\n",
              "      <th>acqic_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_2_norm_count</th>\n",
              "      <th>cano_stscd_0_norm_count</th>\n",
              "      <th>acqic_scity_nunique</th>\n",
              "      <th>acqic_csmcu_nunique</th>\n",
              "      <th>bacno_stscd_2_norm_count</th>\n",
              "      <th>bacno_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_nunique</th>\n",
              "      <th>acqic_etymd_8_norm_count</th>\n",
              "      <th>mchno_etymd_8_norm_count</th>\n",
              "      <th>acqic_stocn_nunique</th>\n",
              "      <th>acqic_hcefg_nunique</th>\n",
              "      <th>bacno_stscd_nunique</th>\n",
              "      <th>acqic_ovrlt_0_norm_count</th>\n",
              "      <th>acqic_ovrlt_1_norm_count</th>\n",
              "      <th>acqic_etymd_5_norm_count</th>\n",
              "      <th>acqic_bacno_nunique</th>\n",
              "      <th>...</th>\n",
              "      <th>acqic_iterm_3_norm_count</th>\n",
              "      <th>acqic_hcefg_7_norm_count</th>\n",
              "      <th>mchno_ovrlt_nunique</th>\n",
              "      <th>mchno_etymd_nunique</th>\n",
              "      <th>acqic_hcefg_8_norm_count</th>\n",
              "      <th>bacno_scity_nunique</th>\n",
              "      <th>mchno_stocn_nunique</th>\n",
              "      <th>acqic_hcefg_1_norm_count</th>\n",
              "      <th>mchno_etymd_7_norm_count</th>\n",
              "      <th>acqic_hcefg_5_norm_count</th>\n",
              "      <th>bacno_etymd_4_norm_count</th>\n",
              "      <th>cano_mchno_nunique</th>\n",
              "      <th>acqic_etymd_10_norm_count</th>\n",
              "      <th>acqic_etymd_7_norm_count</th>\n",
              "      <th>mchno_etymd_0_norm_count</th>\n",
              "      <th>cano_etymd_0_norm_count</th>\n",
              "      <th>acqic_etymd_0_norm_count</th>\n",
              "      <th>cano_etymd_2_norm_count</th>\n",
              "      <th>mchno_iterm_nunique</th>\n",
              "      <th>mchno_hcefg_5_norm_count</th>\n",
              "      <th>cano_iterm_nunique</th>\n",
              "      <th>acqic_hcefg_2_norm_count</th>\n",
              "      <th>mchno_hcefg_1_norm_count</th>\n",
              "      <th>mchno_contp_6_norm_count</th>\n",
              "      <th>cano_scity_nunique</th>\n",
              "      <th>cano_contp_nunique</th>\n",
              "      <th>mchno_contp_2_norm_count</th>\n",
              "      <th>cano_contp_2_norm_count</th>\n",
              "      <th>mchno_contp_5_norm_count</th>\n",
              "      <th>cano_hcefg_nunique</th>\n",
              "      <th>bacno_etymd_0_norm_count</th>\n",
              "      <th>cano_mcc_nunique</th>\n",
              "      <th>acqic_hcefg_0_norm_count</th>\n",
              "      <th>bacno_acqic_nunique</th>\n",
              "      <th>mchno_iterm_0_norm_count</th>\n",
              "      <th>mchno_hcefg_0_norm_count</th>\n",
              "      <th>cano_hcefg_5_norm_count</th>\n",
              "      <th>cano_etymd_6_norm_count</th>\n",
              "      <th>txkey</th>\n",
              "      <th>fraud_ind</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>61954</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59034</td>\n",
              "      <td>37846</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>112785</td>\n",
              "      <td>0.001044</td>\n",
              "      <td>0.998956</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>33</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.212500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.362500</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989928</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.212500</td>\n",
              "      <td>21</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>18</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>516056</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>38216</td>\n",
              "      <td>5795</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>13693</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>45476</td>\n",
              "      <td>451</td>\n",
              "      <td>0</td>\n",
              "      <td>133951</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.997536</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.994768</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.991720</td>\n",
              "      <td>0.008280</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>19252</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0.003276</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.042083</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006682</td>\n",
              "      <td>0.391304</td>\n",
              "      <td>11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024906</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.008377</td>\n",
              "      <td>0.434783</td>\n",
              "      <td>1</td>\n",
              "      <td>0.034542</td>\n",
              "      <td>2</td>\n",
              "      <td>0.013267</td>\n",
              "      <td>0.039878</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.968283</td>\n",
              "      <td>2</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>8</td>\n",
              "      <td>0.931187</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.911000</td>\n",
              "      <td>0.652174</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4376</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>54640</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59034</td>\n",
              "      <td>187354</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>15350</td>\n",
              "      <td>0.001044</td>\n",
              "      <td>0.998956</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989928</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>7</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.0</td>\n",
              "      <td>483434</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6697</td>\n",
              "      <td>5</td>\n",
              "      <td>62128</td>\n",
              "      <td>3267</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>40413</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>62</td>\n",
              "      <td>50185</td>\n",
              "      <td>29812</td>\n",
              "      <td>247</td>\n",
              "      <td>0</td>\n",
              "      <td>156492</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.999643</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>101</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.142619</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.988238</td>\n",
              "      <td>0.011762</td>\n",
              "      <td>0.312315</td>\n",
              "      <td>74622</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002037</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0.003765</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.024410</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.965999</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.245614</td>\n",
              "      <td>5</td>\n",
              "      <td>0.984111</td>\n",
              "      <td>2</td>\n",
              "      <td>0.001563</td>\n",
              "      <td>0.012910</td>\n",
              "      <td>0.010924</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035088</td>\n",
              "      <td>0.989076</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>8</td>\n",
              "      <td>0.861966</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1407164</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>5959</td>\n",
              "      <td>4</td>\n",
              "      <td>65231</td>\n",
              "      <td>5795</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>25962</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>62</td>\n",
              "      <td>93290</td>\n",
              "      <td>80881</td>\n",
              "      <td>263</td>\n",
              "      <td>0</td>\n",
              "      <td>105534</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.999894</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.077908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.990016</td>\n",
              "      <td>0.009984</td>\n",
              "      <td>0.206081</td>\n",
              "      <td>41913</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009496</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.982903</td>\n",
              "      <td>0.089744</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365079</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079365</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000707</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.079365</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.984127</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1051004</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 122 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   stscd  ecfg  stocn  ...  cano_etymd_6_norm_count    txkey  fraud_ind\n",
              "0      0     0    102  ...                      0.0   516056          0\n",
              "1      0     0    102  ...                      0.0     4376          0\n",
              "2      0     0    102  ...                      0.0   483434          0\n",
              "3      0     0    102  ...                      0.0  1407164          0\n",
              "4      0     0    102  ...                      0.0  1051004          0\n",
              "\n",
              "[5 rows x 122 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmQm34KPwGu_",
        "colab_type": "text"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTm0HGGuwGCm",
        "colab_type": "code",
        "outputId": "4f86954d-8ab2-4b5b-8de8-e63e2da82e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "test = pd.read_csv(\"/content/drive/My Drive/玉山人工智慧比賽數據/test_特徵工程完.csv\",index_col=0)\n",
        "test_txkey = test[\"txkey\"]\n",
        "print(test.shape)\n",
        "test.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(421665, 121)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stscd</th>\n",
              "      <th>ecfg</th>\n",
              "      <th>stocn</th>\n",
              "      <th>acqic</th>\n",
              "      <th>etymd</th>\n",
              "      <th>loctm</th>\n",
              "      <th>scity</th>\n",
              "      <th>hcefg</th>\n",
              "      <th>contp</th>\n",
              "      <th>conam</th>\n",
              "      <th>insfg</th>\n",
              "      <th>ovrlt</th>\n",
              "      <th>flg_3dsmk</th>\n",
              "      <th>iterm</th>\n",
              "      <th>csmcu</th>\n",
              "      <th>mchno</th>\n",
              "      <th>cano</th>\n",
              "      <th>mcc</th>\n",
              "      <th>flbmk</th>\n",
              "      <th>bacno</th>\n",
              "      <th>mchno_stscd_2_norm_count</th>\n",
              "      <th>mchno_stscd_0_norm_count</th>\n",
              "      <th>acqic_stscd_2_norm_count</th>\n",
              "      <th>acqic_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_2_norm_count</th>\n",
              "      <th>cano_stscd_0_norm_count</th>\n",
              "      <th>acqic_scity_nunique</th>\n",
              "      <th>acqic_csmcu_nunique</th>\n",
              "      <th>bacno_stscd_2_norm_count</th>\n",
              "      <th>bacno_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_nunique</th>\n",
              "      <th>acqic_etymd_8_norm_count</th>\n",
              "      <th>mchno_etymd_8_norm_count</th>\n",
              "      <th>acqic_stocn_nunique</th>\n",
              "      <th>acqic_hcefg_nunique</th>\n",
              "      <th>bacno_stscd_nunique</th>\n",
              "      <th>acqic_ovrlt_0_norm_count</th>\n",
              "      <th>acqic_ovrlt_1_norm_count</th>\n",
              "      <th>acqic_etymd_5_norm_count</th>\n",
              "      <th>acqic_bacno_nunique</th>\n",
              "      <th>...</th>\n",
              "      <th>cano_etymd_4_norm_count</th>\n",
              "      <th>acqic_iterm_3_norm_count</th>\n",
              "      <th>acqic_hcefg_7_norm_count</th>\n",
              "      <th>mchno_ovrlt_nunique</th>\n",
              "      <th>mchno_etymd_nunique</th>\n",
              "      <th>acqic_hcefg_8_norm_count</th>\n",
              "      <th>bacno_scity_nunique</th>\n",
              "      <th>mchno_stocn_nunique</th>\n",
              "      <th>acqic_hcefg_1_norm_count</th>\n",
              "      <th>mchno_etymd_7_norm_count</th>\n",
              "      <th>acqic_hcefg_5_norm_count</th>\n",
              "      <th>bacno_etymd_4_norm_count</th>\n",
              "      <th>cano_mchno_nunique</th>\n",
              "      <th>acqic_etymd_10_norm_count</th>\n",
              "      <th>acqic_etymd_7_norm_count</th>\n",
              "      <th>mchno_etymd_0_norm_count</th>\n",
              "      <th>cano_etymd_0_norm_count</th>\n",
              "      <th>acqic_etymd_0_norm_count</th>\n",
              "      <th>cano_etymd_2_norm_count</th>\n",
              "      <th>mchno_iterm_nunique</th>\n",
              "      <th>mchno_hcefg_5_norm_count</th>\n",
              "      <th>cano_iterm_nunique</th>\n",
              "      <th>acqic_hcefg_2_norm_count</th>\n",
              "      <th>mchno_hcefg_1_norm_count</th>\n",
              "      <th>mchno_contp_6_norm_count</th>\n",
              "      <th>cano_scity_nunique</th>\n",
              "      <th>cano_contp_nunique</th>\n",
              "      <th>mchno_contp_2_norm_count</th>\n",
              "      <th>cano_contp_2_norm_count</th>\n",
              "      <th>mchno_contp_5_norm_count</th>\n",
              "      <th>cano_hcefg_nunique</th>\n",
              "      <th>bacno_etymd_0_norm_count</th>\n",
              "      <th>cano_mcc_nunique</th>\n",
              "      <th>acqic_hcefg_0_norm_count</th>\n",
              "      <th>bacno_acqic_nunique</th>\n",
              "      <th>mchno_iterm_0_norm_count</th>\n",
              "      <th>mchno_hcefg_0_norm_count</th>\n",
              "      <th>cano_hcefg_5_norm_count</th>\n",
              "      <th>cano_etymd_6_norm_count</th>\n",
              "      <th>txkey</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1521787</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>77950</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>592489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521788</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>79549</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>592452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521789</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>60355</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>590212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521790</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>60296</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>590209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521791</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>77933</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>592488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 121 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         stscd  ecfg  ...  cano_etymd_6_norm_count   txkey\n",
              "1521787      0     0  ...                      0.0  592489\n",
              "1521788      0     0  ...                      0.0  592452\n",
              "1521789      0     0  ...                      0.0  590212\n",
              "1521790      0     0  ...                      0.0  590209\n",
              "1521791      0     0  ...                      0.0  592488\n",
              "\n",
              "[5 rows x 121 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq9VQAwtwMPr",
        "colab_type": "text"
      },
      "source": [
        "# 定義 features & num_features & target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTyemZPdwJR2",
        "colab_type": "code",
        "outputId": "a2c3cee6-ab95-44e7-f641-d9c866857fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# 全部特徵\n",
        "features = train.drop(['fraud_ind', # just target\n",
        "                       'txkey', # just like index\n",
        "                       ],axis=1).columns.tolist()\n",
        "\n",
        "# 新特徵才是num_features\n",
        "num_features = sorted(list(set(features)^set(中文map.keys())))\n",
        "num_features.remove('fraud_ind')\n",
        "num_features.remove('locdt')\n",
        "num_features.remove('txkey')\n",
        "\n",
        "#只用num_features\n",
        "features = num_features\n",
        "\n",
        "y_name = 'fraud_ind'\n",
        "\n",
        "print(len(features),features)\n",
        "print(len(num_features),num_features)\n",
        "print(len([y_name]),[y_name])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 ['acqic_bacno_nunique', 'acqic_cano_nunique', 'acqic_contp_2_norm_count', 'acqic_contp_5_norm_count', 'acqic_contp_6_norm_count', 'acqic_contp_nunique', 'acqic_csmcu_nunique', 'acqic_etymd_0_norm_count', 'acqic_etymd_10_norm_count', 'acqic_etymd_2_norm_count', 'acqic_etymd_4_norm_count', 'acqic_etymd_5_norm_count', 'acqic_etymd_6_norm_count', 'acqic_etymd_7_norm_count', 'acqic_etymd_8_norm_count', 'acqic_etymd_nunique', 'acqic_hcefg_0_norm_count', 'acqic_hcefg_1_norm_count', 'acqic_hcefg_2_norm_count', 'acqic_hcefg_5_norm_count', 'acqic_hcefg_7_norm_count', 'acqic_hcefg_8_norm_count', 'acqic_hcefg_9_norm_count', 'acqic_hcefg_nunique', 'acqic_iterm_0_norm_count', 'acqic_iterm_1_norm_count', 'acqic_iterm_2_norm_count', 'acqic_iterm_3_norm_count', 'acqic_iterm_4_norm_count', 'acqic_iterm_5_norm_count', 'acqic_iterm_6_norm_count', 'acqic_iterm_7_norm_count', 'acqic_iterm_8_norm_count', 'acqic_iterm_nunique', 'acqic_mcc_nunique', 'acqic_mchno_nunique', 'acqic_ovrlt_0_norm_count', 'acqic_ovrlt_1_norm_count', 'acqic_scity_nunique', 'acqic_stocn_nunique', 'acqic_stscd_0_norm_count', 'acqic_stscd_2_norm_count', 'acqic_stscd_nunique', 'bacno_acqic_nunique', 'bacno_cano_nunique', 'bacno_csmcu_nunique', 'bacno_etymd_0_norm_count', 'bacno_etymd_4_norm_count', 'bacno_etymd_5_norm_count', 'bacno_etymd_8_norm_count', 'bacno_scity_nunique', 'bacno_stocn_nunique', 'bacno_stscd_0_norm_count', 'bacno_stscd_2_norm_count', 'bacno_stscd_nunique', 'cano_contp_2_norm_count', 'cano_contp_nunique', 'cano_csmcu_nunique', 'cano_etymd_0_norm_count', 'cano_etymd_2_norm_count', 'cano_etymd_4_norm_count', 'cano_etymd_5_norm_count', 'cano_etymd_6_norm_count', 'cano_etymd_8_norm_count', 'cano_etymd_nunique', 'cano_hcefg_5_norm_count', 'cano_hcefg_nunique', 'cano_iterm_nunique', 'cano_mcc_nunique', 'cano_mchno_nunique', 'cano_scity_nunique', 'cano_stocn_nunique', 'cano_stscd_0_norm_count', 'cano_stscd_2_norm_count', 'cano_stscd_nunique', 'mchno_contp_2_norm_count', 'mchno_contp_5_norm_count', 'mchno_contp_6_norm_count', 'mchno_contp_nunique', 'mchno_csmcu_nunique', 'mchno_etymd_0_norm_count', 'mchno_etymd_2_norm_count', 'mchno_etymd_4_norm_count', 'mchno_etymd_5_norm_count', 'mchno_etymd_7_norm_count', 'mchno_etymd_8_norm_count', 'mchno_etymd_nunique', 'mchno_hcefg_0_norm_count', 'mchno_hcefg_1_norm_count', 'mchno_hcefg_5_norm_count', 'mchno_hcefg_nunique', 'mchno_iterm_0_norm_count', 'mchno_iterm_nunique', 'mchno_ovrlt_0_norm_count', 'mchno_ovrlt_1_norm_count', 'mchno_ovrlt_nunique', 'mchno_stocn_nunique', 'mchno_stscd_0_norm_count', 'mchno_stscd_2_norm_count', 'mchno_stscd_nunique']\n",
            "100 ['acqic_bacno_nunique', 'acqic_cano_nunique', 'acqic_contp_2_norm_count', 'acqic_contp_5_norm_count', 'acqic_contp_6_norm_count', 'acqic_contp_nunique', 'acqic_csmcu_nunique', 'acqic_etymd_0_norm_count', 'acqic_etymd_10_norm_count', 'acqic_etymd_2_norm_count', 'acqic_etymd_4_norm_count', 'acqic_etymd_5_norm_count', 'acqic_etymd_6_norm_count', 'acqic_etymd_7_norm_count', 'acqic_etymd_8_norm_count', 'acqic_etymd_nunique', 'acqic_hcefg_0_norm_count', 'acqic_hcefg_1_norm_count', 'acqic_hcefg_2_norm_count', 'acqic_hcefg_5_norm_count', 'acqic_hcefg_7_norm_count', 'acqic_hcefg_8_norm_count', 'acqic_hcefg_9_norm_count', 'acqic_hcefg_nunique', 'acqic_iterm_0_norm_count', 'acqic_iterm_1_norm_count', 'acqic_iterm_2_norm_count', 'acqic_iterm_3_norm_count', 'acqic_iterm_4_norm_count', 'acqic_iterm_5_norm_count', 'acqic_iterm_6_norm_count', 'acqic_iterm_7_norm_count', 'acqic_iterm_8_norm_count', 'acqic_iterm_nunique', 'acqic_mcc_nunique', 'acqic_mchno_nunique', 'acqic_ovrlt_0_norm_count', 'acqic_ovrlt_1_norm_count', 'acqic_scity_nunique', 'acqic_stocn_nunique', 'acqic_stscd_0_norm_count', 'acqic_stscd_2_norm_count', 'acqic_stscd_nunique', 'bacno_acqic_nunique', 'bacno_cano_nunique', 'bacno_csmcu_nunique', 'bacno_etymd_0_norm_count', 'bacno_etymd_4_norm_count', 'bacno_etymd_5_norm_count', 'bacno_etymd_8_norm_count', 'bacno_scity_nunique', 'bacno_stocn_nunique', 'bacno_stscd_0_norm_count', 'bacno_stscd_2_norm_count', 'bacno_stscd_nunique', 'cano_contp_2_norm_count', 'cano_contp_nunique', 'cano_csmcu_nunique', 'cano_etymd_0_norm_count', 'cano_etymd_2_norm_count', 'cano_etymd_4_norm_count', 'cano_etymd_5_norm_count', 'cano_etymd_6_norm_count', 'cano_etymd_8_norm_count', 'cano_etymd_nunique', 'cano_hcefg_5_norm_count', 'cano_hcefg_nunique', 'cano_iterm_nunique', 'cano_mcc_nunique', 'cano_mchno_nunique', 'cano_scity_nunique', 'cano_stocn_nunique', 'cano_stscd_0_norm_count', 'cano_stscd_2_norm_count', 'cano_stscd_nunique', 'mchno_contp_2_norm_count', 'mchno_contp_5_norm_count', 'mchno_contp_6_norm_count', 'mchno_contp_nunique', 'mchno_csmcu_nunique', 'mchno_etymd_0_norm_count', 'mchno_etymd_2_norm_count', 'mchno_etymd_4_norm_count', 'mchno_etymd_5_norm_count', 'mchno_etymd_7_norm_count', 'mchno_etymd_8_norm_count', 'mchno_etymd_nunique', 'mchno_hcefg_0_norm_count', 'mchno_hcefg_1_norm_count', 'mchno_hcefg_5_norm_count', 'mchno_hcefg_nunique', 'mchno_iterm_0_norm_count', 'mchno_iterm_nunique', 'mchno_ovrlt_0_norm_count', 'mchno_ovrlt_1_norm_count', 'mchno_ovrlt_nunique', 'mchno_stocn_nunique', 'mchno_stscd_0_norm_count', 'mchno_stscd_2_norm_count', 'mchno_stscd_nunique']\n",
            "1 ['fraud_ind']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrAlrJU3xqpd",
        "colab_type": "text"
      },
      "source": [
        "# both / test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtKH9MW-wwS7",
        "colab_type": "code",
        "outputId": "05c9c0ce-2a84-4b3f-dfb6-20a747d697d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "f = {}\n",
        "for col in num_features:\n",
        "  both_value = set(train[col].unique())&set(test[col].unique())\n",
        "  f[col] = len(both_value)/len(test[col].unique())\n",
        "res = pd.DataFrame(f,index=['both/test']).T.sort_values(by='both/test',ascending=False)\n",
        "res.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>both/test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bacno_stscd_2_norm_count</th>\n",
              "      <td>0.878689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bacno_stscd_0_norm_count</th>\n",
              "      <td>0.873786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cano_stscd_0_norm_count</th>\n",
              "      <td>0.851240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bacno_cano_nunique</th>\n",
              "      <td>0.846154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cano_stscd_2_norm_count</th>\n",
              "      <td>0.843882</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          both/test\n",
              "bacno_stscd_2_norm_count   0.878689\n",
              "bacno_stscd_0_norm_count   0.873786\n",
              "cano_stscd_0_norm_count    0.851240\n",
              "bacno_cano_nunique         0.846154\n",
              "cano_stscd_2_norm_count    0.843882"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abpoOjRCx7jw",
        "colab_type": "text"
      },
      "source": [
        "# 切分 train 跟 val_1 , val_2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtSwUARSxu24",
        "colab_type": "code",
        "outputId": "5f34e059-04b1-4e6d-aaa3-d5881519906a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train ,val = train_test_split(train[features+[y_name]] ,test_size=0.25 ,random_state=42)\n",
        "val_1 ,val_2 = train_test_split(val[features+[y_name]] ,test_size=0.50 ,random_state=42)\n",
        "print(train.shape)\n",
        "print(val_1.shape)\n",
        "print(val_2.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1141340, 101)\n",
            "(190223, 101)\n",
            "(190224, 101)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5PaF81P9vn6",
        "colab_type": "text"
      },
      "source": [
        "# scaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-uO7lX79vvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler =  StandardScaler().fit(train[features])\n",
        "\n",
        "train[features] = scaler.transform(train[features])\n",
        "val_1[features] = scaler.transform(val_1[features])\n",
        "val_2[features] = scaler.transform(val_2[features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U5tWubuDIUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test[features] = scaler.transform(test[features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTr7D4bL-lwo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "90d56439-e6c0-430b-98de-d3d602005678"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acqic_bacno_nunique</th>\n",
              "      <th>acqic_cano_nunique</th>\n",
              "      <th>acqic_contp_2_norm_count</th>\n",
              "      <th>acqic_contp_5_norm_count</th>\n",
              "      <th>acqic_contp_6_norm_count</th>\n",
              "      <th>acqic_contp_nunique</th>\n",
              "      <th>acqic_csmcu_nunique</th>\n",
              "      <th>acqic_etymd_0_norm_count</th>\n",
              "      <th>acqic_etymd_10_norm_count</th>\n",
              "      <th>acqic_etymd_2_norm_count</th>\n",
              "      <th>acqic_etymd_4_norm_count</th>\n",
              "      <th>acqic_etymd_5_norm_count</th>\n",
              "      <th>acqic_etymd_6_norm_count</th>\n",
              "      <th>acqic_etymd_7_norm_count</th>\n",
              "      <th>acqic_etymd_8_norm_count</th>\n",
              "      <th>acqic_etymd_nunique</th>\n",
              "      <th>acqic_hcefg_0_norm_count</th>\n",
              "      <th>acqic_hcefg_1_norm_count</th>\n",
              "      <th>acqic_hcefg_2_norm_count</th>\n",
              "      <th>acqic_hcefg_5_norm_count</th>\n",
              "      <th>acqic_hcefg_7_norm_count</th>\n",
              "      <th>acqic_hcefg_8_norm_count</th>\n",
              "      <th>acqic_hcefg_9_norm_count</th>\n",
              "      <th>acqic_hcefg_nunique</th>\n",
              "      <th>acqic_iterm_0_norm_count</th>\n",
              "      <th>acqic_iterm_1_norm_count</th>\n",
              "      <th>acqic_iterm_2_norm_count</th>\n",
              "      <th>acqic_iterm_3_norm_count</th>\n",
              "      <th>acqic_iterm_4_norm_count</th>\n",
              "      <th>acqic_iterm_5_norm_count</th>\n",
              "      <th>acqic_iterm_6_norm_count</th>\n",
              "      <th>acqic_iterm_7_norm_count</th>\n",
              "      <th>acqic_iterm_8_norm_count</th>\n",
              "      <th>acqic_iterm_nunique</th>\n",
              "      <th>acqic_mcc_nunique</th>\n",
              "      <th>acqic_mchno_nunique</th>\n",
              "      <th>acqic_ovrlt_0_norm_count</th>\n",
              "      <th>acqic_ovrlt_1_norm_count</th>\n",
              "      <th>acqic_scity_nunique</th>\n",
              "      <th>acqic_stocn_nunique</th>\n",
              "      <th>...</th>\n",
              "      <th>cano_etymd_5_norm_count</th>\n",
              "      <th>cano_etymd_6_norm_count</th>\n",
              "      <th>cano_etymd_8_norm_count</th>\n",
              "      <th>cano_etymd_nunique</th>\n",
              "      <th>cano_hcefg_5_norm_count</th>\n",
              "      <th>cano_hcefg_nunique</th>\n",
              "      <th>cano_iterm_nunique</th>\n",
              "      <th>cano_mcc_nunique</th>\n",
              "      <th>cano_mchno_nunique</th>\n",
              "      <th>cano_scity_nunique</th>\n",
              "      <th>cano_stocn_nunique</th>\n",
              "      <th>cano_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_2_norm_count</th>\n",
              "      <th>cano_stscd_nunique</th>\n",
              "      <th>mchno_contp_2_norm_count</th>\n",
              "      <th>mchno_contp_5_norm_count</th>\n",
              "      <th>mchno_contp_6_norm_count</th>\n",
              "      <th>mchno_contp_nunique</th>\n",
              "      <th>mchno_csmcu_nunique</th>\n",
              "      <th>mchno_etymd_0_norm_count</th>\n",
              "      <th>mchno_etymd_2_norm_count</th>\n",
              "      <th>mchno_etymd_4_norm_count</th>\n",
              "      <th>mchno_etymd_5_norm_count</th>\n",
              "      <th>mchno_etymd_7_norm_count</th>\n",
              "      <th>mchno_etymd_8_norm_count</th>\n",
              "      <th>mchno_etymd_nunique</th>\n",
              "      <th>mchno_hcefg_0_norm_count</th>\n",
              "      <th>mchno_hcefg_1_norm_count</th>\n",
              "      <th>mchno_hcefg_5_norm_count</th>\n",
              "      <th>mchno_hcefg_nunique</th>\n",
              "      <th>mchno_iterm_0_norm_count</th>\n",
              "      <th>mchno_iterm_nunique</th>\n",
              "      <th>mchno_ovrlt_0_norm_count</th>\n",
              "      <th>mchno_ovrlt_1_norm_count</th>\n",
              "      <th>mchno_ovrlt_nunique</th>\n",
              "      <th>mchno_stocn_nunique</th>\n",
              "      <th>mchno_stscd_0_norm_count</th>\n",
              "      <th>mchno_stscd_2_norm_count</th>\n",
              "      <th>mchno_stscd_nunique</th>\n",
              "      <th>fraud_ind</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1093925</th>\n",
              "      <td>1.437453</td>\n",
              "      <td>1.459014</td>\n",
              "      <td>-0.437868</td>\n",
              "      <td>0.372007</td>\n",
              "      <td>0.903669</td>\n",
              "      <td>1.323815</td>\n",
              "      <td>-0.30617</td>\n",
              "      <td>-0.306401</td>\n",
              "      <td>-0.032631</td>\n",
              "      <td>-0.114440</td>\n",
              "      <td>0.573452</td>\n",
              "      <td>0.229308</td>\n",
              "      <td>-0.063776</td>\n",
              "      <td>-0.03916</td>\n",
              "      <td>-0.074035</td>\n",
              "      <td>0.836057</td>\n",
              "      <td>-0.200257</td>\n",
              "      <td>0.041273</td>\n",
              "      <td>-0.076885</td>\n",
              "      <td>0.198717</td>\n",
              "      <td>-0.425339</td>\n",
              "      <td>0.160068</td>\n",
              "      <td>-0.079314</td>\n",
              "      <td>1.133138</td>\n",
              "      <td>-2.318284</td>\n",
              "      <td>2.111778</td>\n",
              "      <td>2.473632</td>\n",
              "      <td>2.557203</td>\n",
              "      <td>2.304392</td>\n",
              "      <td>2.325461</td>\n",
              "      <td>2.388161</td>\n",
              "      <td>2.070898</td>\n",
              "      <td>2.552755</td>\n",
              "      <td>1.409658</td>\n",
              "      <td>0.652074</td>\n",
              "      <td>2.065039</td>\n",
              "      <td>0.093504</td>\n",
              "      <td>-0.093504</td>\n",
              "      <td>1.392785</td>\n",
              "      <td>-0.273246</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.934722</td>\n",
              "      <td>-0.069349</td>\n",
              "      <td>-0.391584</td>\n",
              "      <td>0.655928</td>\n",
              "      <td>0.39788</td>\n",
              "      <td>-0.600994</td>\n",
              "      <td>1.082730</td>\n",
              "      <td>0.001186</td>\n",
              "      <td>0.238535</td>\n",
              "      <td>0.020919</td>\n",
              "      <td>-0.594603</td>\n",
              "      <td>0.181598</td>\n",
              "      <td>-0.178858</td>\n",
              "      <td>-0.284538</td>\n",
              "      <td>-0.213658</td>\n",
              "      <td>0.353956</td>\n",
              "      <td>-0.318919</td>\n",
              "      <td>-0.601735</td>\n",
              "      <td>0.166912</td>\n",
              "      <td>-0.307268</td>\n",
              "      <td>2.128376</td>\n",
              "      <td>-0.642932</td>\n",
              "      <td>-0.725018</td>\n",
              "      <td>-0.014677</td>\n",
              "      <td>-0.573773</td>\n",
              "      <td>-0.951060</td>\n",
              "      <td>-0.199929</td>\n",
              "      <td>-0.260765</td>\n",
              "      <td>0.323750</td>\n",
              "      <td>-0.321741</td>\n",
              "      <td>-0.010500</td>\n",
              "      <td>0.596390</td>\n",
              "      <td>0.215672</td>\n",
              "      <td>-0.215672</td>\n",
              "      <td>0.706238</td>\n",
              "      <td>-0.113281</td>\n",
              "      <td>0.171583</td>\n",
              "      <td>-0.167924</td>\n",
              "      <td>-0.656681</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304424</th>\n",
              "      <td>-0.267172</td>\n",
              "      <td>-0.286232</td>\n",
              "      <td>2.320913</td>\n",
              "      <td>-1.149010</td>\n",
              "      <td>-0.703043</td>\n",
              "      <td>0.535067</td>\n",
              "      <td>-0.30617</td>\n",
              "      <td>-0.306401</td>\n",
              "      <td>-0.032631</td>\n",
              "      <td>0.808192</td>\n",
              "      <td>1.048119</td>\n",
              "      <td>-0.598013</td>\n",
              "      <td>-0.063776</td>\n",
              "      <td>-0.03916</td>\n",
              "      <td>-0.650083</td>\n",
              "      <td>0.252445</td>\n",
              "      <td>-0.200231</td>\n",
              "      <td>-0.249733</td>\n",
              "      <td>-0.221801</td>\n",
              "      <td>0.304335</td>\n",
              "      <td>-0.425339</td>\n",
              "      <td>-0.175357</td>\n",
              "      <td>-0.640948</td>\n",
              "      <td>0.374891</td>\n",
              "      <td>0.613609</td>\n",
              "      <td>-0.666552</td>\n",
              "      <td>-0.536199</td>\n",
              "      <td>-0.421838</td>\n",
              "      <td>-0.617922</td>\n",
              "      <td>-0.574406</td>\n",
              "      <td>-0.580851</td>\n",
              "      <td>-0.663112</td>\n",
              "      <td>-0.431492</td>\n",
              "      <td>-0.728486</td>\n",
              "      <td>0.632089</td>\n",
              "      <td>0.435141</td>\n",
              "      <td>0.354187</td>\n",
              "      <td>-0.354187</td>\n",
              "      <td>0.097799</td>\n",
              "      <td>-0.273246</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.934722</td>\n",
              "      <td>-0.069349</td>\n",
              "      <td>-0.598951</td>\n",
              "      <td>-1.641513</td>\n",
              "      <td>0.39788</td>\n",
              "      <td>-0.600994</td>\n",
              "      <td>-0.494756</td>\n",
              "      <td>-0.514643</td>\n",
              "      <td>-0.504820</td>\n",
              "      <td>-0.679660</td>\n",
              "      <td>-0.594603</td>\n",
              "      <td>0.181598</td>\n",
              "      <td>-0.178858</td>\n",
              "      <td>-0.284538</td>\n",
              "      <td>-0.213658</td>\n",
              "      <td>0.353956</td>\n",
              "      <td>-0.318919</td>\n",
              "      <td>-0.601735</td>\n",
              "      <td>0.166912</td>\n",
              "      <td>-0.307268</td>\n",
              "      <td>-0.703193</td>\n",
              "      <td>0.414331</td>\n",
              "      <td>1.034632</td>\n",
              "      <td>-0.014677</td>\n",
              "      <td>-0.573773</td>\n",
              "      <td>0.638482</td>\n",
              "      <td>-0.199929</td>\n",
              "      <td>-0.260765</td>\n",
              "      <td>0.256959</td>\n",
              "      <td>-0.321741</td>\n",
              "      <td>0.221754</td>\n",
              "      <td>-0.296257</td>\n",
              "      <td>0.121320</td>\n",
              "      <td>-0.121320</td>\n",
              "      <td>0.706238</td>\n",
              "      <td>-0.113281</td>\n",
              "      <td>0.171583</td>\n",
              "      <td>-0.167924</td>\n",
              "      <td>-0.656681</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1021033</th>\n",
              "      <td>-1.196594</td>\n",
              "      <td>-1.177963</td>\n",
              "      <td>-0.520378</td>\n",
              "      <td>0.844100</td>\n",
              "      <td>-0.703043</td>\n",
              "      <td>-0.253680</td>\n",
              "      <td>-0.30617</td>\n",
              "      <td>-0.306401</td>\n",
              "      <td>-0.032631</td>\n",
              "      <td>-0.913700</td>\n",
              "      <td>0.806971</td>\n",
              "      <td>-0.479555</td>\n",
              "      <td>-0.063776</td>\n",
              "      <td>-0.03916</td>\n",
              "      <td>1.222590</td>\n",
              "      <td>0.836057</td>\n",
              "      <td>-0.200279</td>\n",
              "      <td>-0.285355</td>\n",
              "      <td>-0.267527</td>\n",
              "      <td>0.331057</td>\n",
              "      <td>-0.425339</td>\n",
              "      <td>-0.118044</td>\n",
              "      <td>-0.683697</td>\n",
              "      <td>-0.762479</td>\n",
              "      <td>0.613609</td>\n",
              "      <td>-0.666552</td>\n",
              "      <td>-0.536199</td>\n",
              "      <td>-0.421838</td>\n",
              "      <td>-0.617922</td>\n",
              "      <td>-0.574406</td>\n",
              "      <td>-0.580851</td>\n",
              "      <td>-0.663112</td>\n",
              "      <td>-0.431492</td>\n",
              "      <td>-0.728486</td>\n",
              "      <td>-0.027436</td>\n",
              "      <td>-0.934596</td>\n",
              "      <td>0.061134</td>\n",
              "      <td>-0.061134</td>\n",
              "      <td>-0.263099</td>\n",
              "      <td>-0.273246</td>\n",
              "      <td>...</td>\n",
              "      <td>1.209076</td>\n",
              "      <td>-0.069349</td>\n",
              "      <td>-0.436664</td>\n",
              "      <td>0.655928</td>\n",
              "      <td>0.39788</td>\n",
              "      <td>-0.600994</td>\n",
              "      <td>-0.494756</td>\n",
              "      <td>0.345072</td>\n",
              "      <td>-0.091845</td>\n",
              "      <td>0.020919</td>\n",
              "      <td>-0.594603</td>\n",
              "      <td>0.181598</td>\n",
              "      <td>-0.178858</td>\n",
              "      <td>-0.284538</td>\n",
              "      <td>-0.213658</td>\n",
              "      <td>0.353956</td>\n",
              "      <td>-0.318919</td>\n",
              "      <td>-0.601735</td>\n",
              "      <td>-0.572063</td>\n",
              "      <td>-0.307268</td>\n",
              "      <td>-0.703193</td>\n",
              "      <td>2.425709</td>\n",
              "      <td>-0.725018</td>\n",
              "      <td>-0.014677</td>\n",
              "      <td>-0.573773</td>\n",
              "      <td>-0.951060</td>\n",
              "      <td>-0.199929</td>\n",
              "      <td>-0.260765</td>\n",
              "      <td>0.335056</td>\n",
              "      <td>-0.918390</td>\n",
              "      <td>0.221754</td>\n",
              "      <td>-0.296257</td>\n",
              "      <td>0.345475</td>\n",
              "      <td>-0.345475</td>\n",
              "      <td>-1.415954</td>\n",
              "      <td>-0.113281</td>\n",
              "      <td>0.171583</td>\n",
              "      <td>-0.167924</td>\n",
              "      <td>-0.656681</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1231775</th>\n",
              "      <td>-0.152232</td>\n",
              "      <td>-0.154728</td>\n",
              "      <td>-0.550092</td>\n",
              "      <td>0.882508</td>\n",
              "      <td>-0.703043</td>\n",
              "      <td>-1.831174</td>\n",
              "      <td>-0.30617</td>\n",
              "      <td>3.277373</td>\n",
              "      <td>-0.032631</td>\n",
              "      <td>-0.980550</td>\n",
              "      <td>-1.223366</td>\n",
              "      <td>-1.264828</td>\n",
              "      <td>-0.063776</td>\n",
              "      <td>-0.03916</td>\n",
              "      <td>-0.650083</td>\n",
              "      <td>-2.082002</td>\n",
              "      <td>-0.147192</td>\n",
              "      <td>-0.383493</td>\n",
              "      <td>-0.267527</td>\n",
              "      <td>0.288603</td>\n",
              "      <td>-0.425339</td>\n",
              "      <td>-0.367648</td>\n",
              "      <td>-0.683697</td>\n",
              "      <td>-1.141602</td>\n",
              "      <td>0.613609</td>\n",
              "      <td>-0.666552</td>\n",
              "      <td>-0.536199</td>\n",
              "      <td>-0.421838</td>\n",
              "      <td>-0.617922</td>\n",
              "      <td>-0.574406</td>\n",
              "      <td>-0.580851</td>\n",
              "      <td>-0.663112</td>\n",
              "      <td>-0.431492</td>\n",
              "      <td>-0.728486</td>\n",
              "      <td>-1.826142</td>\n",
              "      <td>-0.940987</td>\n",
              "      <td>0.025082</td>\n",
              "      <td>-0.025082</td>\n",
              "      <td>-0.730143</td>\n",
              "      <td>-0.273246</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.836890</td>\n",
              "      <td>-0.069349</td>\n",
              "      <td>-0.598951</td>\n",
              "      <td>0.655928</td>\n",
              "      <td>0.39788</td>\n",
              "      <td>-0.600994</td>\n",
              "      <td>-0.494756</td>\n",
              "      <td>0.173129</td>\n",
              "      <td>-0.009250</td>\n",
              "      <td>-0.212607</td>\n",
              "      <td>0.102918</td>\n",
              "      <td>-4.644382</td>\n",
              "      <td>4.304766</td>\n",
              "      <td>7.158749</td>\n",
              "      <td>-0.213658</td>\n",
              "      <td>0.353956</td>\n",
              "      <td>-0.318919</td>\n",
              "      <td>-0.601735</td>\n",
              "      <td>0.166912</td>\n",
              "      <td>3.288204</td>\n",
              "      <td>-0.703193</td>\n",
              "      <td>-0.642932</td>\n",
              "      <td>-0.725018</td>\n",
              "      <td>-0.014677</td>\n",
              "      <td>-0.573773</td>\n",
              "      <td>-0.951060</td>\n",
              "      <td>-0.195816</td>\n",
              "      <td>-0.260765</td>\n",
              "      <td>0.288253</td>\n",
              "      <td>0.274907</td>\n",
              "      <td>0.221754</td>\n",
              "      <td>-0.296257</td>\n",
              "      <td>-0.046197</td>\n",
              "      <td>0.046197</td>\n",
              "      <td>0.706238</td>\n",
              "      <td>-0.113281</td>\n",
              "      <td>0.156927</td>\n",
              "      <td>-0.153127</td>\n",
              "      <td>1.185023</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>819345</th>\n",
              "      <td>-1.161580</td>\n",
              "      <td>-1.145781</td>\n",
              "      <td>-0.550092</td>\n",
              "      <td>-4.112084</td>\n",
              "      <td>-0.703043</td>\n",
              "      <td>-1.042427</td>\n",
              "      <td>-0.30617</td>\n",
              "      <td>-0.306401</td>\n",
              "      <td>-0.032631</td>\n",
              "      <td>0.879849</td>\n",
              "      <td>-0.603358</td>\n",
              "      <td>-1.112530</td>\n",
              "      <td>-0.063776</td>\n",
              "      <td>-0.03916</td>\n",
              "      <td>-0.650083</td>\n",
              "      <td>0.252445</td>\n",
              "      <td>-0.199153</td>\n",
              "      <td>-0.324634</td>\n",
              "      <td>-0.267527</td>\n",
              "      <td>0.346197</td>\n",
              "      <td>-0.425339</td>\n",
              "      <td>-0.339762</td>\n",
              "      <td>-0.683697</td>\n",
              "      <td>-0.383355</td>\n",
              "      <td>0.613609</td>\n",
              "      <td>-0.666552</td>\n",
              "      <td>-0.536199</td>\n",
              "      <td>-0.421838</td>\n",
              "      <td>-0.617922</td>\n",
              "      <td>-0.574406</td>\n",
              "      <td>-0.580851</td>\n",
              "      <td>-0.663112</td>\n",
              "      <td>-0.431492</td>\n",
              "      <td>-0.728486</td>\n",
              "      <td>-1.006731</td>\n",
              "      <td>-1.027881</td>\n",
              "      <td>0.142741</td>\n",
              "      <td>-0.142741</td>\n",
              "      <td>-0.135723</td>\n",
              "      <td>0.358127</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.934722</td>\n",
              "      <td>-0.069349</td>\n",
              "      <td>-0.598951</td>\n",
              "      <td>-1.641513</td>\n",
              "      <td>0.39788</td>\n",
              "      <td>-0.600994</td>\n",
              "      <td>-0.494756</td>\n",
              "      <td>-1.202416</td>\n",
              "      <td>-1.000390</td>\n",
              "      <td>-0.913187</td>\n",
              "      <td>-0.594603</td>\n",
              "      <td>0.181598</td>\n",
              "      <td>-0.178858</td>\n",
              "      <td>-0.284538</td>\n",
              "      <td>-0.213658</td>\n",
              "      <td>-0.968573</td>\n",
              "      <td>-0.318919</td>\n",
              "      <td>1.435251</td>\n",
              "      <td>0.166912</td>\n",
              "      <td>-0.307268</td>\n",
              "      <td>1.068876</td>\n",
              "      <td>-0.642932</td>\n",
              "      <td>-0.725018</td>\n",
              "      <td>-0.014677</td>\n",
              "      <td>-0.573773</td>\n",
              "      <td>-0.156289</td>\n",
              "      <td>-0.199929</td>\n",
              "      <td>-0.260765</td>\n",
              "      <td>0.325848</td>\n",
              "      <td>-0.321741</td>\n",
              "      <td>0.221754</td>\n",
              "      <td>-0.296257</td>\n",
              "      <td>-0.006896</td>\n",
              "      <td>0.006896</td>\n",
              "      <td>0.706238</td>\n",
              "      <td>-0.113281</td>\n",
              "      <td>0.166945</td>\n",
              "      <td>-0.167924</td>\n",
              "      <td>1.185023</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         acqic_bacno_nunique  ...  fraud_ind\n",
              "1093925             1.437453  ...          0\n",
              "304424             -0.267172  ...          0\n",
              "1021033            -1.196594  ...          0\n",
              "1231775            -0.152232  ...          0\n",
              "819345             -1.161580  ...          0\n",
              "\n",
              "[5 rows x 101 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec_uR3ocyOqT",
        "colab_type": "text"
      },
      "source": [
        "# 建立ANN模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLiHnkzh2h97",
        "colab_type": "text"
      },
      "source": [
        "評價函數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAS1pRSz1iXo",
        "colab_type": "code",
        "outputId": "c74873d0-5d7f-46c2-c336-5a29901bcc5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um7BPdAcyJ9R",
        "colab_type": "code",
        "outputId": "64b776c4-5a1c-45cc-b514-f034f64db818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        }
      },
      "source": [
        "from keras import backend\n",
        "from keras.layers import Dropout,Dense,Flatten\n",
        "from keras.models import Sequential\n",
        "import math\n",
        "\n",
        "model = Sequential()\n",
        "num_unit = 128\n",
        "num_layers = 3\n",
        "out_dim = 1\n",
        "input_dim = 100\n",
        "\n",
        "#輸入層\n",
        "model.add(Dense(num_unit,\n",
        "                activation = 'relu',\n",
        "                input_dim = input_dim))\n",
        "\n",
        "#隱藏層\n",
        "for i in range(num_layers-1):\n",
        "  model.add(Dense(num_unit,\n",
        "                  activation = 'relu'))\n",
        "\n",
        "#輸出層\n",
        "model.add(Dense(out_dim,\n",
        "                activation = 'sigmoid'))\n",
        "\n",
        "#編譯\n",
        "model.compile(optimizer = 'Adam',\n",
        "              loss = 'binary_crossentropy',\n",
        "              metrics=[f1_m])\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               12928     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 46,081\n",
            "Trainable params: 46,081\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffdx3Sd_z8aV",
        "colab_type": "text"
      },
      "source": [
        "# Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Kj3IBWzJ-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "es = EarlyStopping(monitor='val_loss',\n",
        "                   min_delta=0,\n",
        "                   patience=40,\n",
        "                   mode='min',\n",
        "                   restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpFowLto0Eyx",
        "colab_type": "code",
        "outputId": "3c36603f-682f-43f9-a908-5fd1156c9597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit( train[features] ,train[y_name],\n",
        "                    epochs = 150,\n",
        "                    batch_size = train.shape[0],\n",
        "                    validation_data = (val_1[features],val_1[y_name]),\n",
        "                    callbacks = [es])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 1141340 samples, validate on 190223 samples\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1141340/1141340 [==============================] - 8s 7us/step - loss: 0.9023 - f1_m: 0.0289 - val_loss: 0.6575 - val_f1_m: 0.0415\n",
            "Epoch 2/150\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.6574 - f1_m: 0.0418 - val_loss: 0.4811 - val_f1_m: 0.0148\n",
            "Epoch 3/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.4808 - f1_m: 0.0145 - val_loss: 0.3569 - val_f1_m: 0.0000e+00\n",
            "Epoch 4/150\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.3565 - f1_m: 0.0000e+00 - val_loss: 0.2695 - val_f1_m: 0.0000e+00\n",
            "Epoch 5/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.2690 - f1_m: 0.0000e+00 - val_loss: 0.2083 - val_f1_m: 0.0000e+00\n",
            "Epoch 6/150\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.2079 - f1_m: 0.0000e+00 - val_loss: 0.1664 - val_f1_m: 0.0000e+00\n",
            "Epoch 7/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.1660 - f1_m: 0.0000e+00 - val_loss: 0.1383 - val_f1_m: 0.0000e+00\n",
            "Epoch 8/150\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.1379 - f1_m: 0.0000e+00 - val_loss: 0.1201 - val_f1_m: 0.0000e+00\n",
            "Epoch 9/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.1197 - f1_m: 0.0000e+00 - val_loss: 0.1085 - val_f1_m: 0.0000e+00\n",
            "Epoch 10/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.1081 - f1_m: 0.0000e+00 - val_loss: 0.1013 - val_f1_m: 0.0000e+00\n",
            "Epoch 11/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.1009 - f1_m: 0.0000e+00 - val_loss: 0.0967 - val_f1_m: 0.0000e+00\n",
            "Epoch 12/150\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0962 - f1_m: 0.0000e+00 - val_loss: 0.0934 - val_f1_m: 0.0000e+00\n",
            "Epoch 13/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0929 - f1_m: 0.0000e+00 - val_loss: 0.0906 - val_f1_m: 0.0000e+00\n",
            "Epoch 14/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0901 - f1_m: 0.0000e+00 - val_loss: 0.0879 - val_f1_m: 0.0000e+00\n",
            "Epoch 15/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0874 - f1_m: 0.0000e+00 - val_loss: 0.0849 - val_f1_m: 0.0000e+00\n",
            "Epoch 16/150\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0844 - f1_m: 0.0000e+00 - val_loss: 0.0816 - val_f1_m: 0.0000e+00\n",
            "Epoch 17/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0812 - f1_m: 0.0000e+00 - val_loss: 0.0781 - val_f1_m: 0.0000e+00\n",
            "Epoch 18/150\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0776 - f1_m: 0.0000e+00 - val_loss: 0.0744 - val_f1_m: 0.0000e+00\n",
            "Epoch 19/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0739 - f1_m: 0.0000e+00 - val_loss: 0.0705 - val_f1_m: 0.0000e+00\n",
            "Epoch 20/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0701 - f1_m: 0.0000e+00 - val_loss: 0.0668 - val_f1_m: 0.0000e+00\n",
            "Epoch 21/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0663 - f1_m: 0.0000e+00 - val_loss: 0.0632 - val_f1_m: 0.0000e+00\n",
            "Epoch 22/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0628 - f1_m: 0.0000e+00 - val_loss: 0.0599 - val_f1_m: 0.0000e+00\n",
            "Epoch 23/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0595 - f1_m: 0.0000e+00 - val_loss: 0.0570 - val_f1_m: 0.0000e+00\n",
            "Epoch 24/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0565 - f1_m: 0.0000e+00 - val_loss: 0.0545 - val_f1_m: 0.0000e+00\n",
            "Epoch 25/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0541 - f1_m: 0.0000e+00 - val_loss: 0.0525 - val_f1_m: 0.0000e+00\n",
            "Epoch 26/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0520 - f1_m: 0.0000e+00 - val_loss: 0.0508 - val_f1_m: 0.0000e+00\n",
            "Epoch 27/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0504 - f1_m: 0.0000e+00 - val_loss: 0.0496 - val_f1_m: 0.0000e+00\n",
            "Epoch 28/150\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0492 - f1_m: 0.0000e+00 - val_loss: 0.0487 - val_f1_m: 0.0000e+00\n",
            "Epoch 29/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0483 - f1_m: 0.0000e+00 - val_loss: 0.0481 - val_f1_m: 0.0000e+00\n",
            "Epoch 30/150\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0478 - f1_m: 0.0000e+00 - val_loss: 0.0477 - val_f1_m: 0.0000e+00\n",
            "Epoch 31/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0474 - f1_m: 0.0000e+00 - val_loss: 0.0474 - val_f1_m: 0.0000e+00\n",
            "Epoch 32/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0471 - f1_m: 0.0000e+00 - val_loss: 0.0472 - val_f1_m: 0.0024\n",
            "Epoch 33/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0469 - f1_m: 6.5441e-04 - val_loss: 0.0470 - val_f1_m: 0.0102\n",
            "Epoch 34/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0467 - f1_m: 0.0078 - val_loss: 0.0467 - val_f1_m: 0.0171\n",
            "Epoch 35/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0464 - f1_m: 0.0178 - val_loss: 0.0465 - val_f1_m: 0.0353\n",
            "Epoch 36/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0462 - f1_m: 0.0280 - val_loss: 0.0461 - val_f1_m: 0.0557\n",
            "Epoch 37/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0459 - f1_m: 0.0477 - val_loss: 0.0458 - val_f1_m: 0.0753\n",
            "Epoch 38/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0455 - f1_m: 0.0738 - val_loss: 0.0453 - val_f1_m: 0.1012\n",
            "Epoch 39/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0451 - f1_m: 0.1006 - val_loss: 0.0448 - val_f1_m: 0.1229\n",
            "Epoch 40/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0446 - f1_m: 0.1256 - val_loss: 0.0444 - val_f1_m: 0.1430\n",
            "Epoch 41/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0441 - f1_m: 0.1493 - val_loss: 0.0439 - val_f1_m: 0.1593\n",
            "Epoch 42/150\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0436 - f1_m: 0.1651 - val_loss: 0.0434 - val_f1_m: 0.1760\n",
            "Epoch 43/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0432 - f1_m: 0.1781 - val_loss: 0.0430 - val_f1_m: 0.1800\n",
            "Epoch 44/150\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0428 - f1_m: 0.1845 - val_loss: 0.0427 - val_f1_m: 0.1879\n",
            "Epoch 45/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0424 - f1_m: 0.1885 - val_loss: 0.0424 - val_f1_m: 0.1911\n",
            "Epoch 46/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0421 - f1_m: 0.1914 - val_loss: 0.0421 - val_f1_m: 0.1912\n",
            "Epoch 47/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0419 - f1_m: 0.1932 - val_loss: 0.0420 - val_f1_m: 0.1916\n",
            "Epoch 48/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0418 - f1_m: 0.1952 - val_loss: 0.0418 - val_f1_m: 0.1915\n",
            "Epoch 49/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0416 - f1_m: 0.1971 - val_loss: 0.0417 - val_f1_m: 0.1904\n",
            "Epoch 50/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0415 - f1_m: 0.1983 - val_loss: 0.0416 - val_f1_m: 0.1924\n",
            "Epoch 51/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0414 - f1_m: 0.1993 - val_loss: 0.0415 - val_f1_m: 0.1991\n",
            "Epoch 52/150\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0413 - f1_m: 0.2020 - val_loss: 0.0414 - val_f1_m: 0.2011\n",
            "Epoch 53/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0412 - f1_m: 0.2072 - val_loss: 0.0413 - val_f1_m: 0.2040\n",
            "Epoch 54/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0411 - f1_m: 0.2113 - val_loss: 0.0411 - val_f1_m: 0.2128\n",
            "Epoch 55/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0409 - f1_m: 0.2175 - val_loss: 0.0410 - val_f1_m: 0.2204\n",
            "Epoch 56/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0407 - f1_m: 0.2242 - val_loss: 0.0408 - val_f1_m: 0.2304\n",
            "Epoch 57/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0406 - f1_m: 0.2349 - val_loss: 0.0406 - val_f1_m: 0.2369\n",
            "Epoch 58/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0404 - f1_m: 0.2443 - val_loss: 0.0404 - val_f1_m: 0.2481\n",
            "Epoch 59/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0402 - f1_m: 0.2542 - val_loss: 0.0403 - val_f1_m: 0.2607\n",
            "Epoch 60/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0400 - f1_m: 0.2652 - val_loss: 0.0401 - val_f1_m: 0.2712\n",
            "Epoch 61/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0399 - f1_m: 0.2762 - val_loss: 0.0400 - val_f1_m: 0.2810\n",
            "Epoch 62/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0397 - f1_m: 0.2837 - val_loss: 0.0399 - val_f1_m: 0.2915\n",
            "Epoch 63/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0396 - f1_m: 0.2914 - val_loss: 0.0397 - val_f1_m: 0.3005\n",
            "Epoch 64/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0395 - f1_m: 0.3028 - val_loss: 0.0396 - val_f1_m: 0.3055\n",
            "Epoch 65/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0394 - f1_m: 0.3107 - val_loss: 0.0396 - val_f1_m: 0.3120\n",
            "Epoch 66/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0393 - f1_m: 0.3181 - val_loss: 0.0395 - val_f1_m: 0.3209\n",
            "Epoch 67/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0392 - f1_m: 0.3236 - val_loss: 0.0394 - val_f1_m: 0.3256\n",
            "Epoch 68/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0391 - f1_m: 0.3285 - val_loss: 0.0393 - val_f1_m: 0.3296\n",
            "Epoch 69/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0391 - f1_m: 0.3300 - val_loss: 0.0392 - val_f1_m: 0.3292\n",
            "Epoch 70/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0390 - f1_m: 0.3321 - val_loss: 0.0391 - val_f1_m: 0.3304\n",
            "Epoch 71/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0389 - f1_m: 0.3319 - val_loss: 0.0391 - val_f1_m: 0.3306\n",
            "Epoch 72/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0388 - f1_m: 0.3304 - val_loss: 0.0390 - val_f1_m: 0.3302\n",
            "Epoch 73/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0387 - f1_m: 0.3285 - val_loss: 0.0389 - val_f1_m: 0.3291\n",
            "Epoch 74/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0386 - f1_m: 0.3266 - val_loss: 0.0388 - val_f1_m: 0.3273\n",
            "Epoch 75/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0385 - f1_m: 0.3243 - val_loss: 0.0387 - val_f1_m: 0.3245\n",
            "Epoch 76/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0384 - f1_m: 0.3209 - val_loss: 0.0387 - val_f1_m: 0.3190\n",
            "Epoch 77/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0384 - f1_m: 0.3190 - val_loss: 0.0386 - val_f1_m: 0.3179\n",
            "Epoch 78/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0383 - f1_m: 0.3170 - val_loss: 0.0385 - val_f1_m: 0.3161\n",
            "Epoch 79/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0382 - f1_m: 0.3144 - val_loss: 0.0385 - val_f1_m: 0.3168\n",
            "Epoch 80/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0382 - f1_m: 0.3144 - val_loss: 0.0384 - val_f1_m: 0.3167\n",
            "Epoch 81/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0381 - f1_m: 0.3145 - val_loss: 0.0384 - val_f1_m: 0.3187\n",
            "Epoch 82/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0381 - f1_m: 0.3157 - val_loss: 0.0383 - val_f1_m: 0.3226\n",
            "Epoch 83/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0380 - f1_m: 0.3178 - val_loss: 0.0383 - val_f1_m: 0.3259\n",
            "Epoch 84/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0379 - f1_m: 0.3196 - val_loss: 0.0382 - val_f1_m: 0.3259\n",
            "Epoch 85/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0379 - f1_m: 0.3218 - val_loss: 0.0381 - val_f1_m: 0.3278\n",
            "Epoch 86/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0378 - f1_m: 0.3255 - val_loss: 0.0381 - val_f1_m: 0.3298\n",
            "Epoch 87/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0378 - f1_m: 0.3279 - val_loss: 0.0380 - val_f1_m: 0.3347\n",
            "Epoch 88/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0377 - f1_m: 0.3340 - val_loss: 0.0380 - val_f1_m: 0.3390\n",
            "Epoch 89/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0377 - f1_m: 0.3373 - val_loss: 0.0379 - val_f1_m: 0.3454\n",
            "Epoch 90/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0376 - f1_m: 0.3412 - val_loss: 0.0379 - val_f1_m: 0.3496\n",
            "Epoch 91/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0376 - f1_m: 0.3449 - val_loss: 0.0378 - val_f1_m: 0.3513\n",
            "Epoch 92/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0375 - f1_m: 0.3477 - val_loss: 0.0378 - val_f1_m: 0.3538\n",
            "Epoch 93/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0375 - f1_m: 0.3500 - val_loss: 0.0377 - val_f1_m: 0.3576\n",
            "Epoch 94/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0374 - f1_m: 0.3520 - val_loss: 0.0377 - val_f1_m: 0.3585\n",
            "Epoch 95/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0374 - f1_m: 0.3531 - val_loss: 0.0376 - val_f1_m: 0.3600\n",
            "Epoch 96/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0373 - f1_m: 0.3535 - val_loss: 0.0376 - val_f1_m: 0.3599\n",
            "Epoch 97/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0373 - f1_m: 0.3546 - val_loss: 0.0376 - val_f1_m: 0.3577\n",
            "Epoch 98/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0372 - f1_m: 0.3541 - val_loss: 0.0375 - val_f1_m: 0.3577\n",
            "Epoch 99/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0372 - f1_m: 0.3540 - val_loss: 0.0375 - val_f1_m: 0.3597\n",
            "Epoch 100/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0371 - f1_m: 0.3548 - val_loss: 0.0374 - val_f1_m: 0.3588\n",
            "Epoch 101/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0371 - f1_m: 0.3549 - val_loss: 0.0374 - val_f1_m: 0.3598\n",
            "Epoch 102/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0371 - f1_m: 0.3551 - val_loss: 0.0374 - val_f1_m: 0.3601\n",
            "Epoch 103/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0370 - f1_m: 0.3550 - val_loss: 0.0373 - val_f1_m: 0.3600\n",
            "Epoch 104/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0370 - f1_m: 0.3557 - val_loss: 0.0373 - val_f1_m: 0.3610\n",
            "Epoch 105/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0369 - f1_m: 0.3564 - val_loss: 0.0373 - val_f1_m: 0.3602\n",
            "Epoch 106/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0369 - f1_m: 0.3570 - val_loss: 0.0372 - val_f1_m: 0.3624\n",
            "Epoch 107/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0369 - f1_m: 0.3585 - val_loss: 0.0372 - val_f1_m: 0.3645\n",
            "Epoch 108/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0368 - f1_m: 0.3602 - val_loss: 0.0372 - val_f1_m: 0.3671\n",
            "Epoch 109/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0368 - f1_m: 0.3642 - val_loss: 0.0371 - val_f1_m: 0.3697\n",
            "Epoch 110/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0367 - f1_m: 0.3672 - val_loss: 0.0371 - val_f1_m: 0.3702\n",
            "Epoch 111/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0367 - f1_m: 0.3690 - val_loss: 0.0370 - val_f1_m: 0.3705\n",
            "Epoch 112/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0367 - f1_m: 0.3720 - val_loss: 0.0370 - val_f1_m: 0.3741\n",
            "Epoch 113/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0366 - f1_m: 0.3744 - val_loss: 0.0370 - val_f1_m: 0.3764\n",
            "Epoch 114/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0366 - f1_m: 0.3766 - val_loss: 0.0369 - val_f1_m: 0.3786\n",
            "Epoch 115/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0366 - f1_m: 0.3782 - val_loss: 0.0369 - val_f1_m: 0.3785\n",
            "Epoch 116/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0365 - f1_m: 0.3800 - val_loss: 0.0369 - val_f1_m: 0.3810\n",
            "Epoch 117/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0365 - f1_m: 0.3830 - val_loss: 0.0368 - val_f1_m: 0.3834\n",
            "Epoch 118/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0365 - f1_m: 0.3850 - val_loss: 0.0368 - val_f1_m: 0.3827\n",
            "Epoch 119/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0364 - f1_m: 0.3856 - val_loss: 0.0368 - val_f1_m: 0.3826\n",
            "Epoch 120/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0364 - f1_m: 0.3865 - val_loss: 0.0368 - val_f1_m: 0.3838\n",
            "Epoch 121/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0364 - f1_m: 0.3869 - val_loss: 0.0367 - val_f1_m: 0.3852\n",
            "Epoch 122/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0363 - f1_m: 0.3882 - val_loss: 0.0367 - val_f1_m: 0.3844\n",
            "Epoch 123/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0363 - f1_m: 0.3883 - val_loss: 0.0367 - val_f1_m: 0.3860\n",
            "Epoch 124/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0363 - f1_m: 0.3890 - val_loss: 0.0366 - val_f1_m: 0.3846\n",
            "Epoch 125/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0362 - f1_m: 0.3891 - val_loss: 0.0366 - val_f1_m: 0.3855\n",
            "Epoch 126/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0362 - f1_m: 0.3897 - val_loss: 0.0366 - val_f1_m: 0.3874\n",
            "Epoch 127/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0362 - f1_m: 0.3900 - val_loss: 0.0365 - val_f1_m: 0.3877\n",
            "Epoch 128/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0361 - f1_m: 0.3907 - val_loss: 0.0365 - val_f1_m: 0.3899\n",
            "Epoch 129/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0361 - f1_m: 0.3918 - val_loss: 0.0365 - val_f1_m: 0.3911\n",
            "Epoch 130/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0361 - f1_m: 0.3929 - val_loss: 0.0364 - val_f1_m: 0.3925\n",
            "Epoch 131/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0360 - f1_m: 0.3950 - val_loss: 0.0364 - val_f1_m: 0.3956\n",
            "Epoch 132/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0360 - f1_m: 0.3962 - val_loss: 0.0364 - val_f1_m: 0.3960\n",
            "Epoch 133/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0360 - f1_m: 0.3971 - val_loss: 0.0364 - val_f1_m: 0.3975\n",
            "Epoch 134/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0359 - f1_m: 0.3988 - val_loss: 0.0363 - val_f1_m: 0.3981\n",
            "Epoch 135/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0359 - f1_m: 0.4003 - val_loss: 0.0363 - val_f1_m: 0.3986\n",
            "Epoch 136/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0359 - f1_m: 0.4012 - val_loss: 0.0363 - val_f1_m: 0.3991\n",
            "Epoch 137/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0358 - f1_m: 0.4010 - val_loss: 0.0362 - val_f1_m: 0.3995\n",
            "Epoch 138/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0358 - f1_m: 0.4020 - val_loss: 0.0362 - val_f1_m: 0.3997\n",
            "Epoch 139/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0358 - f1_m: 0.4018 - val_loss: 0.0362 - val_f1_m: 0.3991\n",
            "Epoch 140/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0358 - f1_m: 0.4021 - val_loss: 0.0362 - val_f1_m: 0.3988\n",
            "Epoch 141/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0357 - f1_m: 0.4026 - val_loss: 0.0361 - val_f1_m: 0.3981\n",
            "Epoch 142/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0357 - f1_m: 0.4023 - val_loss: 0.0361 - val_f1_m: 0.3988\n",
            "Epoch 143/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0357 - f1_m: 0.4037 - val_loss: 0.0361 - val_f1_m: 0.3991\n",
            "Epoch 144/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0356 - f1_m: 0.4040 - val_loss: 0.0361 - val_f1_m: 0.4003\n",
            "Epoch 145/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0356 - f1_m: 0.4056 - val_loss: 0.0360 - val_f1_m: 0.4003\n",
            "Epoch 146/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0356 - f1_m: 0.4080 - val_loss: 0.0360 - val_f1_m: 0.4018\n",
            "Epoch 147/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0355 - f1_m: 0.4089 - val_loss: 0.0360 - val_f1_m: 0.4040\n",
            "Epoch 148/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0355 - f1_m: 0.4096 - val_loss: 0.0359 - val_f1_m: 0.4053\n",
            "Epoch 149/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0355 - f1_m: 0.4113 - val_loss: 0.0359 - val_f1_m: 0.4061\n",
            "Epoch 150/150\n",
            "1141340/1141340 [==============================] - 3s 3us/step - loss: 0.0355 - f1_m: 0.4117 - val_loss: 0.0359 - val_f1_m: 0.4075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9gB_dwZ3jkI",
        "colab_type": "text"
      },
      "source": [
        "# 訓練過程評估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S970Vc8k0cp3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "ecc90898-1f60-4fe0-b28e-e32519caca22"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1,1)\n",
        "ax.plot(history.history['loss'][n:], color='b', label=\"Training loss\")\n",
        "ax.plot(history.history['val_loss'][n:], color='r', label=\"validation loss\")\n",
        "legend = ax.legend(loc='best', shadow=True)\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(1,1)\n",
        "ax.plot(history.history['f1_m'][n:], color='b', label=\"Training f1\")\n",
        "ax.plot(history.history['val_f1_m'][n:], color='r', label=\"validation f1\")\n",
        "legend = ax.legend(loc='best', shadow=True)\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD8CAYAAABZ/vJZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5P8BEJJqX9rjWzUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCxMeMPw4Zkq",
        "colab_type": "text"
      },
      "source": [
        "# val_2's F1 report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kASFgsao4aF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "f23d0bc8-aa73-4c40-d797-5d8f0d4fc380"
      },
      "source": [
        "from sklearn.metrics import classification_report \n",
        "\n",
        "val_2_pred_01 = pd.Series(model.predict(val_2[features]).ravel()).apply(round)\n",
        "\n",
        "print(classification_report(val_2[y_name],val_2_pred_01,target_names=['0','1']))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99    187682\n",
            "           1       0.69      0.28      0.40      2542\n",
            "\n",
            "    accuracy                           0.99    190224\n",
            "   macro avg       0.84      0.64      0.70    190224\n",
            "weighted avg       0.99      0.99      0.99    190224\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6PFh2lCBwBG",
        "colab_type": "text"
      },
      "source": [
        "# 產生submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNZ2Eing7kUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "5ecdb969-9df2-421b-95bd-04249348eb74"
      },
      "source": [
        "submission = pd.DataFrame({\"txkey\":test_txkey,\n",
        "                           \"fraud_ind\":pd.Series(model.predict(test[features]).ravel()).apply(round).values})\n",
        "\n",
        "# value_counts\n",
        "print(submission[\"fraud_ind\"].value_counts())\n",
        "submission.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    418358\n",
            "1      3307\n",
            "Name: fraud_ind, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>txkey</th>\n",
              "      <th>fraud_ind</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1521787</th>\n",
              "      <td>592489</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521788</th>\n",
              "      <td>592452</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521789</th>\n",
              "      <td>590212</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521790</th>\n",
              "      <td>590209</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521791</th>\n",
              "      <td>592488</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          txkey  fraud_ind\n",
              "1521787  592489          0\n",
              "1521788  592452          0\n",
              "1521789  590212          0\n",
              "1521790  590209          0\n",
              "1521791  592488          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1duNRbRlDgAG",
        "colab_type": "text"
      },
      "source": [
        "# 這裡有個想法如果test上盜刷的比例 跟train上面盜刷的比例 愈相近愈好"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNz1_v5aB33k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "c4ef1b16-f989-4503-f581-93745f922b8f"
      },
      "source": [
        "train_p = round((train['fraud_ind'].value_counts()[1] / #盜刷樣本數\n",
        "                len(train['fraud_ind']))*100 ,5) #總數\n",
        "\n",
        "val_1_p = round((val_1['fraud_ind'].value_counts()[1] / #盜刷樣本數\n",
        "                len(val_1['fraud_ind']))*100 ,5) #總數\n",
        "\n",
        "val_2_p = round((val_2['fraud_ind'].value_counts()[1] / #盜刷樣本數\n",
        "                len(val_2['fraud_ind']))*100 ,5) #總數\n",
        "\n",
        "test_p = round((submission['fraud_ind'].value_counts()[1] / #盜刷樣本數\n",
        "                len(submission['fraud_ind']))*100 ,5) #總數\n",
        "pd.DataFrame({'train_p':train_p,\n",
        "              'val_1_p':val_1_p,\n",
        "              'val_2_p':val_2_p,\n",
        "              'test_p':test_p},\n",
        "               index=['盜刷比例%'])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_p</th>\n",
              "      <th>val_1_p</th>\n",
              "      <th>val_2_p</th>\n",
              "      <th>test_p</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>盜刷比例%</th>\n",
              "      <td>1.33816</td>\n",
              "      <td>1.33527</td>\n",
              "      <td>1.33632</td>\n",
              "      <td>0.78427</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       train_p  val_1_p  val_2_p   test_p\n",
              "盜刷比例%  1.33816  1.33527  1.33632  0.78427"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNMCpHJ8Dkqc",
        "colab_type": "text"
      },
      "source": [
        "# 保存預測結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSoM9qNeDhyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv(\"./submission_33.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg3PQVLODmUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ac956636-39cb-4829-bf66-b1a8bdfd1b1d"
      },
      "source": [
        "print(submission[\"fraud_ind\"].value_counts())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    418358\n",
            "1      3307\n",
            "Name: fraud_ind, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix3KqTveDr_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}