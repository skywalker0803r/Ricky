{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "玉山AI比賽_modeling_ANN版本.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skywalker0803r/Ricky/blob/master/%E7%8E%89%E5%B1%B1AI%E6%AF%94%E8%B3%BD_modeling_ANN%E7%89%88%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr44CknfvyTj",
        "colab_type": "code",
        "outputId": "fe49de71-9ea4-4d90-be2d-553982a4e226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import warnings \n",
        "warnings.simplefilter('ignore')\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbAV7ns3v8LW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "中文map = {'bacno':'歸戶帳號','txkey':'交易序號','locdt':'授權日期','loctm':'授權時間','cano':'交易卡號',\n",
        "         'contp':'交易類別','etymd':'交易型態','mchno':'特店代號','acqic':'收單行代碼','mcc':'MCC_CODE',\n",
        "         'conam':'交易金額-台幣(經過轉換)','ecfg':'網路交易註記','insfg':'分期交易註記','iterm':'分期期數',\n",
        "         'stocn':'消費地國別','scity':'消費城市','stscd':'狀態碼','ovrlt':'超額註記碼','flbmk':'Fallback註記',\n",
        "         'hcefg':'支付型態','csmcu':'消費地幣別','flg_3dsmk':'3DS交易註記','fraud_ind':'盜刷註記'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJs7NefWwDKv",
        "colab_type": "text"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Baq7Qt00wBmh",
        "colab_type": "code",
        "outputId": "268dafa7-f97b-4c78-8634-b70402ab8a55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"/content/drive/My Drive/玉山人工智慧比賽數據/train_特徵工程完.csv\",index_col=0)\n",
        "print(train.shape)\n",
        "train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1521787, 122)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stscd</th>\n",
              "      <th>ecfg</th>\n",
              "      <th>stocn</th>\n",
              "      <th>acqic</th>\n",
              "      <th>etymd</th>\n",
              "      <th>loctm</th>\n",
              "      <th>scity</th>\n",
              "      <th>hcefg</th>\n",
              "      <th>contp</th>\n",
              "      <th>conam</th>\n",
              "      <th>insfg</th>\n",
              "      <th>ovrlt</th>\n",
              "      <th>flg_3dsmk</th>\n",
              "      <th>iterm</th>\n",
              "      <th>csmcu</th>\n",
              "      <th>mchno</th>\n",
              "      <th>cano</th>\n",
              "      <th>mcc</th>\n",
              "      <th>flbmk</th>\n",
              "      <th>bacno</th>\n",
              "      <th>mchno_stscd_2_norm_count</th>\n",
              "      <th>mchno_stscd_0_norm_count</th>\n",
              "      <th>acqic_stscd_2_norm_count</th>\n",
              "      <th>acqic_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_2_norm_count</th>\n",
              "      <th>cano_stscd_0_norm_count</th>\n",
              "      <th>acqic_scity_nunique</th>\n",
              "      <th>acqic_csmcu_nunique</th>\n",
              "      <th>bacno_stscd_2_norm_count</th>\n",
              "      <th>bacno_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_nunique</th>\n",
              "      <th>acqic_etymd_8_norm_count</th>\n",
              "      <th>mchno_etymd_8_norm_count</th>\n",
              "      <th>acqic_stocn_nunique</th>\n",
              "      <th>acqic_hcefg_nunique</th>\n",
              "      <th>bacno_stscd_nunique</th>\n",
              "      <th>acqic_ovrlt_0_norm_count</th>\n",
              "      <th>acqic_ovrlt_1_norm_count</th>\n",
              "      <th>acqic_etymd_5_norm_count</th>\n",
              "      <th>acqic_bacno_nunique</th>\n",
              "      <th>...</th>\n",
              "      <th>acqic_iterm_3_norm_count</th>\n",
              "      <th>acqic_hcefg_7_norm_count</th>\n",
              "      <th>mchno_ovrlt_nunique</th>\n",
              "      <th>mchno_etymd_nunique</th>\n",
              "      <th>acqic_hcefg_8_norm_count</th>\n",
              "      <th>bacno_scity_nunique</th>\n",
              "      <th>mchno_stocn_nunique</th>\n",
              "      <th>acqic_hcefg_1_norm_count</th>\n",
              "      <th>mchno_etymd_7_norm_count</th>\n",
              "      <th>acqic_hcefg_5_norm_count</th>\n",
              "      <th>bacno_etymd_4_norm_count</th>\n",
              "      <th>cano_mchno_nunique</th>\n",
              "      <th>acqic_etymd_10_norm_count</th>\n",
              "      <th>acqic_etymd_7_norm_count</th>\n",
              "      <th>mchno_etymd_0_norm_count</th>\n",
              "      <th>cano_etymd_0_norm_count</th>\n",
              "      <th>acqic_etymd_0_norm_count</th>\n",
              "      <th>cano_etymd_2_norm_count</th>\n",
              "      <th>mchno_iterm_nunique</th>\n",
              "      <th>mchno_hcefg_5_norm_count</th>\n",
              "      <th>cano_iterm_nunique</th>\n",
              "      <th>acqic_hcefg_2_norm_count</th>\n",
              "      <th>mchno_hcefg_1_norm_count</th>\n",
              "      <th>mchno_contp_6_norm_count</th>\n",
              "      <th>cano_scity_nunique</th>\n",
              "      <th>cano_contp_nunique</th>\n",
              "      <th>mchno_contp_2_norm_count</th>\n",
              "      <th>cano_contp_2_norm_count</th>\n",
              "      <th>mchno_contp_5_norm_count</th>\n",
              "      <th>cano_hcefg_nunique</th>\n",
              "      <th>bacno_etymd_0_norm_count</th>\n",
              "      <th>cano_mcc_nunique</th>\n",
              "      <th>acqic_hcefg_0_norm_count</th>\n",
              "      <th>bacno_acqic_nunique</th>\n",
              "      <th>mchno_iterm_0_norm_count</th>\n",
              "      <th>mchno_hcefg_0_norm_count</th>\n",
              "      <th>cano_hcefg_5_norm_count</th>\n",
              "      <th>cano_etymd_6_norm_count</th>\n",
              "      <th>txkey</th>\n",
              "      <th>fraud_ind</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>61954</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59034</td>\n",
              "      <td>37846</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>112785</td>\n",
              "      <td>0.001044</td>\n",
              "      <td>0.998956</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>33</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.212500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.362500</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989928</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.212500</td>\n",
              "      <td>21</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>18</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>516056</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>38216</td>\n",
              "      <td>5795</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>13693</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>45476</td>\n",
              "      <td>451</td>\n",
              "      <td>0</td>\n",
              "      <td>133951</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.997536</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.994768</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.991720</td>\n",
              "      <td>0.008280</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>19252</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0.003276</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.042083</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006682</td>\n",
              "      <td>0.391304</td>\n",
              "      <td>11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024906</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.008377</td>\n",
              "      <td>0.434783</td>\n",
              "      <td>1</td>\n",
              "      <td>0.034542</td>\n",
              "      <td>2</td>\n",
              "      <td>0.013267</td>\n",
              "      <td>0.039878</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.968283</td>\n",
              "      <td>2</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>8</td>\n",
              "      <td>0.931187</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.911000</td>\n",
              "      <td>0.652174</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4376</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>54640</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59034</td>\n",
              "      <td>187354</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>15350</td>\n",
              "      <td>0.001044</td>\n",
              "      <td>0.998956</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989928</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>7</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.0</td>\n",
              "      <td>483434</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6697</td>\n",
              "      <td>5</td>\n",
              "      <td>62128</td>\n",
              "      <td>3267</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>40413</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>62</td>\n",
              "      <td>50185</td>\n",
              "      <td>29812</td>\n",
              "      <td>247</td>\n",
              "      <td>0</td>\n",
              "      <td>156492</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.999643</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>101</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.142619</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.988238</td>\n",
              "      <td>0.011762</td>\n",
              "      <td>0.312315</td>\n",
              "      <td>74622</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002037</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0.003765</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.024410</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.965999</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.245614</td>\n",
              "      <td>5</td>\n",
              "      <td>0.984111</td>\n",
              "      <td>2</td>\n",
              "      <td>0.001563</td>\n",
              "      <td>0.012910</td>\n",
              "      <td>0.010924</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035088</td>\n",
              "      <td>0.989076</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>8</td>\n",
              "      <td>0.861966</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1407164</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>5959</td>\n",
              "      <td>4</td>\n",
              "      <td>65231</td>\n",
              "      <td>5795</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>25962</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>62</td>\n",
              "      <td>93290</td>\n",
              "      <td>80881</td>\n",
              "      <td>263</td>\n",
              "      <td>0</td>\n",
              "      <td>105534</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.999894</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.077908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.990016</td>\n",
              "      <td>0.009984</td>\n",
              "      <td>0.206081</td>\n",
              "      <td>41913</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009496</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.982903</td>\n",
              "      <td>0.089744</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365079</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079365</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000707</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.079365</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.984127</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1051004</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 122 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   stscd  ecfg  stocn  ...  cano_etymd_6_norm_count    txkey  fraud_ind\n",
              "0      0     0    102  ...                      0.0   516056          0\n",
              "1      0     0    102  ...                      0.0     4376          0\n",
              "2      0     0    102  ...                      0.0   483434          0\n",
              "3      0     0    102  ...                      0.0  1407164          0\n",
              "4      0     0    102  ...                      0.0  1051004          0\n",
              "\n",
              "[5 rows x 122 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmQm34KPwGu_",
        "colab_type": "text"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTm0HGGuwGCm",
        "colab_type": "code",
        "outputId": "8203d8de-aa8f-4d3f-ef3c-9767d9f95f08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "test = pd.read_csv(\"/content/drive/My Drive/玉山人工智慧比賽數據/test_特徵工程完.csv\",index_col=0)\n",
        "test_txkey = test[\"txkey\"]\n",
        "print(test.shape)\n",
        "test.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(421665, 121)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stscd</th>\n",
              "      <th>ecfg</th>\n",
              "      <th>stocn</th>\n",
              "      <th>acqic</th>\n",
              "      <th>etymd</th>\n",
              "      <th>loctm</th>\n",
              "      <th>scity</th>\n",
              "      <th>hcefg</th>\n",
              "      <th>contp</th>\n",
              "      <th>conam</th>\n",
              "      <th>insfg</th>\n",
              "      <th>ovrlt</th>\n",
              "      <th>flg_3dsmk</th>\n",
              "      <th>iterm</th>\n",
              "      <th>csmcu</th>\n",
              "      <th>mchno</th>\n",
              "      <th>cano</th>\n",
              "      <th>mcc</th>\n",
              "      <th>flbmk</th>\n",
              "      <th>bacno</th>\n",
              "      <th>mchno_stscd_2_norm_count</th>\n",
              "      <th>mchno_stscd_0_norm_count</th>\n",
              "      <th>acqic_stscd_2_norm_count</th>\n",
              "      <th>acqic_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_2_norm_count</th>\n",
              "      <th>cano_stscd_0_norm_count</th>\n",
              "      <th>acqic_scity_nunique</th>\n",
              "      <th>acqic_csmcu_nunique</th>\n",
              "      <th>bacno_stscd_2_norm_count</th>\n",
              "      <th>bacno_stscd_0_norm_count</th>\n",
              "      <th>cano_stscd_nunique</th>\n",
              "      <th>acqic_etymd_8_norm_count</th>\n",
              "      <th>mchno_etymd_8_norm_count</th>\n",
              "      <th>acqic_stocn_nunique</th>\n",
              "      <th>acqic_hcefg_nunique</th>\n",
              "      <th>bacno_stscd_nunique</th>\n",
              "      <th>acqic_ovrlt_0_norm_count</th>\n",
              "      <th>acqic_ovrlt_1_norm_count</th>\n",
              "      <th>acqic_etymd_5_norm_count</th>\n",
              "      <th>acqic_bacno_nunique</th>\n",
              "      <th>...</th>\n",
              "      <th>cano_etymd_4_norm_count</th>\n",
              "      <th>acqic_iterm_3_norm_count</th>\n",
              "      <th>acqic_hcefg_7_norm_count</th>\n",
              "      <th>mchno_ovrlt_nunique</th>\n",
              "      <th>mchno_etymd_nunique</th>\n",
              "      <th>acqic_hcefg_8_norm_count</th>\n",
              "      <th>bacno_scity_nunique</th>\n",
              "      <th>mchno_stocn_nunique</th>\n",
              "      <th>acqic_hcefg_1_norm_count</th>\n",
              "      <th>mchno_etymd_7_norm_count</th>\n",
              "      <th>acqic_hcefg_5_norm_count</th>\n",
              "      <th>bacno_etymd_4_norm_count</th>\n",
              "      <th>cano_mchno_nunique</th>\n",
              "      <th>acqic_etymd_10_norm_count</th>\n",
              "      <th>acqic_etymd_7_norm_count</th>\n",
              "      <th>mchno_etymd_0_norm_count</th>\n",
              "      <th>cano_etymd_0_norm_count</th>\n",
              "      <th>acqic_etymd_0_norm_count</th>\n",
              "      <th>cano_etymd_2_norm_count</th>\n",
              "      <th>mchno_iterm_nunique</th>\n",
              "      <th>mchno_hcefg_5_norm_count</th>\n",
              "      <th>cano_iterm_nunique</th>\n",
              "      <th>acqic_hcefg_2_norm_count</th>\n",
              "      <th>mchno_hcefg_1_norm_count</th>\n",
              "      <th>mchno_contp_6_norm_count</th>\n",
              "      <th>cano_scity_nunique</th>\n",
              "      <th>cano_contp_nunique</th>\n",
              "      <th>mchno_contp_2_norm_count</th>\n",
              "      <th>cano_contp_2_norm_count</th>\n",
              "      <th>mchno_contp_5_norm_count</th>\n",
              "      <th>cano_hcefg_nunique</th>\n",
              "      <th>bacno_etymd_0_norm_count</th>\n",
              "      <th>cano_mcc_nunique</th>\n",
              "      <th>acqic_hcefg_0_norm_count</th>\n",
              "      <th>bacno_acqic_nunique</th>\n",
              "      <th>mchno_iterm_0_norm_count</th>\n",
              "      <th>mchno_hcefg_0_norm_count</th>\n",
              "      <th>cano_hcefg_5_norm_count</th>\n",
              "      <th>cano_etymd_6_norm_count</th>\n",
              "      <th>txkey</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1521787</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>77950</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>592489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521788</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>79549</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>592452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521789</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>60355</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>590212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521790</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>60296</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>590209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521791</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>6862</td>\n",
              "      <td>0</td>\n",
              "      <td>77933</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>16158</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59060</td>\n",
              "      <td>116168</td>\n",
              "      <td>457</td>\n",
              "      <td>0</td>\n",
              "      <td>162489</td>\n",
              "      <td>0.00193</td>\n",
              "      <td>0.99807</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.998656</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.987376</td>\n",
              "      <td>0.012624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.983448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.989191</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009383</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>592488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 121 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         stscd  ecfg  ...  cano_etymd_6_norm_count   txkey\n",
              "1521787      0     0  ...                      0.0  592489\n",
              "1521788      0     0  ...                      0.0  592452\n",
              "1521789      0     0  ...                      0.0  590212\n",
              "1521790      0     0  ...                      0.0  590209\n",
              "1521791      0     0  ...                      0.0  592488\n",
              "\n",
              "[5 rows x 121 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq9VQAwtwMPr",
        "colab_type": "text"
      },
      "source": [
        "# 定義 features & num_features & target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTyemZPdwJR2",
        "colab_type": "code",
        "outputId": "99a44989-e2f7-49a7-bf69-96a85daa7f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# 全部特徵\n",
        "features = train.drop(['fraud_ind', # just target\n",
        "                       'txkey', # just like index\n",
        "                       ],axis=1).columns.tolist()\n",
        "\n",
        "# 新特徵才是num_features\n",
        "num_features = sorted(list(set(features)^set(中文map.keys())))\n",
        "num_features.remove('fraud_ind')\n",
        "num_features.remove('locdt')\n",
        "num_features.remove('txkey')\n",
        "\n",
        "#只用num_features\n",
        "features = num_features\n",
        "\n",
        "y_name = 'fraud_ind'\n",
        "\n",
        "print(len(features),features)\n",
        "print(len(num_features),num_features)\n",
        "print(len([y_name]),[y_name])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 ['acqic_bacno_nunique', 'acqic_cano_nunique', 'acqic_contp_2_norm_count', 'acqic_contp_5_norm_count', 'acqic_contp_6_norm_count', 'acqic_contp_nunique', 'acqic_csmcu_nunique', 'acqic_etymd_0_norm_count', 'acqic_etymd_10_norm_count', 'acqic_etymd_2_norm_count', 'acqic_etymd_4_norm_count', 'acqic_etymd_5_norm_count', 'acqic_etymd_6_norm_count', 'acqic_etymd_7_norm_count', 'acqic_etymd_8_norm_count', 'acqic_etymd_nunique', 'acqic_hcefg_0_norm_count', 'acqic_hcefg_1_norm_count', 'acqic_hcefg_2_norm_count', 'acqic_hcefg_5_norm_count', 'acqic_hcefg_7_norm_count', 'acqic_hcefg_8_norm_count', 'acqic_hcefg_9_norm_count', 'acqic_hcefg_nunique', 'acqic_iterm_0_norm_count', 'acqic_iterm_1_norm_count', 'acqic_iterm_2_norm_count', 'acqic_iterm_3_norm_count', 'acqic_iterm_4_norm_count', 'acqic_iterm_5_norm_count', 'acqic_iterm_6_norm_count', 'acqic_iterm_7_norm_count', 'acqic_iterm_8_norm_count', 'acqic_iterm_nunique', 'acqic_mcc_nunique', 'acqic_mchno_nunique', 'acqic_ovrlt_0_norm_count', 'acqic_ovrlt_1_norm_count', 'acqic_scity_nunique', 'acqic_stocn_nunique', 'acqic_stscd_0_norm_count', 'acqic_stscd_2_norm_count', 'acqic_stscd_nunique', 'bacno_acqic_nunique', 'bacno_cano_nunique', 'bacno_csmcu_nunique', 'bacno_etymd_0_norm_count', 'bacno_etymd_4_norm_count', 'bacno_etymd_5_norm_count', 'bacno_etymd_8_norm_count', 'bacno_scity_nunique', 'bacno_stocn_nunique', 'bacno_stscd_0_norm_count', 'bacno_stscd_2_norm_count', 'bacno_stscd_nunique', 'cano_contp_2_norm_count', 'cano_contp_nunique', 'cano_csmcu_nunique', 'cano_etymd_0_norm_count', 'cano_etymd_2_norm_count', 'cano_etymd_4_norm_count', 'cano_etymd_5_norm_count', 'cano_etymd_6_norm_count', 'cano_etymd_8_norm_count', 'cano_etymd_nunique', 'cano_hcefg_5_norm_count', 'cano_hcefg_nunique', 'cano_iterm_nunique', 'cano_mcc_nunique', 'cano_mchno_nunique', 'cano_scity_nunique', 'cano_stocn_nunique', 'cano_stscd_0_norm_count', 'cano_stscd_2_norm_count', 'cano_stscd_nunique', 'mchno_contp_2_norm_count', 'mchno_contp_5_norm_count', 'mchno_contp_6_norm_count', 'mchno_contp_nunique', 'mchno_csmcu_nunique', 'mchno_etymd_0_norm_count', 'mchno_etymd_2_norm_count', 'mchno_etymd_4_norm_count', 'mchno_etymd_5_norm_count', 'mchno_etymd_7_norm_count', 'mchno_etymd_8_norm_count', 'mchno_etymd_nunique', 'mchno_hcefg_0_norm_count', 'mchno_hcefg_1_norm_count', 'mchno_hcefg_5_norm_count', 'mchno_hcefg_nunique', 'mchno_iterm_0_norm_count', 'mchno_iterm_nunique', 'mchno_ovrlt_0_norm_count', 'mchno_ovrlt_1_norm_count', 'mchno_ovrlt_nunique', 'mchno_stocn_nunique', 'mchno_stscd_0_norm_count', 'mchno_stscd_2_norm_count', 'mchno_stscd_nunique']\n",
            "100 ['acqic_bacno_nunique', 'acqic_cano_nunique', 'acqic_contp_2_norm_count', 'acqic_contp_5_norm_count', 'acqic_contp_6_norm_count', 'acqic_contp_nunique', 'acqic_csmcu_nunique', 'acqic_etymd_0_norm_count', 'acqic_etymd_10_norm_count', 'acqic_etymd_2_norm_count', 'acqic_etymd_4_norm_count', 'acqic_etymd_5_norm_count', 'acqic_etymd_6_norm_count', 'acqic_etymd_7_norm_count', 'acqic_etymd_8_norm_count', 'acqic_etymd_nunique', 'acqic_hcefg_0_norm_count', 'acqic_hcefg_1_norm_count', 'acqic_hcefg_2_norm_count', 'acqic_hcefg_5_norm_count', 'acqic_hcefg_7_norm_count', 'acqic_hcefg_8_norm_count', 'acqic_hcefg_9_norm_count', 'acqic_hcefg_nunique', 'acqic_iterm_0_norm_count', 'acqic_iterm_1_norm_count', 'acqic_iterm_2_norm_count', 'acqic_iterm_3_norm_count', 'acqic_iterm_4_norm_count', 'acqic_iterm_5_norm_count', 'acqic_iterm_6_norm_count', 'acqic_iterm_7_norm_count', 'acqic_iterm_8_norm_count', 'acqic_iterm_nunique', 'acqic_mcc_nunique', 'acqic_mchno_nunique', 'acqic_ovrlt_0_norm_count', 'acqic_ovrlt_1_norm_count', 'acqic_scity_nunique', 'acqic_stocn_nunique', 'acqic_stscd_0_norm_count', 'acqic_stscd_2_norm_count', 'acqic_stscd_nunique', 'bacno_acqic_nunique', 'bacno_cano_nunique', 'bacno_csmcu_nunique', 'bacno_etymd_0_norm_count', 'bacno_etymd_4_norm_count', 'bacno_etymd_5_norm_count', 'bacno_etymd_8_norm_count', 'bacno_scity_nunique', 'bacno_stocn_nunique', 'bacno_stscd_0_norm_count', 'bacno_stscd_2_norm_count', 'bacno_stscd_nunique', 'cano_contp_2_norm_count', 'cano_contp_nunique', 'cano_csmcu_nunique', 'cano_etymd_0_norm_count', 'cano_etymd_2_norm_count', 'cano_etymd_4_norm_count', 'cano_etymd_5_norm_count', 'cano_etymd_6_norm_count', 'cano_etymd_8_norm_count', 'cano_etymd_nunique', 'cano_hcefg_5_norm_count', 'cano_hcefg_nunique', 'cano_iterm_nunique', 'cano_mcc_nunique', 'cano_mchno_nunique', 'cano_scity_nunique', 'cano_stocn_nunique', 'cano_stscd_0_norm_count', 'cano_stscd_2_norm_count', 'cano_stscd_nunique', 'mchno_contp_2_norm_count', 'mchno_contp_5_norm_count', 'mchno_contp_6_norm_count', 'mchno_contp_nunique', 'mchno_csmcu_nunique', 'mchno_etymd_0_norm_count', 'mchno_etymd_2_norm_count', 'mchno_etymd_4_norm_count', 'mchno_etymd_5_norm_count', 'mchno_etymd_7_norm_count', 'mchno_etymd_8_norm_count', 'mchno_etymd_nunique', 'mchno_hcefg_0_norm_count', 'mchno_hcefg_1_norm_count', 'mchno_hcefg_5_norm_count', 'mchno_hcefg_nunique', 'mchno_iterm_0_norm_count', 'mchno_iterm_nunique', 'mchno_ovrlt_0_norm_count', 'mchno_ovrlt_1_norm_count', 'mchno_ovrlt_nunique', 'mchno_stocn_nunique', 'mchno_stscd_0_norm_count', 'mchno_stscd_2_norm_count', 'mchno_stscd_nunique']\n",
            "1 ['fraud_ind']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrAlrJU3xqpd",
        "colab_type": "text"
      },
      "source": [
        "# both / test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtKH9MW-wwS7",
        "colab_type": "code",
        "outputId": "d7ae7627-991c-4a3e-cb8e-702fe26e1ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "f = {}\n",
        "for col in num_features:\n",
        "  both_value = set(train[col].unique())&set(test[col].unique())\n",
        "  f[col] = len(both_value)/len(test[col].unique())\n",
        "res = pd.DataFrame(f,index=['both/test']).T.sort_values(by='both/test',ascending=False)\n",
        "res.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>both/test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bacno_stscd_2_norm_count</th>\n",
              "      <td>0.878689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bacno_stscd_0_norm_count</th>\n",
              "      <td>0.873786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cano_stscd_0_norm_count</th>\n",
              "      <td>0.851240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bacno_cano_nunique</th>\n",
              "      <td>0.846154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cano_stscd_2_norm_count</th>\n",
              "      <td>0.843882</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          both/test\n",
              "bacno_stscd_2_norm_count   0.878689\n",
              "bacno_stscd_0_norm_count   0.873786\n",
              "cano_stscd_0_norm_count    0.851240\n",
              "bacno_cano_nunique         0.846154\n",
              "cano_stscd_2_norm_count    0.843882"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abpoOjRCx7jw",
        "colab_type": "text"
      },
      "source": [
        "# 切分 train 跟 val_1 , val_2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtSwUARSxu24",
        "colab_type": "code",
        "outputId": "2ccc3ccc-ead2-4dc2-f082-98ce7dc2da3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train ,val = train_test_split(train[features+[y_name]] ,test_size=0.25 ,random_state=42)\n",
        "val_1 ,val_2 = train_test_split(val[features+[y_name]] ,test_size=0.50 ,random_state=42)\n",
        "print(train.shape)\n",
        "print(val_1.shape)\n",
        "print(val_2.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1141340, 101)\n",
            "(190223, 101)\n",
            "(190224, 101)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5PaF81P9vn6",
        "colab_type": "text"
      },
      "source": [
        "# scaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-uO7lX79vvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler =  StandardScaler().fit(train[features])\n",
        "\n",
        "train[features] = scaler.transform(train[features])\n",
        "val_1[features] = scaler.transform(val_1[features])\n",
        "val_2[features] = scaler.transform(val_2[features])\n",
        "test[features] = scaler.transform(test[features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec_uR3ocyOqT",
        "colab_type": "text"
      },
      "source": [
        "# 建立ANN模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLiHnkzh2h97",
        "colab_type": "text"
      },
      "source": [
        "評價函數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAS1pRSz1iXo",
        "colab_type": "code",
        "outputId": "a2fe3752-f637-44a6-fa60-9ba9f1ff6dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqSfcw9AGr9-",
        "colab_type": "text"
      },
      "source": [
        "# optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM0YuK5LGsGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "adam = keras.optimizers.Adam(lr=0.001/2)#除2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um7BPdAcyJ9R",
        "colab_type": "code",
        "outputId": "078ace8d-01c6-4b5b-d377-35cbb85c84d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "source": [
        "from keras import backend\n",
        "from keras.layers import Dropout,Dense,Flatten\n",
        "from keras.models import Sequential\n",
        "import math\n",
        "\n",
        "model = Sequential()\n",
        "num_unit = 128\n",
        "num_layers = 5\n",
        "out_dim = 1\n",
        "input_dim = 100\n",
        "\n",
        "#輸入層\n",
        "model.add(Dense(num_unit,\n",
        "                activation = 'relu',\n",
        "                input_dim = input_dim))\n",
        "\n",
        "#隱藏層\n",
        "for i in range(num_layers-1):\n",
        "  model.add(Dense(num_unit,\n",
        "                  activation = 'relu'))\n",
        "\n",
        "#輸出層\n",
        "model.add(Dense(out_dim,\n",
        "                activation = 'sigmoid'))\n",
        "\n",
        "#編譯\n",
        "model.compile(optimizer = adam,\n",
        "              loss = 'binary_crossentropy',\n",
        "              metrics=[f1_m])\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               12928     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 79,105\n",
            "Trainable params: 79,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffdx3Sd_z8aV",
        "colab_type": "text"
      },
      "source": [
        "# Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Kj3IBWzJ-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "es = EarlyStopping(monitor='val_loss',\n",
        "                   min_delta=0,\n",
        "                   patience=40,\n",
        "                   mode='min',\n",
        "                   restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpFowLto0Eyx",
        "colab_type": "code",
        "outputId": "f0e52151-ae82-4e12-ddd8-4ca5bf0990ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit( train[features] ,train[y_name],\n",
        "                    epochs = 300,\n",
        "                    batch_size = train.shape[0],\n",
        "                    validation_data = (val_1[features],val_1[y_name]),\n",
        "                    callbacks = [es])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 1141340 samples, validate on 190223 samples\n",
            "Epoch 1/300\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1141340/1141340 [==============================] - 9s 8us/step - loss: 0.8202 - f1_m: 0.0270 - val_loss: 0.6404 - val_f1_m: 0.0160\n",
            "Epoch 2/300\n",
            "1141340/1141340 [==============================] - 4s 4us/step - loss: 0.6403 - f1_m: 0.0159 - val_loss: 0.5151 - val_f1_m: 0.0000e+00\n",
            "Epoch 3/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.5147 - f1_m: 0.0000e+00 - val_loss: 0.4170 - val_f1_m: 0.0000e+00\n",
            "Epoch 4/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.4165 - f1_m: 0.0000e+00 - val_loss: 0.3308 - val_f1_m: 0.0000e+00\n",
            "Epoch 5/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.3302 - f1_m: 0.0000e+00 - val_loss: 0.2541 - val_f1_m: 0.0000e+00\n",
            "Epoch 6/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.2534 - f1_m: 0.0000e+00 - val_loss: 0.1903 - val_f1_m: 0.0000e+00\n",
            "Epoch 7/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.1897 - f1_m: 0.0000e+00 - val_loss: 0.1436 - val_f1_m: 0.0000e+00\n",
            "Epoch 8/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.1428 - f1_m: 0.0000e+00 - val_loss: 0.1146 - val_f1_m: 0.0000e+00\n",
            "Epoch 9/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.1138 - f1_m: 0.0000e+00 - val_loss: 0.1008 - val_f1_m: 0.0000e+00\n",
            "Epoch 10/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0999 - f1_m: 0.0000e+00 - val_loss: 0.0968 - val_f1_m: 0.0000e+00\n",
            "Epoch 11/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0958 - f1_m: 0.0000e+00 - val_loss: 0.0976 - val_f1_m: 0.0000e+00\n",
            "Epoch 12/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0964 - f1_m: 0.0000e+00 - val_loss: 0.0993 - val_f1_m: 0.0000e+00\n",
            "Epoch 13/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0981 - f1_m: 0.0000e+00 - val_loss: 0.1001 - val_f1_m: 0.0000e+00\n",
            "Epoch 14/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0988 - f1_m: 0.0000e+00 - val_loss: 0.0991 - val_f1_m: 0.0000e+00\n",
            "Epoch 15/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0978 - f1_m: 0.0000e+00 - val_loss: 0.0963 - val_f1_m: 0.0000e+00\n",
            "Epoch 16/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0950 - f1_m: 0.0000e+00 - val_loss: 0.0920 - val_f1_m: 0.0000e+00\n",
            "Epoch 17/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0907 - f1_m: 0.0000e+00 - val_loss: 0.0866 - val_f1_m: 0.0000e+00\n",
            "Epoch 18/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0854 - f1_m: 0.0000e+00 - val_loss: 0.0806 - val_f1_m: 0.0000e+00\n",
            "Epoch 19/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0795 - f1_m: 0.0000e+00 - val_loss: 0.0746 - val_f1_m: 0.0000e+00\n",
            "Epoch 20/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0736 - f1_m: 0.0000e+00 - val_loss: 0.0689 - val_f1_m: 0.0000e+00\n",
            "Epoch 21/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0679 - f1_m: 0.0000e+00 - val_loss: 0.0638 - val_f1_m: 0.0000e+00\n",
            "Epoch 22/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0629 - f1_m: 0.0000e+00 - val_loss: 0.0595 - val_f1_m: 0.0000e+00\n",
            "Epoch 23/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0587 - f1_m: 0.0000e+00 - val_loss: 0.0562 - val_f1_m: 0.0000e+00\n",
            "Epoch 24/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0555 - f1_m: 0.0000e+00 - val_loss: 0.0539 - val_f1_m: 0.0000e+00\n",
            "Epoch 25/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0532 - f1_m: 0.0000e+00 - val_loss: 0.0524 - val_f1_m: 0.0000e+00\n",
            "Epoch 26/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0518 - f1_m: 0.0000e+00 - val_loss: 0.0515 - val_f1_m: 0.0000e+00\n",
            "Epoch 27/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0509 - f1_m: 0.0000e+00 - val_loss: 0.0510 - val_f1_m: 0.0000e+00\n",
            "Epoch 28/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0505 - f1_m: 0.0000e+00 - val_loss: 0.0508 - val_f1_m: 0.0000e+00\n",
            "Epoch 29/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0503 - f1_m: 0.0000e+00 - val_loss: 0.0507 - val_f1_m: 0.0000e+00\n",
            "Epoch 30/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0503 - f1_m: 0.0000e+00 - val_loss: 0.0507 - val_f1_m: 0.0000e+00\n",
            "Epoch 31/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0502 - f1_m: 0.0000e+00 - val_loss: 0.0505 - val_f1_m: 0.0000e+00\n",
            "Epoch 32/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0501 - f1_m: 0.0000e+00 - val_loss: 0.0502 - val_f1_m: 0.0000e+00\n",
            "Epoch 33/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0498 - f1_m: 0.0000e+00 - val_loss: 0.0497 - val_f1_m: 0.0000e+00\n",
            "Epoch 34/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0493 - f1_m: 0.0000e+00 - val_loss: 0.0490 - val_f1_m: 0.0000e+00\n",
            "Epoch 35/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0487 - f1_m: 0.0000e+00 - val_loss: 0.0482 - val_f1_m: 0.0000e+00\n",
            "Epoch 36/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0479 - f1_m: 0.0000e+00 - val_loss: 0.0474 - val_f1_m: 0.0000e+00\n",
            "Epoch 37/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0471 - f1_m: 0.0000e+00 - val_loss: 0.0465 - val_f1_m: 0.0000e+00\n",
            "Epoch 38/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0462 - f1_m: 0.0000e+00 - val_loss: 0.0457 - val_f1_m: 0.0000e+00\n",
            "Epoch 39/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0454 - f1_m: 0.0000e+00 - val_loss: 0.0449 - val_f1_m: 0.0000e+00\n",
            "Epoch 40/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0446 - f1_m: 0.0000e+00 - val_loss: 0.0443 - val_f1_m: 0.0000e+00\n",
            "Epoch 41/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0440 - f1_m: 0.0000e+00 - val_loss: 0.0438 - val_f1_m: 0.0000e+00\n",
            "Epoch 42/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0436 - f1_m: 0.0000e+00 - val_loss: 0.0434 - val_f1_m: 0.0000e+00\n",
            "Epoch 43/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0432 - f1_m: 0.0000e+00 - val_loss: 0.0432 - val_f1_m: 0.0000e+00\n",
            "Epoch 44/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0430 - f1_m: 0.0000e+00 - val_loss: 0.0430 - val_f1_m: 0.0000e+00\n",
            "Epoch 45/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0428 - f1_m: 0.0000e+00 - val_loss: 0.0428 - val_f1_m: 0.0000e+00\n",
            "Epoch 46/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0426 - f1_m: 0.0000e+00 - val_loss: 0.0426 - val_f1_m: 0.0000e+00\n",
            "Epoch 47/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0424 - f1_m: 0.0000e+00 - val_loss: 0.0424 - val_f1_m: 0.0000e+00\n",
            "Epoch 48/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0422 - f1_m: 0.0000e+00 - val_loss: 0.0422 - val_f1_m: 0.0000e+00\n",
            "Epoch 49/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0420 - f1_m: 0.0000e+00 - val_loss: 0.0419 - val_f1_m: 0.0000e+00\n",
            "Epoch 50/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0417 - f1_m: 0.0000e+00 - val_loss: 0.0415 - val_f1_m: 0.0000e+00\n",
            "Epoch 51/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0413 - f1_m: 0.0000e+00 - val_loss: 0.0412 - val_f1_m: 0.0000e+00\n",
            "Epoch 52/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0410 - f1_m: 0.0000e+00 - val_loss: 0.0408 - val_f1_m: 0.0000e+00\n",
            "Epoch 53/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0407 - f1_m: 0.0000e+00 - val_loss: 0.0405 - val_f1_m: 0.0000e+00\n",
            "Epoch 54/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0404 - f1_m: 0.0000e+00 - val_loss: 0.0403 - val_f1_m: 0.0000e+00\n",
            "Epoch 55/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0401 - f1_m: 0.0000e+00 - val_loss: 0.0401 - val_f1_m: 0.0000e+00\n",
            "Epoch 56/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0399 - f1_m: 0.0000e+00 - val_loss: 0.0399 - val_f1_m: 0.0102\n",
            "Epoch 57/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0397 - f1_m: 0.0087 - val_loss: 0.0398 - val_f1_m: 0.0566\n",
            "Epoch 58/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0396 - f1_m: 0.0528 - val_loss: 0.0397 - val_f1_m: 0.1134\n",
            "Epoch 59/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0395 - f1_m: 0.1036 - val_loss: 0.0396 - val_f1_m: 0.1692\n",
            "Epoch 60/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0394 - f1_m: 0.1536 - val_loss: 0.0395 - val_f1_m: 0.1995\n",
            "Epoch 61/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0393 - f1_m: 0.1888 - val_loss: 0.0393 - val_f1_m: 0.2204\n",
            "Epoch 62/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0392 - f1_m: 0.2118 - val_loss: 0.0392 - val_f1_m: 0.2369\n",
            "Epoch 63/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0390 - f1_m: 0.2282 - val_loss: 0.0390 - val_f1_m: 0.2482\n",
            "Epoch 64/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0389 - f1_m: 0.2412 - val_loss: 0.0389 - val_f1_m: 0.2545\n",
            "Epoch 65/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0387 - f1_m: 0.2493 - val_loss: 0.0388 - val_f1_m: 0.2619\n",
            "Epoch 66/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0386 - f1_m: 0.2562 - val_loss: 0.0387 - val_f1_m: 0.2662\n",
            "Epoch 67/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0385 - f1_m: 0.2599 - val_loss: 0.0386 - val_f1_m: 0.2699\n",
            "Epoch 68/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0384 - f1_m: 0.2630 - val_loss: 0.0385 - val_f1_m: 0.2732\n",
            "Epoch 69/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0383 - f1_m: 0.2668 - val_loss: 0.0384 - val_f1_m: 0.2780\n",
            "Epoch 70/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0383 - f1_m: 0.2712 - val_loss: 0.0384 - val_f1_m: 0.2924\n",
            "Epoch 71/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0382 - f1_m: 0.2757 - val_loss: 0.0383 - val_f1_m: 0.2953\n",
            "Epoch 72/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0381 - f1_m: 0.2820 - val_loss: 0.0382 - val_f1_m: 0.3047\n",
            "Epoch 73/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0380 - f1_m: 0.2883 - val_loss: 0.0381 - val_f1_m: 0.3132\n",
            "Epoch 74/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0379 - f1_m: 0.2936 - val_loss: 0.0380 - val_f1_m: 0.3228\n",
            "Epoch 75/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0378 - f1_m: 0.3041 - val_loss: 0.0379 - val_f1_m: 0.3290\n",
            "Epoch 76/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0378 - f1_m: 0.3120 - val_loss: 0.0378 - val_f1_m: 0.3376\n",
            "Epoch 77/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0377 - f1_m: 0.3183 - val_loss: 0.0378 - val_f1_m: 0.3431\n",
            "Epoch 78/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0376 - f1_m: 0.3271 - val_loss: 0.0377 - val_f1_m: 0.3481\n",
            "Epoch 79/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0376 - f1_m: 0.3333 - val_loss: 0.0377 - val_f1_m: 0.3530\n",
            "Epoch 80/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0375 - f1_m: 0.3380 - val_loss: 0.0376 - val_f1_m: 0.3544\n",
            "Epoch 81/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0374 - f1_m: 0.3426 - val_loss: 0.0375 - val_f1_m: 0.3578\n",
            "Epoch 82/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0374 - f1_m: 0.3448 - val_loss: 0.0375 - val_f1_m: 0.3592\n",
            "Epoch 83/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0373 - f1_m: 0.3453 - val_loss: 0.0374 - val_f1_m: 0.3584\n",
            "Epoch 84/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0372 - f1_m: 0.3457 - val_loss: 0.0373 - val_f1_m: 0.3584\n",
            "Epoch 85/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0371 - f1_m: 0.3447 - val_loss: 0.0373 - val_f1_m: 0.3568\n",
            "Epoch 86/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0371 - f1_m: 0.3429 - val_loss: 0.0372 - val_f1_m: 0.3559\n",
            "Epoch 87/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0370 - f1_m: 0.3406 - val_loss: 0.0372 - val_f1_m: 0.3559\n",
            "Epoch 88/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0370 - f1_m: 0.3397 - val_loss: 0.0371 - val_f1_m: 0.3559\n",
            "Epoch 89/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0369 - f1_m: 0.3394 - val_loss: 0.0371 - val_f1_m: 0.3554\n",
            "Epoch 90/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0368 - f1_m: 0.3387 - val_loss: 0.0370 - val_f1_m: 0.3572\n",
            "Epoch 91/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0367 - f1_m: 0.3405 - val_loss: 0.0369 - val_f1_m: 0.3595\n",
            "Epoch 92/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0366 - f1_m: 0.3441 - val_loss: 0.0369 - val_f1_m: 0.3650\n",
            "Epoch 93/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0365 - f1_m: 0.3495 - val_loss: 0.0368 - val_f1_m: 0.3708\n",
            "Epoch 94/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0364 - f1_m: 0.3546 - val_loss: 0.0367 - val_f1_m: 0.3748\n",
            "Epoch 95/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0363 - f1_m: 0.3589 - val_loss: 0.0366 - val_f1_m: 0.3806\n",
            "Epoch 96/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0362 - f1_m: 0.3639 - val_loss: 0.0366 - val_f1_m: 0.3822\n",
            "Epoch 97/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0361 - f1_m: 0.3680 - val_loss: 0.0365 - val_f1_m: 0.3862\n",
            "Epoch 98/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0360 - f1_m: 0.3727 - val_loss: 0.0363 - val_f1_m: 0.3850\n",
            "Epoch 99/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0359 - f1_m: 0.3762 - val_loss: 0.0362 - val_f1_m: 0.3861\n",
            "Epoch 100/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0358 - f1_m: 0.3798 - val_loss: 0.0361 - val_f1_m: 0.3871\n",
            "Epoch 101/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0357 - f1_m: 0.3816 - val_loss: 0.0360 - val_f1_m: 0.3877\n",
            "Epoch 102/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0356 - f1_m: 0.3832 - val_loss: 0.0359 - val_f1_m: 0.3913\n",
            "Epoch 103/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0355 - f1_m: 0.3858 - val_loss: 0.0358 - val_f1_m: 0.3940\n",
            "Epoch 104/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0354 - f1_m: 0.3892 - val_loss: 0.0358 - val_f1_m: 0.3984\n",
            "Epoch 105/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0354 - f1_m: 0.3935 - val_loss: 0.0357 - val_f1_m: 0.4001\n",
            "Epoch 106/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0353 - f1_m: 0.3981 - val_loss: 0.0356 - val_f1_m: 0.4047\n",
            "Epoch 107/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0353 - f1_m: 0.4020 - val_loss: 0.0356 - val_f1_m: 0.4109\n",
            "Epoch 108/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0352 - f1_m: 0.4067 - val_loss: 0.0355 - val_f1_m: 0.4146\n",
            "Epoch 109/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0351 - f1_m: 0.4103 - val_loss: 0.0355 - val_f1_m: 0.4211\n",
            "Epoch 110/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0351 - f1_m: 0.4129 - val_loss: 0.0354 - val_f1_m: 0.4206\n",
            "Epoch 111/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0350 - f1_m: 0.4141 - val_loss: 0.0354 - val_f1_m: 0.4197\n",
            "Epoch 112/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0350 - f1_m: 0.4144 - val_loss: 0.0353 - val_f1_m: 0.4188\n",
            "Epoch 113/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0349 - f1_m: 0.4142 - val_loss: 0.0352 - val_f1_m: 0.4168\n",
            "Epoch 114/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0348 - f1_m: 0.4143 - val_loss: 0.0352 - val_f1_m: 0.4170\n",
            "Epoch 115/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0348 - f1_m: 0.4145 - val_loss: 0.0351 - val_f1_m: 0.4182\n",
            "Epoch 116/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0347 - f1_m: 0.4147 - val_loss: 0.0351 - val_f1_m: 0.4207\n",
            "Epoch 117/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0346 - f1_m: 0.4175 - val_loss: 0.0350 - val_f1_m: 0.4219\n",
            "Epoch 118/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0346 - f1_m: 0.4198 - val_loss: 0.0349 - val_f1_m: 0.4232\n",
            "Epoch 119/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0345 - f1_m: 0.4241 - val_loss: 0.0349 - val_f1_m: 0.4246\n",
            "Epoch 120/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0345 - f1_m: 0.4272 - val_loss: 0.0348 - val_f1_m: 0.4278\n",
            "Epoch 121/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0344 - f1_m: 0.4296 - val_loss: 0.0348 - val_f1_m: 0.4264\n",
            "Epoch 122/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0343 - f1_m: 0.4279 - val_loss: 0.0347 - val_f1_m: 0.4288\n",
            "Epoch 123/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0343 - f1_m: 0.4306 - val_loss: 0.0346 - val_f1_m: 0.4302\n",
            "Epoch 124/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0342 - f1_m: 0.4324 - val_loss: 0.0346 - val_f1_m: 0.4306\n",
            "Epoch 125/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0342 - f1_m: 0.4337 - val_loss: 0.0345 - val_f1_m: 0.4314\n",
            "Epoch 126/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0341 - f1_m: 0.4351 - val_loss: 0.0345 - val_f1_m: 0.4333\n",
            "Epoch 127/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0341 - f1_m: 0.4369 - val_loss: 0.0344 - val_f1_m: 0.4364\n",
            "Epoch 128/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0340 - f1_m: 0.4392 - val_loss: 0.0344 - val_f1_m: 0.4359\n",
            "Epoch 129/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0339 - f1_m: 0.4391 - val_loss: 0.0343 - val_f1_m: 0.4344\n",
            "Epoch 130/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0339 - f1_m: 0.4399 - val_loss: 0.0343 - val_f1_m: 0.4349\n",
            "Epoch 131/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0338 - f1_m: 0.4401 - val_loss: 0.0342 - val_f1_m: 0.4358\n",
            "Epoch 132/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0338 - f1_m: 0.4387 - val_loss: 0.0342 - val_f1_m: 0.4344\n",
            "Epoch 133/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0337 - f1_m: 0.4405 - val_loss: 0.0341 - val_f1_m: 0.4362\n",
            "Epoch 134/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0337 - f1_m: 0.4414 - val_loss: 0.0341 - val_f1_m: 0.4379\n",
            "Epoch 135/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0336 - f1_m: 0.4429 - val_loss: 0.0340 - val_f1_m: 0.4376\n",
            "Epoch 136/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0336 - f1_m: 0.4437 - val_loss: 0.0340 - val_f1_m: 0.4380\n",
            "Epoch 137/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0335 - f1_m: 0.4454 - val_loss: 0.0340 - val_f1_m: 0.4378\n",
            "Epoch 138/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0334 - f1_m: 0.4456 - val_loss: 0.0339 - val_f1_m: 0.4372\n",
            "Epoch 139/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0334 - f1_m: 0.4462 - val_loss: 0.0339 - val_f1_m: 0.4380\n",
            "Epoch 140/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0333 - f1_m: 0.4461 - val_loss: 0.0338 - val_f1_m: 0.4382\n",
            "Epoch 141/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0333 - f1_m: 0.4470 - val_loss: 0.0338 - val_f1_m: 0.4400\n",
            "Epoch 142/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0332 - f1_m: 0.4489 - val_loss: 0.0337 - val_f1_m: 0.4402\n",
            "Epoch 143/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0332 - f1_m: 0.4496 - val_loss: 0.0337 - val_f1_m: 0.4411\n",
            "Epoch 144/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0331 - f1_m: 0.4510 - val_loss: 0.0337 - val_f1_m: 0.4409\n",
            "Epoch 145/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0331 - f1_m: 0.4515 - val_loss: 0.0336 - val_f1_m: 0.4389\n",
            "Epoch 146/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0330 - f1_m: 0.4510 - val_loss: 0.0336 - val_f1_m: 0.4391\n",
            "Epoch 147/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0330 - f1_m: 0.4507 - val_loss: 0.0335 - val_f1_m: 0.4411\n",
            "Epoch 148/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0329 - f1_m: 0.4515 - val_loss: 0.0335 - val_f1_m: 0.4396\n",
            "Epoch 149/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0329 - f1_m: 0.4520 - val_loss: 0.0335 - val_f1_m: 0.4409\n",
            "Epoch 150/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0328 - f1_m: 0.4521 - val_loss: 0.0334 - val_f1_m: 0.4408\n",
            "Epoch 151/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0328 - f1_m: 0.4547 - val_loss: 0.0334 - val_f1_m: 0.4416\n",
            "Epoch 152/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0327 - f1_m: 0.4562 - val_loss: 0.0333 - val_f1_m: 0.4463\n",
            "Epoch 153/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0327 - f1_m: 0.4572 - val_loss: 0.0333 - val_f1_m: 0.4493\n",
            "Epoch 154/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0326 - f1_m: 0.4601 - val_loss: 0.0333 - val_f1_m: 0.4491\n",
            "Epoch 155/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0326 - f1_m: 0.4606 - val_loss: 0.0332 - val_f1_m: 0.4482\n",
            "Epoch 156/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0325 - f1_m: 0.4615 - val_loss: 0.0332 - val_f1_m: 0.4474\n",
            "Epoch 157/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0325 - f1_m: 0.4629 - val_loss: 0.0331 - val_f1_m: 0.4477\n",
            "Epoch 158/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0324 - f1_m: 0.4630 - val_loss: 0.0331 - val_f1_m: 0.4470\n",
            "Epoch 159/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0324 - f1_m: 0.4635 - val_loss: 0.0331 - val_f1_m: 0.4482\n",
            "Epoch 160/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0323 - f1_m: 0.4644 - val_loss: 0.0330 - val_f1_m: 0.4483\n",
            "Epoch 161/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0323 - f1_m: 0.4650 - val_loss: 0.0330 - val_f1_m: 0.4485\n",
            "Epoch 162/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0323 - f1_m: 0.4664 - val_loss: 0.0329 - val_f1_m: 0.4499\n",
            "Epoch 163/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0322 - f1_m: 0.4677 - val_loss: 0.0329 - val_f1_m: 0.4499\n",
            "Epoch 164/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0322 - f1_m: 0.4682 - val_loss: 0.0328 - val_f1_m: 0.4496\n",
            "Epoch 165/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0321 - f1_m: 0.4683 - val_loss: 0.0328 - val_f1_m: 0.4520\n",
            "Epoch 166/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0321 - f1_m: 0.4695 - val_loss: 0.0328 - val_f1_m: 0.4524\n",
            "Epoch 167/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0320 - f1_m: 0.4705 - val_loss: 0.0327 - val_f1_m: 0.4530\n",
            "Epoch 168/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0320 - f1_m: 0.4711 - val_loss: 0.0327 - val_f1_m: 0.4549\n",
            "Epoch 169/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0319 - f1_m: 0.4727 - val_loss: 0.0326 - val_f1_m: 0.4550\n",
            "Epoch 170/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0319 - f1_m: 0.4735 - val_loss: 0.0326 - val_f1_m: 0.4559\n",
            "Epoch 171/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0318 - f1_m: 0.4749 - val_loss: 0.0325 - val_f1_m: 0.4565\n",
            "Epoch 172/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0318 - f1_m: 0.4750 - val_loss: 0.0325 - val_f1_m: 0.4591\n",
            "Epoch 173/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0317 - f1_m: 0.4770 - val_loss: 0.0324 - val_f1_m: 0.4612\n",
            "Epoch 174/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0317 - f1_m: 0.4791 - val_loss: 0.0324 - val_f1_m: 0.4604\n",
            "Epoch 175/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0316 - f1_m: 0.4793 - val_loss: 0.0324 - val_f1_m: 0.4607\n",
            "Epoch 176/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0316 - f1_m: 0.4797 - val_loss: 0.0323 - val_f1_m: 0.4608\n",
            "Epoch 177/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0315 - f1_m: 0.4802 - val_loss: 0.0323 - val_f1_m: 0.4609\n",
            "Epoch 178/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0315 - f1_m: 0.4800 - val_loss: 0.0323 - val_f1_m: 0.4619\n",
            "Epoch 179/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0314 - f1_m: 0.4804 - val_loss: 0.0322 - val_f1_m: 0.4621\n",
            "Epoch 180/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0314 - f1_m: 0.4818 - val_loss: 0.0322 - val_f1_m: 0.4621\n",
            "Epoch 181/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0313 - f1_m: 0.4822 - val_loss: 0.0321 - val_f1_m: 0.4627\n",
            "Epoch 182/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0313 - f1_m: 0.4817 - val_loss: 0.0321 - val_f1_m: 0.4642\n",
            "Epoch 183/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0312 - f1_m: 0.4830 - val_loss: 0.0321 - val_f1_m: 0.4652\n",
            "Epoch 184/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0312 - f1_m: 0.4847 - val_loss: 0.0320 - val_f1_m: 0.4652\n",
            "Epoch 185/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0312 - f1_m: 0.4848 - val_loss: 0.0320 - val_f1_m: 0.4671\n",
            "Epoch 186/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0311 - f1_m: 0.4854 - val_loss: 0.0320 - val_f1_m: 0.4657\n",
            "Epoch 187/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0311 - f1_m: 0.4843 - val_loss: 0.0319 - val_f1_m: 0.4681\n",
            "Epoch 188/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0310 - f1_m: 0.4879 - val_loss: 0.0319 - val_f1_m: 0.4703\n",
            "Epoch 189/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0310 - f1_m: 0.4898 - val_loss: 0.0319 - val_f1_m: 0.4706\n",
            "Epoch 190/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0309 - f1_m: 0.4895 - val_loss: 0.0318 - val_f1_m: 0.4719\n",
            "Epoch 191/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0309 - f1_m: 0.4909 - val_loss: 0.0318 - val_f1_m: 0.4699\n",
            "Epoch 192/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0308 - f1_m: 0.4902 - val_loss: 0.0318 - val_f1_m: 0.4715\n",
            "Epoch 193/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0308 - f1_m: 0.4929 - val_loss: 0.0317 - val_f1_m: 0.4709\n",
            "Epoch 194/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0307 - f1_m: 0.4925 - val_loss: 0.0317 - val_f1_m: 0.4758\n",
            "Epoch 195/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0307 - f1_m: 0.4992 - val_loss: 0.0317 - val_f1_m: 0.4725\n",
            "Epoch 196/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0306 - f1_m: 0.4963 - val_loss: 0.0316 - val_f1_m: 0.4799\n",
            "Epoch 197/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0306 - f1_m: 0.5013 - val_loss: 0.0316 - val_f1_m: 0.4720\n",
            "Epoch 198/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0306 - f1_m: 0.4972 - val_loss: 0.0316 - val_f1_m: 0.4854\n",
            "Epoch 199/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0305 - f1_m: 0.5056 - val_loss: 0.0316 - val_f1_m: 0.4721\n",
            "Epoch 200/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0305 - f1_m: 0.4967 - val_loss: 0.0315 - val_f1_m: 0.4849\n",
            "Epoch 201/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0304 - f1_m: 0.5066 - val_loss: 0.0315 - val_f1_m: 0.4772\n",
            "Epoch 202/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0304 - f1_m: 0.5001 - val_loss: 0.0314 - val_f1_m: 0.4785\n",
            "Epoch 203/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0303 - f1_m: 0.5023 - val_loss: 0.0314 - val_f1_m: 0.4846\n",
            "Epoch 204/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0303 - f1_m: 0.5097 - val_loss: 0.0314 - val_f1_m: 0.4753\n",
            "Epoch 205/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0303 - f1_m: 0.4985 - val_loss: 0.0313 - val_f1_m: 0.4905\n",
            "Epoch 206/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0302 - f1_m: 0.5119 - val_loss: 0.0313 - val_f1_m: 0.4781\n",
            "Epoch 207/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0302 - f1_m: 0.5015 - val_loss: 0.0313 - val_f1_m: 0.4836\n",
            "Epoch 208/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0301 - f1_m: 0.5081 - val_loss: 0.0313 - val_f1_m: 0.4875\n",
            "Epoch 209/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0301 - f1_m: 0.5113 - val_loss: 0.0312 - val_f1_m: 0.4808\n",
            "Epoch 210/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0300 - f1_m: 0.5042 - val_loss: 0.0312 - val_f1_m: 0.4916\n",
            "Epoch 211/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0300 - f1_m: 0.5142 - val_loss: 0.0312 - val_f1_m: 0.4789\n",
            "Epoch 212/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0300 - f1_m: 0.5042 - val_loss: 0.0311 - val_f1_m: 0.4912\n",
            "Epoch 213/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0299 - f1_m: 0.5139 - val_loss: 0.0311 - val_f1_m: 0.4864\n",
            "Epoch 214/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0299 - f1_m: 0.5099 - val_loss: 0.0311 - val_f1_m: 0.4904\n",
            "Epoch 215/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0298 - f1_m: 0.5126 - val_loss: 0.0311 - val_f1_m: 0.4895\n",
            "Epoch 216/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0298 - f1_m: 0.5121 - val_loss: 0.0310 - val_f1_m: 0.4875\n",
            "Epoch 217/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0297 - f1_m: 0.5110 - val_loss: 0.0310 - val_f1_m: 0.4911\n",
            "Epoch 218/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0297 - f1_m: 0.5159 - val_loss: 0.0310 - val_f1_m: 0.4826\n",
            "Epoch 219/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0297 - f1_m: 0.5087 - val_loss: 0.0310 - val_f1_m: 0.4971\n",
            "Epoch 220/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0296 - f1_m: 0.5204 - val_loss: 0.0310 - val_f1_m: 0.4714\n",
            "Epoch 221/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0297 - f1_m: 0.4985 - val_loss: 0.0310 - val_f1_m: 0.5025\n",
            "Epoch 222/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0296 - f1_m: 0.5257 - val_loss: 0.0310 - val_f1_m: 0.4750\n",
            "Epoch 223/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0296 - f1_m: 0.5029 - val_loss: 0.0309 - val_f1_m: 0.4949\n",
            "Epoch 224/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0295 - f1_m: 0.5182 - val_loss: 0.0309 - val_f1_m: 0.4977\n",
            "Epoch 225/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0294 - f1_m: 0.5216 - val_loss: 0.0309 - val_f1_m: 0.4748\n",
            "Epoch 226/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0295 - f1_m: 0.5022 - val_loss: 0.0308 - val_f1_m: 0.4996\n",
            "Epoch 227/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0294 - f1_m: 0.5237 - val_loss: 0.0308 - val_f1_m: 0.4899\n",
            "Epoch 228/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0293 - f1_m: 0.5157 - val_loss: 0.0308 - val_f1_m: 0.4858\n",
            "Epoch 229/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0293 - f1_m: 0.5104 - val_loss: 0.0308 - val_f1_m: 0.5013\n",
            "Epoch 230/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0293 - f1_m: 0.5249 - val_loss: 0.0308 - val_f1_m: 0.4804\n",
            "Epoch 231/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0292 - f1_m: 0.5085 - val_loss: 0.0307 - val_f1_m: 0.4929\n",
            "Epoch 232/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0291 - f1_m: 0.5213 - val_loss: 0.0307 - val_f1_m: 0.4934\n",
            "Epoch 233/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0291 - f1_m: 0.5221 - val_loss: 0.0307 - val_f1_m: 0.4837\n",
            "Epoch 234/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0291 - f1_m: 0.5118 - val_loss: 0.0306 - val_f1_m: 0.5032\n",
            "Epoch 235/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0291 - f1_m: 0.5285 - val_loss: 0.0306 - val_f1_m: 0.4836\n",
            "Epoch 236/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0290 - f1_m: 0.5141 - val_loss: 0.0306 - val_f1_m: 0.4929\n",
            "Epoch 237/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0289 - f1_m: 0.5213 - val_loss: 0.0306 - val_f1_m: 0.4978\n",
            "Epoch 238/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0289 - f1_m: 0.5260 - val_loss: 0.0306 - val_f1_m: 0.4848\n",
            "Epoch 239/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0289 - f1_m: 0.5158 - val_loss: 0.0305 - val_f1_m: 0.5036\n",
            "Epoch 240/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0289 - f1_m: 0.5299 - val_loss: 0.0305 - val_f1_m: 0.4856\n",
            "Epoch 241/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0288 - f1_m: 0.5163 - val_loss: 0.0305 - val_f1_m: 0.4986\n",
            "Epoch 242/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0288 - f1_m: 0.5271 - val_loss: 0.0304 - val_f1_m: 0.4972\n",
            "Epoch 243/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0287 - f1_m: 0.5249 - val_loss: 0.0304 - val_f1_m: 0.4918\n",
            "Epoch 244/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0287 - f1_m: 0.5233 - val_loss: 0.0304 - val_f1_m: 0.5014\n",
            "Epoch 245/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0287 - f1_m: 0.5307 - val_loss: 0.0304 - val_f1_m: 0.4864\n",
            "Epoch 246/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0286 - f1_m: 0.5187 - val_loss: 0.0304 - val_f1_m: 0.5048\n",
            "Epoch 247/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0286 - f1_m: 0.5326 - val_loss: 0.0304 - val_f1_m: 0.4872\n",
            "Epoch 248/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0286 - f1_m: 0.5201 - val_loss: 0.0303 - val_f1_m: 0.5036\n",
            "Epoch 249/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0285 - f1_m: 0.5330 - val_loss: 0.0303 - val_f1_m: 0.4918\n",
            "Epoch 250/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0285 - f1_m: 0.5238 - val_loss: 0.0303 - val_f1_m: 0.4988\n",
            "Epoch 251/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0284 - f1_m: 0.5297 - val_loss: 0.0302 - val_f1_m: 0.4986\n",
            "Epoch 252/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0284 - f1_m: 0.5285 - val_loss: 0.0302 - val_f1_m: 0.4957\n",
            "Epoch 253/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0283 - f1_m: 0.5279 - val_loss: 0.0302 - val_f1_m: 0.5042\n",
            "Epoch 254/300\n",
            "1141340/1141340 [==============================] - 4s 4us/step - loss: 0.0283 - f1_m: 0.5344 - val_loss: 0.0302 - val_f1_m: 0.4889\n",
            "Epoch 255/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0283 - f1_m: 0.5221 - val_loss: 0.0302 - val_f1_m: 0.5117\n",
            "Epoch 256/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0283 - f1_m: 0.5437 - val_loss: 0.0303 - val_f1_m: 0.4857\n",
            "Epoch 257/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0284 - f1_m: 0.5133 - val_loss: 0.0303 - val_f1_m: 0.5297\n",
            "Epoch 258/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0284 - f1_m: 0.5594 - val_loss: 0.0304 - val_f1_m: 0.4828\n",
            "Epoch 259/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0284 - f1_m: 0.5077 - val_loss: 0.0301 - val_f1_m: 0.5190\n",
            "Epoch 260/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0282 - f1_m: 0.5491 - val_loss: 0.0301 - val_f1_m: 0.5070\n",
            "Epoch 261/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0280 - f1_m: 0.5388 - val_loss: 0.0302 - val_f1_m: 0.4882\n",
            "Epoch 262/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0281 - f1_m: 0.5188 - val_loss: 0.0301 - val_f1_m: 0.5258\n",
            "Epoch 263/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0281 - f1_m: 0.5569 - val_loss: 0.0301 - val_f1_m: 0.4894\n",
            "Epoch 264/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0280 - f1_m: 0.5242 - val_loss: 0.0300 - val_f1_m: 0.5031\n",
            "Epoch 265/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0279 - f1_m: 0.5378 - val_loss: 0.0300 - val_f1_m: 0.5161\n",
            "Epoch 266/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0279 - f1_m: 0.5500 - val_loss: 0.0301 - val_f1_m: 0.4878\n",
            "Epoch 267/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0280 - f1_m: 0.5204 - val_loss: 0.0300 - val_f1_m: 0.5173\n",
            "Epoch 268/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0279 - f1_m: 0.5509 - val_loss: 0.0299 - val_f1_m: 0.5026\n",
            "Epoch 269/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0278 - f1_m: 0.5388 - val_loss: 0.0299 - val_f1_m: 0.4933\n",
            "Epoch 270/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0278 - f1_m: 0.5317 - val_loss: 0.0299 - val_f1_m: 0.5235\n",
            "Epoch 271/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0278 - f1_m: 0.5554 - val_loss: 0.0299 - val_f1_m: 0.4922\n",
            "Epoch 272/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0277 - f1_m: 0.5291 - val_loss: 0.0298 - val_f1_m: 0.5104\n",
            "Epoch 273/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0276 - f1_m: 0.5490 - val_loss: 0.0298 - val_f1_m: 0.5066\n",
            "Epoch 274/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0276 - f1_m: 0.5460 - val_loss: 0.0299 - val_f1_m: 0.4966\n",
            "Epoch 275/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0276 - f1_m: 0.5356 - val_loss: 0.0298 - val_f1_m: 0.5214\n",
            "Epoch 276/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0276 - f1_m: 0.5581 - val_loss: 0.0299 - val_f1_m: 0.4930\n",
            "Epoch 277/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0276 - f1_m: 0.5321 - val_loss: 0.0298 - val_f1_m: 0.5176\n",
            "Epoch 278/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0275 - f1_m: 0.5560 - val_loss: 0.0297 - val_f1_m: 0.5064\n",
            "Epoch 279/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0274 - f1_m: 0.5446 - val_loss: 0.0297 - val_f1_m: 0.5054\n",
            "Epoch 280/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0274 - f1_m: 0.5457 - val_loss: 0.0297 - val_f1_m: 0.5150\n",
            "Epoch 281/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0274 - f1_m: 0.5565 - val_loss: 0.0298 - val_f1_m: 0.4982\n",
            "Epoch 282/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0274 - f1_m: 0.5355 - val_loss: 0.0297 - val_f1_m: 0.5353\n",
            "Epoch 283/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0274 - f1_m: 0.5702 - val_loss: 0.0298 - val_f1_m: 0.4939\n",
            "Epoch 284/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0274 - f1_m: 0.5336 - val_loss: 0.0297 - val_f1_m: 0.5326\n",
            "Epoch 285/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0273 - f1_m: 0.5673 - val_loss: 0.0297 - val_f1_m: 0.5035\n",
            "Epoch 286/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0272 - f1_m: 0.5406 - val_loss: 0.0296 - val_f1_m: 0.5239\n",
            "Epoch 287/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0272 - f1_m: 0.5628 - val_loss: 0.0296 - val_f1_m: 0.5121\n",
            "Epoch 288/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0271 - f1_m: 0.5539 - val_loss: 0.0296 - val_f1_m: 0.5153\n",
            "Epoch 289/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0271 - f1_m: 0.5551 - val_loss: 0.0296 - val_f1_m: 0.5202\n",
            "Epoch 290/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0271 - f1_m: 0.5610 - val_loss: 0.0296 - val_f1_m: 0.5081\n",
            "Epoch 291/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0270 - f1_m: 0.5475 - val_loss: 0.0296 - val_f1_m: 0.5302\n",
            "Epoch 292/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0271 - f1_m: 0.5734 - val_loss: 0.0297 - val_f1_m: 0.4958\n",
            "Epoch 293/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0271 - f1_m: 0.5374 - val_loss: 0.0297 - val_f1_m: 0.5480\n",
            "Epoch 294/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0271 - f1_m: 0.5855 - val_loss: 0.0298 - val_f1_m: 0.4866\n",
            "Epoch 295/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0272 - f1_m: 0.5287 - val_loss: 0.0297 - val_f1_m: 0.5499\n",
            "Epoch 296/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0271 - f1_m: 0.5877 - val_loss: 0.0296 - val_f1_m: 0.5060\n",
            "Epoch 297/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0269 - f1_m: 0.5438 - val_loss: 0.0294 - val_f1_m: 0.5222\n",
            "Epoch 298/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0268 - f1_m: 0.5642 - val_loss: 0.0294 - val_f1_m: 0.5268\n",
            "Epoch 299/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0268 - f1_m: 0.5705 - val_loss: 0.0295 - val_f1_m: 0.5029\n",
            "Epoch 300/300\n",
            "1141340/1141340 [==============================] - 4s 3us/step - loss: 0.0269 - f1_m: 0.5434 - val_loss: 0.0295 - val_f1_m: 0.5502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9gB_dwZ3jkI",
        "colab_type": "text"
      },
      "source": [
        "# 訓練過程評估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S970Vc8k0cp3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "02aa5df2-060f-4747-a4c9-e2a8f07e9150"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n=1\n",
        "\n",
        "fig, ax = plt.subplots(1,1)\n",
        "ax.plot(history.history['loss'][n:], color='b', label=\"Training loss\")\n",
        "ax.plot(history.history['val_loss'][n:], color='r', label=\"validation loss\")\n",
        "legend = ax.legend(loc='best', shadow=True)\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(1,1)\n",
        "ax.plot(history.history['f1_m'][n:], color='b', label=\"Training f1\")\n",
        "ax.plot(history.history['val_f1_m'][n:], color='r', label=\"validation f1\")\n",
        "legend = ax.legend(loc='best', shadow=True)\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCxMeMPw4Zkq",
        "colab_type": "text"
      },
      "source": [
        "# val_2's F1 report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kASFgsao4aF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "84518344-980a-4305-a890-f9b830042b21"
      },
      "source": [
        "from sklearn.metrics import classification_report \n",
        "\n",
        "val_2_pred_01 = pd.Series(model.predict(val_2[features]).ravel()).apply(round)\n",
        "\n",
        "print(classification_report(val_2[y_name],val_2_pred_01,target_names=['0','1']))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99    187682\n",
            "           1       0.69      0.45      0.54      2542\n",
            "\n",
            "    accuracy                           0.99    190224\n",
            "   macro avg       0.84      0.72      0.77    190224\n",
            "weighted avg       0.99      0.99      0.99    190224\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6PFh2lCBwBG",
        "colab_type": "text"
      },
      "source": [
        "# 產生submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNZ2Eing7kUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "efe28d64-dbc9-4b08-814c-1030ca6ffe94"
      },
      "source": [
        "submission = pd.DataFrame({\"txkey\":test_txkey,\n",
        "                           \"fraud_ind\":pd.Series(model.predict(test[features]).ravel()).apply(round).values})\n",
        "\n",
        "# value_counts\n",
        "print(submission[\"fraud_ind\"].value_counts())\n",
        "submission.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    417178\n",
            "1      4487\n",
            "Name: fraud_ind, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>txkey</th>\n",
              "      <th>fraud_ind</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1521787</th>\n",
              "      <td>592489</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521788</th>\n",
              "      <td>592452</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521789</th>\n",
              "      <td>590212</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521790</th>\n",
              "      <td>590209</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521791</th>\n",
              "      <td>592488</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          txkey  fraud_ind\n",
              "1521787  592489          0\n",
              "1521788  592452          0\n",
              "1521789  590212          0\n",
              "1521790  590209          0\n",
              "1521791  592488          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1duNRbRlDgAG",
        "colab_type": "text"
      },
      "source": [
        "# 這裡有個想法如果test上盜刷的比例 跟train上面盜刷的比例 愈相近愈好"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNz1_v5aB33k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "4c7938af-b53c-4cf3-df21-171ac475a9b6"
      },
      "source": [
        "train_p = round((train['fraud_ind'].value_counts()[1] / #盜刷樣本數\n",
        "                len(train['fraud_ind']))*100 ,5) #總數\n",
        "\n",
        "val_1_p = round((val_1['fraud_ind'].value_counts()[1] / #盜刷樣本數\n",
        "                len(val_1['fraud_ind']))*100 ,5) #總數\n",
        "\n",
        "val_2_p = round((val_2['fraud_ind'].value_counts()[1] / #盜刷樣本數\n",
        "                len(val_2['fraud_ind']))*100 ,5) #總數\n",
        "\n",
        "test_p = round((submission['fraud_ind'].value_counts()[1] / #盜刷樣本數\n",
        "                len(submission['fraud_ind']))*100 ,5) #總數\n",
        "pd.DataFrame({'train_p':train_p,\n",
        "              'val_1_p':val_1_p,\n",
        "              'val_2_p':val_2_p,\n",
        "              'test_p':test_p},\n",
        "               index=['盜刷比例%'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_p</th>\n",
              "      <th>val_1_p</th>\n",
              "      <th>val_2_p</th>\n",
              "      <th>test_p</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>盜刷比例%</th>\n",
              "      <td>1.33816</td>\n",
              "      <td>1.33527</td>\n",
              "      <td>1.33632</td>\n",
              "      <td>1.06411</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       train_p  val_1_p  val_2_p   test_p\n",
              "盜刷比例%  1.33816  1.33527  1.33632  1.06411"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNMCpHJ8Dkqc",
        "colab_type": "text"
      },
      "source": [
        "# 保存預測結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSoM9qNeDhyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv(\"./submission_34.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg3PQVLODmUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "942cc41c-8c0c-4176-cce8-92e3b068e217"
      },
      "source": [
        "print(submission[\"fraud_ind\"].value_counts())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    417178\n",
            "1      4487\n",
            "Name: fraud_ind, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix3KqTveDr_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}