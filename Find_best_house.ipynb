{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "化一部第二階段尋找操作條件.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPiIDfS+mYudXD3jBWsDmYf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skywalker0803r/Ricky/blob/master/Find_best_house.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmyMBtXqVaSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import tensor\n",
        "from torch.nn import Linear,ReLU,Sigmoid\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwVDDccaYj0w",
        "colab_type": "text"
      },
      "source": [
        "# data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJpwLl5Yj-A",
        "colab_type": "code",
        "outputId": "4228b504-0e49-4f76-ad9c-5c1f9d974693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "import pandas as pd\n",
        "\n",
        "bos = load_boston()\n",
        "df = pd.DataFrame(bos.data)\n",
        "df.columns = bos.feature_names\n",
        "df['Price'] = bos.target\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...    TAX  PTRATIO       B  LSTAT  Price\n",
              "0  0.00632  18.0   2.31   0.0  0.538  ...  296.0     15.3  396.90   4.98   24.0\n",
              "1  0.02731   0.0   7.07   0.0  0.469  ...  242.0     17.8  396.90   9.14   21.6\n",
              "2  0.02729   0.0   7.07   0.0  0.469  ...  242.0     17.8  392.83   4.03   34.7\n",
              "3  0.03237   0.0   2.18   0.0  0.458  ...  222.0     18.7  394.63   2.94   33.4\n",
              "4  0.06905   0.0   2.18   0.0  0.458  ...  222.0     18.7  396.90   5.33   36.2\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBr4S2qUZ34f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "122f6f6e-02ea-4db4-d1be-4ced096047c8"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.613524</td>\n",
              "      <td>11.363636</td>\n",
              "      <td>11.136779</td>\n",
              "      <td>0.069170</td>\n",
              "      <td>0.554695</td>\n",
              "      <td>6.284634</td>\n",
              "      <td>68.574901</td>\n",
              "      <td>3.795043</td>\n",
              "      <td>9.549407</td>\n",
              "      <td>408.237154</td>\n",
              "      <td>18.455534</td>\n",
              "      <td>356.674032</td>\n",
              "      <td>12.653063</td>\n",
              "      <td>22.532806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.601545</td>\n",
              "      <td>23.322453</td>\n",
              "      <td>6.860353</td>\n",
              "      <td>0.253994</td>\n",
              "      <td>0.115878</td>\n",
              "      <td>0.702617</td>\n",
              "      <td>28.148861</td>\n",
              "      <td>2.105710</td>\n",
              "      <td>8.707259</td>\n",
              "      <td>168.537116</td>\n",
              "      <td>2.164946</td>\n",
              "      <td>91.294864</td>\n",
              "      <td>7.141062</td>\n",
              "      <td>9.197104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.006320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.385000</td>\n",
              "      <td>3.561000</td>\n",
              "      <td>2.900000</td>\n",
              "      <td>1.129600</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>187.000000</td>\n",
              "      <td>12.600000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>1.730000</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.082045</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.190000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.449000</td>\n",
              "      <td>5.885500</td>\n",
              "      <td>45.025000</td>\n",
              "      <td>2.100175</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>279.000000</td>\n",
              "      <td>17.400000</td>\n",
              "      <td>375.377500</td>\n",
              "      <td>6.950000</td>\n",
              "      <td>17.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.256510</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.690000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>6.208500</td>\n",
              "      <td>77.500000</td>\n",
              "      <td>3.207450</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>330.000000</td>\n",
              "      <td>19.050000</td>\n",
              "      <td>391.440000</td>\n",
              "      <td>11.360000</td>\n",
              "      <td>21.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.677083</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>18.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.624000</td>\n",
              "      <td>6.623500</td>\n",
              "      <td>94.075000</td>\n",
              "      <td>5.188425</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>666.000000</td>\n",
              "      <td>20.200000</td>\n",
              "      <td>396.225000</td>\n",
              "      <td>16.955000</td>\n",
              "      <td>25.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>88.976200</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>27.740000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.871000</td>\n",
              "      <td>8.780000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>12.126500</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>711.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>396.900000</td>\n",
              "      <td>37.970000</td>\n",
              "      <td>50.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             CRIM          ZN       INDUS  ...           B       LSTAT       Price\n",
              "count  506.000000  506.000000  506.000000  ...  506.000000  506.000000  506.000000\n",
              "mean     3.613524   11.363636   11.136779  ...  356.674032   12.653063   22.532806\n",
              "std      8.601545   23.322453    6.860353  ...   91.294864    7.141062    9.197104\n",
              "min      0.006320    0.000000    0.460000  ...    0.320000    1.730000    5.000000\n",
              "25%      0.082045    0.000000    5.190000  ...  375.377500    6.950000   17.025000\n",
              "50%      0.256510    0.000000    9.690000  ...  391.440000   11.360000   21.200000\n",
              "75%      3.677083   12.500000   18.100000  ...  396.225000   16.955000   25.000000\n",
              "max     88.976200  100.000000   27.740000  ...  396.900000   37.970000   50.000000\n",
              "\n",
              "[8 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdn0DVSJal0r",
        "colab_type": "text"
      },
      "source": [
        "# 特徵縮放"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDoSwMxSYrQA",
        "colab_type": "code",
        "outputId": "92551589-0f6d-49b2-ddb4-5a5a2d530a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X = df.drop('Price',axis=1)\n",
        "y = df['Price']\n",
        "\n",
        "mm = MinMaxScaler().fit(X)\n",
        "X[:] = mm.transform(X[:])\n",
        "\n",
        "X.join(y).head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.067815</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.314815</td>\n",
              "      <td>0.577505</td>\n",
              "      <td>0.641607</td>\n",
              "      <td>0.269203</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.208015</td>\n",
              "      <td>0.287234</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.089680</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.242302</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.172840</td>\n",
              "      <td>0.547998</td>\n",
              "      <td>0.782698</td>\n",
              "      <td>0.348962</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.104962</td>\n",
              "      <td>0.553191</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.204470</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.242302</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.172840</td>\n",
              "      <td>0.694386</td>\n",
              "      <td>0.599382</td>\n",
              "      <td>0.348962</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.104962</td>\n",
              "      <td>0.553191</td>\n",
              "      <td>0.989737</td>\n",
              "      <td>0.063466</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.063050</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.150206</td>\n",
              "      <td>0.658555</td>\n",
              "      <td>0.441813</td>\n",
              "      <td>0.448545</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.066794</td>\n",
              "      <td>0.648936</td>\n",
              "      <td>0.994276</td>\n",
              "      <td>0.033389</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000705</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.063050</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.150206</td>\n",
              "      <td>0.687105</td>\n",
              "      <td>0.528321</td>\n",
              "      <td>0.448545</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.066794</td>\n",
              "      <td>0.648936</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.099338</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       CRIM    ZN     INDUS  CHAS  ...   PTRATIO         B     LSTAT  Price\n",
              "0  0.000000  0.18  0.067815   0.0  ...  0.287234  1.000000  0.089680   24.0\n",
              "1  0.000236  0.00  0.242302   0.0  ...  0.553191  1.000000  0.204470   21.6\n",
              "2  0.000236  0.00  0.242302   0.0  ...  0.553191  0.989737  0.063466   34.7\n",
              "3  0.000293  0.00  0.063050   0.0  ...  0.648936  0.994276  0.033389   33.4\n",
              "4  0.000705  0.00  0.063050   0.0  ...  0.648936  1.000000  0.099338   36.2\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBxtzJHCaqLR",
        "colab_type": "text"
      },
      "source": [
        "# 向量化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WcnghvtaqSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X.to_numpy()\n",
        "Y = y.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAFG2pK0a8RI",
        "colab_type": "text"
      },
      "source": [
        "# 切分資料集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Fa8vfpa8Zy",
        "colab_type": "code",
        "outputId": "1caf075d-c488-40d7-c11b-4e10264251f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(354, 13)\n",
            "(152, 13)\n",
            "(354,)\n",
            "(152,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGDrDrtpcc2Q",
        "colab_type": "text"
      },
      "source": [
        "# tensor化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2rH4HPzbH6Y",
        "colab_type": "code",
        "outputId": "a65e33d0-461d-406e-ae1a-930248c4f747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "n_train = X_train.shape[0]\n",
        "X_train = torch.tensor(X_train, dtype=torch.float)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float)\n",
        "Y_train = torch.tensor(Y_train, dtype=torch.float).view(-1, 1)\n",
        "Y_test = torch.tensor(Y_test, dtype=torch.float).view(-1, 1)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([354, 13])\n",
            "torch.Size([152, 13])\n",
            "torch.Size([354, 1])\n",
            "torch.Size([152, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYByjYaqbc_6",
        "colab_type": "text"
      },
      "source": [
        "# Construct the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHk8T-xgbdi4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "3f43b9cb-e9be-4b2d-acc5-f2c146061260"
      },
      "source": [
        "def build_net(input_shape,output_shape):\n",
        "  net = torch.nn.Sequential(\n",
        "      Linear(input_shape,256),ReLU(),\n",
        "      Linear(256,128),ReLU(),\n",
        "      Linear(128,64),ReLU(),\n",
        "      Linear(64,32),ReLU(),\n",
        "      Linear(32,16),ReLU(),\n",
        "      Linear(16,8),ReLU(),\n",
        "      Linear(8,output_shape)\n",
        "      )\n",
        "  return net\n",
        "\n",
        "def init_weights(m):\n",
        "  if type(m) == nn.Linear:\n",
        "    torch.nn.init.xavier_uniform(m.weight)\n",
        "    m.bias.data.fill_(0)\n",
        "\n",
        "net = build_net(X_train.shape[1],Y_train.shape[1])\n",
        "net.apply(init_weights)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=13, out_features=256, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (5): ReLU()\n",
              "  (6): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (7): ReLU()\n",
              "  (8): Linear(in_features=32, out_features=16, bias=True)\n",
              "  (9): ReLU()\n",
              "  (10): Linear(in_features=16, out_features=8, bias=True)\n",
              "  (11): ReLU()\n",
              "  (12): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ4Ppo4mblPx",
        "colab_type": "code",
        "outputId": "9845509b-8ad7-4271-b325-6de3d169971d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# test\n",
        "mini_batch = X_train[0:5]\n",
        "net(mini_batch)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0773],\n",
              "        [0.0837],\n",
              "        [0.0574],\n",
              "        [0.0703],\n",
              "        [0.0912]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuhVi3CYclLQ",
        "colab_type": "text"
      },
      "source": [
        "# The usage of DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mlvP7dIcnC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = torch.utils.data.TensorDataset(X_train, Y_train)\n",
        "train_iter = torch.utils.data.DataLoader(datasets, batch_size=128, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkPwnv4acyBJ",
        "colab_type": "text"
      },
      "source": [
        "# Loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c61KxMfcyTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(),lr=0.0002)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VW0iYGYYk5I",
        "colab_type": "text"
      },
      "source": [
        "# Training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8swXRzOVOEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net,train_iter,loss_function,optimizer,num_epochs=500):\n",
        "  history = []\n",
        "  for epoch in range(num_epochs):\n",
        "    for x,y in train_iter:\n",
        "      loss = loss_function(net(x),y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "    print(\"epochs {} loss {:.4f}\".format(epoch,loss.item()))\n",
        "    history.append(loss.item())\n",
        "  # plt train loss\n",
        "  plt.plot(np.array(history))\n",
        "  plt.title('train loss')\n",
        "  # return trained net\n",
        "  return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ouyGOHmGQK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1524750-78fd-46b9-89ee-395f5da3a163"
      },
      "source": [
        "net = train(net,train_iter,loss_function,optimizer,500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs 0 loss 566.4850\n",
            "epochs 1 loss 638.9546\n",
            "epochs 2 loss 521.2278\n",
            "epochs 3 loss 556.3463\n",
            "epochs 4 loss 608.0468\n",
            "epochs 5 loss 578.3714\n",
            "epochs 6 loss 555.1410\n",
            "epochs 7 loss 546.0974\n",
            "epochs 8 loss 542.8843\n",
            "epochs 9 loss 561.6533\n",
            "epochs 10 loss 572.2120\n",
            "epochs 11 loss 527.5114\n",
            "epochs 12 loss 526.5989\n",
            "epochs 13 loss 551.0486\n",
            "epochs 14 loss 480.0778\n",
            "epochs 15 loss 409.3257\n",
            "epochs 16 loss 547.6725\n",
            "epochs 17 loss 457.4003\n",
            "epochs 18 loss 438.8082\n",
            "epochs 19 loss 405.8566\n",
            "epochs 20 loss 409.5986\n",
            "epochs 21 loss 425.4425\n",
            "epochs 22 loss 343.0398\n",
            "epochs 23 loss 363.1266\n",
            "epochs 24 loss 303.9435\n",
            "epochs 25 loss 233.9698\n",
            "epochs 26 loss 259.3962\n",
            "epochs 27 loss 206.0773\n",
            "epochs 28 loss 193.2438\n",
            "epochs 29 loss 136.5106\n",
            "epochs 30 loss 122.1734\n",
            "epochs 31 loss 137.9547\n",
            "epochs 32 loss 139.4664\n",
            "epochs 33 loss 120.9392\n",
            "epochs 34 loss 146.6852\n",
            "epochs 35 loss 151.6649\n",
            "epochs 36 loss 138.9177\n",
            "epochs 37 loss 111.7829\n",
            "epochs 38 loss 85.2379\n",
            "epochs 39 loss 101.0807\n",
            "epochs 40 loss 120.2431\n",
            "epochs 41 loss 86.4979\n",
            "epochs 42 loss 99.9313\n",
            "epochs 43 loss 93.6782\n",
            "epochs 44 loss 87.2785\n",
            "epochs 45 loss 80.4272\n",
            "epochs 46 loss 92.9142\n",
            "epochs 47 loss 92.6488\n",
            "epochs 48 loss 78.7472\n",
            "epochs 49 loss 86.5706\n",
            "epochs 50 loss 77.5480\n",
            "epochs 51 loss 74.0373\n",
            "epochs 52 loss 74.1274\n",
            "epochs 53 loss 61.0599\n",
            "epochs 54 loss 65.3611\n",
            "epochs 55 loss 74.6253\n",
            "epochs 56 loss 42.6879\n",
            "epochs 57 loss 59.9068\n",
            "epochs 58 loss 47.8651\n",
            "epochs 59 loss 78.1342\n",
            "epochs 60 loss 71.7160\n",
            "epochs 61 loss 70.7944\n",
            "epochs 62 loss 53.5658\n",
            "epochs 63 loss 57.5506\n",
            "epochs 64 loss 51.1437\n",
            "epochs 65 loss 59.0924\n",
            "epochs 66 loss 52.9975\n",
            "epochs 67 loss 55.8406\n",
            "epochs 68 loss 52.0380\n",
            "epochs 69 loss 41.8914\n",
            "epochs 70 loss 37.6616\n",
            "epochs 71 loss 52.9081\n",
            "epochs 72 loss 47.2263\n",
            "epochs 73 loss 70.2357\n",
            "epochs 74 loss 41.9357\n",
            "epochs 75 loss 45.2643\n",
            "epochs 76 loss 37.9261\n",
            "epochs 77 loss 48.1808\n",
            "epochs 78 loss 39.4862\n",
            "epochs 79 loss 52.8727\n",
            "epochs 80 loss 33.1822\n",
            "epochs 81 loss 29.5265\n",
            "epochs 82 loss 44.6636\n",
            "epochs 83 loss 56.3258\n",
            "epochs 84 loss 62.4530\n",
            "epochs 85 loss 36.7668\n",
            "epochs 86 loss 46.7561\n",
            "epochs 87 loss 49.7996\n",
            "epochs 88 loss 45.7347\n",
            "epochs 89 loss 44.2551\n",
            "epochs 90 loss 38.3208\n",
            "epochs 91 loss 31.7841\n",
            "epochs 92 loss 41.7308\n",
            "epochs 93 loss 38.6443\n",
            "epochs 94 loss 22.0509\n",
            "epochs 95 loss 46.3975\n",
            "epochs 96 loss 47.6011\n",
            "epochs 97 loss 37.8234\n",
            "epochs 98 loss 47.2000\n",
            "epochs 99 loss 40.1704\n",
            "epochs 100 loss 40.6262\n",
            "epochs 101 loss 32.2664\n",
            "epochs 102 loss 30.1872\n",
            "epochs 103 loss 32.8928\n",
            "epochs 104 loss 44.7063\n",
            "epochs 105 loss 48.0949\n",
            "epochs 106 loss 29.5192\n",
            "epochs 107 loss 31.4869\n",
            "epochs 108 loss 37.3058\n",
            "epochs 109 loss 43.1832\n",
            "epochs 110 loss 27.9538\n",
            "epochs 111 loss 25.3024\n",
            "epochs 112 loss 25.6232\n",
            "epochs 113 loss 41.4011\n",
            "epochs 114 loss 24.9911\n",
            "epochs 115 loss 28.3329\n",
            "epochs 116 loss 24.3882\n",
            "epochs 117 loss 27.7798\n",
            "epochs 118 loss 18.7043\n",
            "epochs 119 loss 26.7450\n",
            "epochs 120 loss 24.4662\n",
            "epochs 121 loss 22.9111\n",
            "epochs 122 loss 20.4621\n",
            "epochs 123 loss 22.2359\n",
            "epochs 124 loss 16.5544\n",
            "epochs 125 loss 21.4152\n",
            "epochs 126 loss 33.0614\n",
            "epochs 127 loss 34.0222\n",
            "epochs 128 loss 23.0442\n",
            "epochs 129 loss 19.0562\n",
            "epochs 130 loss 20.7615\n",
            "epochs 131 loss 21.2649\n",
            "epochs 132 loss 24.7394\n",
            "epochs 133 loss 25.7318\n",
            "epochs 134 loss 26.4788\n",
            "epochs 135 loss 31.5717\n",
            "epochs 136 loss 29.1769\n",
            "epochs 137 loss 19.0263\n",
            "epochs 138 loss 27.9084\n",
            "epochs 139 loss 23.9973\n",
            "epochs 140 loss 16.1528\n",
            "epochs 141 loss 19.6023\n",
            "epochs 142 loss 20.8359\n",
            "epochs 143 loss 24.6898\n",
            "epochs 144 loss 17.8745\n",
            "epochs 145 loss 24.4904\n",
            "epochs 146 loss 15.6878\n",
            "epochs 147 loss 30.9807\n",
            "epochs 148 loss 16.3123\n",
            "epochs 149 loss 13.5279\n",
            "epochs 150 loss 21.7105\n",
            "epochs 151 loss 21.0578\n",
            "epochs 152 loss 19.6510\n",
            "epochs 153 loss 21.0220\n",
            "epochs 154 loss 24.6808\n",
            "epochs 155 loss 19.6015\n",
            "epochs 156 loss 21.0083\n",
            "epochs 157 loss 24.0158\n",
            "epochs 158 loss 24.9750\n",
            "epochs 159 loss 23.0445\n",
            "epochs 160 loss 27.2297\n",
            "epochs 161 loss 24.3126\n",
            "epochs 162 loss 11.7328\n",
            "epochs 163 loss 20.8328\n",
            "epochs 164 loss 15.5239\n",
            "epochs 165 loss 19.2066\n",
            "epochs 166 loss 12.4774\n",
            "epochs 167 loss 11.7349\n",
            "epochs 168 loss 21.5595\n",
            "epochs 169 loss 21.6223\n",
            "epochs 170 loss 23.4757\n",
            "epochs 171 loss 24.5801\n",
            "epochs 172 loss 19.9599\n",
            "epochs 173 loss 8.9506\n",
            "epochs 174 loss 14.4432\n",
            "epochs 175 loss 19.7784\n",
            "epochs 176 loss 11.4294\n",
            "epochs 177 loss 10.6540\n",
            "epochs 178 loss 17.2390\n",
            "epochs 179 loss 18.2382\n",
            "epochs 180 loss 11.6608\n",
            "epochs 181 loss 21.7751\n",
            "epochs 182 loss 13.4975\n",
            "epochs 183 loss 10.8289\n",
            "epochs 184 loss 11.7694\n",
            "epochs 185 loss 11.7823\n",
            "epochs 186 loss 21.1541\n",
            "epochs 187 loss 12.6075\n",
            "epochs 188 loss 9.9441\n",
            "epochs 189 loss 12.9521\n",
            "epochs 190 loss 9.9582\n",
            "epochs 191 loss 15.1128\n",
            "epochs 192 loss 10.8920\n",
            "epochs 193 loss 17.4177\n",
            "epochs 194 loss 18.7177\n",
            "epochs 195 loss 17.5631\n",
            "epochs 196 loss 17.3453\n",
            "epochs 197 loss 17.6919\n",
            "epochs 198 loss 15.9219\n",
            "epochs 199 loss 10.2776\n",
            "epochs 200 loss 14.2151\n",
            "epochs 201 loss 15.2867\n",
            "epochs 202 loss 11.9819\n",
            "epochs 203 loss 7.1876\n",
            "epochs 204 loss 19.8582\n",
            "epochs 205 loss 14.9241\n",
            "epochs 206 loss 13.1885\n",
            "epochs 207 loss 15.2048\n",
            "epochs 208 loss 15.6090\n",
            "epochs 209 loss 11.2596\n",
            "epochs 210 loss 14.8375\n",
            "epochs 211 loss 14.1824\n",
            "epochs 212 loss 9.8851\n",
            "epochs 213 loss 15.3199\n",
            "epochs 214 loss 20.0234\n",
            "epochs 215 loss 8.6038\n",
            "epochs 216 loss 9.5364\n",
            "epochs 217 loss 18.7929\n",
            "epochs 218 loss 13.7661\n",
            "epochs 219 loss 8.2090\n",
            "epochs 220 loss 15.0069\n",
            "epochs 221 loss 18.8356\n",
            "epochs 222 loss 21.2523\n",
            "epochs 223 loss 8.2059\n",
            "epochs 224 loss 8.4970\n",
            "epochs 225 loss 11.7824\n",
            "epochs 226 loss 18.6285\n",
            "epochs 227 loss 9.3239\n",
            "epochs 228 loss 11.1350\n",
            "epochs 229 loss 17.7361\n",
            "epochs 230 loss 10.6573\n",
            "epochs 231 loss 13.9485\n",
            "epochs 232 loss 12.6673\n",
            "epochs 233 loss 13.7248\n",
            "epochs 234 loss 11.8339\n",
            "epochs 235 loss 11.9832\n",
            "epochs 236 loss 12.4797\n",
            "epochs 237 loss 8.7769\n",
            "epochs 238 loss 11.7027\n",
            "epochs 239 loss 12.7827\n",
            "epochs 240 loss 13.2346\n",
            "epochs 241 loss 7.5222\n",
            "epochs 242 loss 11.7961\n",
            "epochs 243 loss 8.1009\n",
            "epochs 244 loss 7.7155\n",
            "epochs 245 loss 7.3348\n",
            "epochs 246 loss 9.9304\n",
            "epochs 247 loss 8.6576\n",
            "epochs 248 loss 7.9393\n",
            "epochs 249 loss 21.2486\n",
            "epochs 250 loss 17.0565\n",
            "epochs 251 loss 7.5783\n",
            "epochs 252 loss 18.8703\n",
            "epochs 253 loss 11.9255\n",
            "epochs 254 loss 9.9055\n",
            "epochs 255 loss 14.6552\n",
            "epochs 256 loss 12.0321\n",
            "epochs 257 loss 10.7813\n",
            "epochs 258 loss 17.1611\n",
            "epochs 259 loss 12.5302\n",
            "epochs 260 loss 15.5104\n",
            "epochs 261 loss 12.1379\n",
            "epochs 262 loss 13.6391\n",
            "epochs 263 loss 15.6405\n",
            "epochs 264 loss 15.0233\n",
            "epochs 265 loss 7.4916\n",
            "epochs 266 loss 13.1328\n",
            "epochs 267 loss 6.2341\n",
            "epochs 268 loss 7.9114\n",
            "epochs 269 loss 5.9796\n",
            "epochs 270 loss 15.2849\n",
            "epochs 271 loss 10.6093\n",
            "epochs 272 loss 6.9184\n",
            "epochs 273 loss 16.4499\n",
            "epochs 274 loss 15.6260\n",
            "epochs 275 loss 10.7398\n",
            "epochs 276 loss 10.2097\n",
            "epochs 277 loss 17.8953\n",
            "epochs 278 loss 9.0663\n",
            "epochs 279 loss 6.8514\n",
            "epochs 280 loss 15.1895\n",
            "epochs 281 loss 15.6057\n",
            "epochs 282 loss 5.5788\n",
            "epochs 283 loss 14.1465\n",
            "epochs 284 loss 8.6039\n",
            "epochs 285 loss 16.8227\n",
            "epochs 286 loss 6.5168\n",
            "epochs 287 loss 6.7420\n",
            "epochs 288 loss 7.1931\n",
            "epochs 289 loss 6.1077\n",
            "epochs 290 loss 9.8469\n",
            "epochs 291 loss 10.4587\n",
            "epochs 292 loss 15.2076\n",
            "epochs 293 loss 5.6628\n",
            "epochs 294 loss 8.2170\n",
            "epochs 295 loss 11.6372\n",
            "epochs 296 loss 11.6798\n",
            "epochs 297 loss 11.0546\n",
            "epochs 298 loss 9.8713\n",
            "epochs 299 loss 12.8638\n",
            "epochs 300 loss 12.4437\n",
            "epochs 301 loss 7.4661\n",
            "epochs 302 loss 9.2725\n",
            "epochs 303 loss 10.9959\n",
            "epochs 304 loss 7.7189\n",
            "epochs 305 loss 6.2967\n",
            "epochs 306 loss 15.5078\n",
            "epochs 307 loss 10.9674\n",
            "epochs 308 loss 9.3863\n",
            "epochs 309 loss 15.7051\n",
            "epochs 310 loss 16.2057\n",
            "epochs 311 loss 7.5183\n",
            "epochs 312 loss 9.3418\n",
            "epochs 313 loss 13.4378\n",
            "epochs 314 loss 10.6213\n",
            "epochs 315 loss 14.0128\n",
            "epochs 316 loss 8.4358\n",
            "epochs 317 loss 9.5083\n",
            "epochs 318 loss 8.8923\n",
            "epochs 319 loss 11.8212\n",
            "epochs 320 loss 6.8268\n",
            "epochs 321 loss 6.3063\n",
            "epochs 322 loss 9.0990\n",
            "epochs 323 loss 6.8158\n",
            "epochs 324 loss 6.8615\n",
            "epochs 325 loss 6.0731\n",
            "epochs 326 loss 10.5925\n",
            "epochs 327 loss 7.1712\n",
            "epochs 328 loss 12.1840\n",
            "epochs 329 loss 7.1876\n",
            "epochs 330 loss 11.3646\n",
            "epochs 331 loss 9.1049\n",
            "epochs 332 loss 6.4672\n",
            "epochs 333 loss 5.9907\n",
            "epochs 334 loss 11.6130\n",
            "epochs 335 loss 6.2794\n",
            "epochs 336 loss 8.6461\n",
            "epochs 337 loss 6.5260\n",
            "epochs 338 loss 15.3423\n",
            "epochs 339 loss 10.4025\n",
            "epochs 340 loss 5.3274\n",
            "epochs 341 loss 8.1852\n",
            "epochs 342 loss 6.5719\n",
            "epochs 343 loss 6.0185\n",
            "epochs 344 loss 6.7823\n",
            "epochs 345 loss 5.5112\n",
            "epochs 346 loss 5.3900\n",
            "epochs 347 loss 9.3341\n",
            "epochs 348 loss 13.7259\n",
            "epochs 349 loss 7.6056\n",
            "epochs 350 loss 6.1063\n",
            "epochs 351 loss 5.1959\n",
            "epochs 352 loss 11.0593\n",
            "epochs 353 loss 7.5728\n",
            "epochs 354 loss 7.1609\n",
            "epochs 355 loss 11.1796\n",
            "epochs 356 loss 11.1579\n",
            "epochs 357 loss 15.7599\n",
            "epochs 358 loss 13.7384\n",
            "epochs 359 loss 6.1356\n",
            "epochs 360 loss 6.1707\n",
            "epochs 361 loss 5.1546\n",
            "epochs 362 loss 7.2957\n",
            "epochs 363 loss 4.8361\n",
            "epochs 364 loss 8.4267\n",
            "epochs 365 loss 6.3802\n",
            "epochs 366 loss 11.4106\n",
            "epochs 367 loss 6.6460\n",
            "epochs 368 loss 8.8690\n",
            "epochs 369 loss 11.2767\n",
            "epochs 370 loss 12.0086\n",
            "epochs 371 loss 7.8362\n",
            "epochs 372 loss 7.2757\n",
            "epochs 373 loss 9.6589\n",
            "epochs 374 loss 10.2931\n",
            "epochs 375 loss 6.4358\n",
            "epochs 376 loss 5.7880\n",
            "epochs 377 loss 10.0716\n",
            "epochs 378 loss 6.1218\n",
            "epochs 379 loss 6.1224\n",
            "epochs 380 loss 4.5951\n",
            "epochs 381 loss 7.5152\n",
            "epochs 382 loss 9.3448\n",
            "epochs 383 loss 8.7133\n",
            "epochs 384 loss 10.7021\n",
            "epochs 385 loss 6.6137\n",
            "epochs 386 loss 11.9406\n",
            "epochs 387 loss 7.9533\n",
            "epochs 388 loss 6.0520\n",
            "epochs 389 loss 10.0321\n",
            "epochs 390 loss 6.0031\n",
            "epochs 391 loss 5.6112\n",
            "epochs 392 loss 10.0521\n",
            "epochs 393 loss 11.2008\n",
            "epochs 394 loss 12.1325\n",
            "epochs 395 loss 9.2618\n",
            "epochs 396 loss 7.3482\n",
            "epochs 397 loss 10.2734\n",
            "epochs 398 loss 8.8268\n",
            "epochs 399 loss 11.5583\n",
            "epochs 400 loss 6.3577\n",
            "epochs 401 loss 8.6040\n",
            "epochs 402 loss 9.6095\n",
            "epochs 403 loss 7.6530\n",
            "epochs 404 loss 11.2391\n",
            "epochs 405 loss 9.0941\n",
            "epochs 406 loss 5.7590\n",
            "epochs 407 loss 10.6090\n",
            "epochs 408 loss 5.4578\n",
            "epochs 409 loss 4.7149\n",
            "epochs 410 loss 10.6466\n",
            "epochs 411 loss 7.1678\n",
            "epochs 412 loss 8.3280\n",
            "epochs 413 loss 10.7600\n",
            "epochs 414 loss 7.0221\n",
            "epochs 415 loss 5.3481\n",
            "epochs 416 loss 5.4671\n",
            "epochs 417 loss 8.5302\n",
            "epochs 418 loss 10.4713\n",
            "epochs 419 loss 8.2455\n",
            "epochs 420 loss 9.0604\n",
            "epochs 421 loss 8.6390\n",
            "epochs 422 loss 6.9787\n",
            "epochs 423 loss 11.1197\n",
            "epochs 424 loss 7.5032\n",
            "epochs 425 loss 6.6868\n",
            "epochs 426 loss 5.9145\n",
            "epochs 427 loss 5.2649\n",
            "epochs 428 loss 6.4688\n",
            "epochs 429 loss 7.4393\n",
            "epochs 430 loss 9.0077\n",
            "epochs 431 loss 4.1970\n",
            "epochs 432 loss 7.0940\n",
            "epochs 433 loss 6.5033\n",
            "epochs 434 loss 7.2305\n",
            "epochs 435 loss 7.0544\n",
            "epochs 436 loss 5.1542\n",
            "epochs 437 loss 7.3630\n",
            "epochs 438 loss 7.6203\n",
            "epochs 439 loss 9.7220\n",
            "epochs 440 loss 6.9399\n",
            "epochs 441 loss 4.4862\n",
            "epochs 442 loss 7.8021\n",
            "epochs 443 loss 8.8558\n",
            "epochs 444 loss 8.3444\n",
            "epochs 445 loss 5.3249\n",
            "epochs 446 loss 9.1600\n",
            "epochs 447 loss 8.8087\n",
            "epochs 448 loss 11.3399\n",
            "epochs 449 loss 7.2921\n",
            "epochs 450 loss 4.4164\n",
            "epochs 451 loss 6.2571\n",
            "epochs 452 loss 7.4845\n",
            "epochs 453 loss 5.1430\n",
            "epochs 454 loss 8.7063\n",
            "epochs 455 loss 4.8170\n",
            "epochs 456 loss 6.0246\n",
            "epochs 457 loss 6.4605\n",
            "epochs 458 loss 5.0227\n",
            "epochs 459 loss 5.6557\n",
            "epochs 460 loss 4.7287\n",
            "epochs 461 loss 4.3460\n",
            "epochs 462 loss 8.9979\n",
            "epochs 463 loss 6.9872\n",
            "epochs 464 loss 7.0134\n",
            "epochs 465 loss 5.2669\n",
            "epochs 466 loss 5.6356\n",
            "epochs 467 loss 7.6964\n",
            "epochs 468 loss 4.8195\n",
            "epochs 469 loss 7.3702\n",
            "epochs 470 loss 4.1356\n",
            "epochs 471 loss 7.8863\n",
            "epochs 472 loss 4.6067\n",
            "epochs 473 loss 5.1368\n",
            "epochs 474 loss 7.0698\n",
            "epochs 475 loss 4.8614\n",
            "epochs 476 loss 3.7581\n",
            "epochs 477 loss 7.4934\n",
            "epochs 478 loss 5.9457\n",
            "epochs 479 loss 4.6727\n",
            "epochs 480 loss 6.2993\n",
            "epochs 481 loss 6.9277\n",
            "epochs 482 loss 6.1278\n",
            "epochs 483 loss 5.1268\n",
            "epochs 484 loss 5.4468\n",
            "epochs 485 loss 6.6150\n",
            "epochs 486 loss 4.5320\n",
            "epochs 487 loss 5.8895\n",
            "epochs 488 loss 9.7314\n",
            "epochs 489 loss 4.1571\n",
            "epochs 490 loss 6.9856\n",
            "epochs 491 loss 6.9112\n",
            "epochs 492 loss 4.3691\n",
            "epochs 493 loss 7.1247\n",
            "epochs 494 loss 5.8078\n",
            "epochs 495 loss 7.9036\n",
            "epochs 496 loss 3.8043\n",
            "epochs 497 loss 8.9793\n",
            "epochs 498 loss 6.6749\n",
            "epochs 499 loss 7.2120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMuO-7K8Q6kQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "665f1e67-f169-4bbd-971d-deac626d7934"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "y_pred = net(X_test).detach().numpy().ravel()\n",
        "y_real = Y_test.numpy().ravel()\n",
        "print(y_pred.shape,y_real.shape)\n",
        "r2_score(y_real,y_pred)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(152,) (152,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8573791007021546"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at4KLa8-S5d0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "price_model = net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgbDHZrwSugQ",
        "colab_type": "text"
      },
      "source": [
        "# find the what condition can make high price "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNUfBI6Cgw95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_net(input_shape,output_shape):\n",
        "  net = torch.nn.Sequential(\n",
        "      Linear(input_shape,128),\n",
        "      Linear(128,output_shape),\n",
        "      Sigmoid()\n",
        "      )\n",
        "  return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLdtrCcaWIHa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c835fe40-5a49-4ad0-d078-0dec649de2b2"
      },
      "source": [
        "net = build_net(10,X_train.shape[1])\n",
        "net.apply(init_weights)\n",
        "net"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=10, out_features=128, bias=True)\n",
              "  (1): Linear(in_features=128, out_features=13, bias=True)\n",
              "  (2): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At_LtRXdUGnz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a4575d3-175c-499a-9bb5-ffb0bbc9d5aa"
      },
      "source": [
        "noise = torch.tensor(np.random.uniform(size=(5000,10)), dtype=torch.float)\n",
        "noise_datasets = torch.utils.data.TensorDataset(noise)\n",
        "noise_iter = torch.utils.data.DataLoader(noise_datasets,batch_size=128)\n",
        "noise_iter"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7ff2f49e92e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhdjzY-vVjsZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed5e36d6-b31d-4689-ed38-3b07e3493a79"
      },
      "source": [
        "obj_function = lambda x:price_model(x).mean()\n",
        "obj_function"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.<lambda>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLR1Y1HoWJ3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(net.parameters(),lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pINm_CuwR6If",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train2(net,noise_iter,obj_function,optimizer,num_epochs=50):\n",
        "  history = []\n",
        "  for epoch in range(num_epochs):\n",
        "    for noise in noise_iter:\n",
        "      loss = obj_function(net(noise[0]))\n",
        "      loss *= -1\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "    print(\"epochs {} loss {:.4f}\".format(epoch,loss.item()))\n",
        "    history.append(loss.item())\n",
        "  # plt train loss\n",
        "  plt.plot(np.array(history))\n",
        "  plt.title('train loss')\n",
        "  # return trained net\n",
        "  return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHAMXXNdUFE1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b63b01cf-02b3-48a8-a329-f682bceb06ac"
      },
      "source": [
        "G = train2(net,noise_iter,obj_function,optimizer,num_epochs=50)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs 0 loss -10.4281\n",
            "epochs 1 loss -14.3161\n",
            "epochs 2 loss -18.8804\n",
            "epochs 3 loss -24.7828\n",
            "epochs 4 loss -33.6029\n",
            "epochs 5 loss -43.4978\n",
            "epochs 6 loss -52.4856\n",
            "epochs 7 loss -59.7848\n",
            "epochs 8 loss -65.2535\n",
            "epochs 9 loss -69.2601\n",
            "epochs 10 loss -72.2451\n",
            "epochs 11 loss -74.4361\n",
            "epochs 12 loss -76.0556\n",
            "epochs 13 loss -77.2416\n",
            "epochs 14 loss -78.1405\n",
            "epochs 15 loss -78.8416\n",
            "epochs 16 loss -79.3957\n",
            "epochs 17 loss -79.8395\n",
            "epochs 18 loss -80.2244\n",
            "epochs 19 loss -80.6086\n",
            "epochs 20 loss -81.3655\n",
            "epochs 21 loss -82.5103\n",
            "epochs 22 loss -83.3638\n",
            "epochs 23 loss -83.7390\n",
            "epochs 24 loss -83.9880\n",
            "epochs 25 loss -84.1770\n",
            "epochs 26 loss -84.3258\n",
            "epochs 27 loss -84.4467\n",
            "epochs 28 loss -84.5470\n",
            "epochs 29 loss -84.6315\n",
            "epochs 30 loss -84.7037\n",
            "epochs 31 loss -84.7660\n",
            "epochs 32 loss -84.8202\n",
            "epochs 33 loss -84.8678\n",
            "epochs 34 loss -84.9098\n",
            "epochs 35 loss -84.9472\n",
            "epochs 36 loss -84.9805\n",
            "epochs 37 loss -85.0104\n",
            "epochs 38 loss -85.0373\n",
            "epochs 39 loss -85.0617\n",
            "epochs 40 loss -85.0838\n",
            "epochs 41 loss -85.1039\n",
            "epochs 42 loss -85.1222\n",
            "epochs 43 loss -85.1390\n",
            "epochs 44 loss -85.1544\n",
            "epochs 45 loss -85.1685\n",
            "epochs 46 loss -85.1816\n",
            "epochs 47 loss -85.1936\n",
            "epochs 48 loss -85.2048\n",
            "epochs 49 loss -85.2151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEICAYAAAC6fYRZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niTCFvIhIhCnkRUQiTCEvIhJh/x/Vg546imPnFgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwzVXvCLYPAT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "99733558-80ea-4f78-85c2-be2490891bbc"
      },
      "source": [
        "a = pd.DataFrame(G(noise).detach().numpy(),columns = bos.feature_names)\n",
        "b = pd.Series(price_model(G(noise)).detach().numpy().ravel(),name='price')\n",
        "\n",
        "a[:] = mm.inverse_transform(a[:])\n",
        "res = a.join(b)\n",
        "res = round(res,2) \n",
        "res"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.45</td>\n",
              "      <td>99.470001</td>\n",
              "      <td>27.230000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.39</td>\n",
              "      <td>8.77</td>\n",
              "      <td>4.71</td>\n",
              "      <td>1.16</td>\n",
              "      <td>23.900000</td>\n",
              "      <td>190.139999</td>\n",
              "      <td>12.62</td>\n",
              "      <td>395.440002</td>\n",
              "      <td>1.78</td>\n",
              "      <td>84.790001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.08</td>\n",
              "      <td>99.849998</td>\n",
              "      <td>27.600000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.39</td>\n",
              "      <td>8.78</td>\n",
              "      <td>3.71</td>\n",
              "      <td>1.14</td>\n",
              "      <td>23.980000</td>\n",
              "      <td>187.949997</td>\n",
              "      <td>12.60</td>\n",
              "      <td>396.559998</td>\n",
              "      <td>1.74</td>\n",
              "      <td>85.269997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.15</td>\n",
              "      <td>99.669998</td>\n",
              "      <td>27.490000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.39</td>\n",
              "      <td>8.78</td>\n",
              "      <td>3.89</td>\n",
              "      <td>1.14</td>\n",
              "      <td>23.959999</td>\n",
              "      <td>189.029999</td>\n",
              "      <td>12.61</td>\n",
              "      <td>396.200012</td>\n",
              "      <td>1.76</td>\n",
              "      <td>85.120003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.26</td>\n",
              "      <td>99.510002</td>\n",
              "      <td>27.350000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.39</td>\n",
              "      <td>8.77</td>\n",
              "      <td>5.07</td>\n",
              "      <td>1.16</td>\n",
              "      <td>23.900000</td>\n",
              "      <td>190.009995</td>\n",
              "      <td>12.62</td>\n",
              "      <td>395.290009</td>\n",
              "      <td>1.79</td>\n",
              "      <td>84.860001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.20</td>\n",
              "      <td>99.489998</td>\n",
              "      <td>27.459999</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.39</td>\n",
              "      <td>8.78</td>\n",
              "      <td>4.74</td>\n",
              "      <td>1.15</td>\n",
              "      <td>23.950001</td>\n",
              "      <td>189.990005</td>\n",
              "      <td>12.61</td>\n",
              "      <td>395.839996</td>\n",
              "      <td>1.77</td>\n",
              "      <td>85.029999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>0.06</td>\n",
              "      <td>99.889999</td>\n",
              "      <td>27.620001</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.39</td>\n",
              "      <td>8.78</td>\n",
              "      <td>3.48</td>\n",
              "      <td>1.13</td>\n",
              "      <td>23.980000</td>\n",
              "      <td>187.770004</td>\n",
              "      <td>12.60</td>\n",
              "      <td>396.640015</td>\n",
              "      <td>1.74</td>\n",
              "      <td>85.290001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>0.10</td>\n",
              "      <td>99.849998</td>\n",
              "      <td>27.590000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.39</td>\n",
              "      <td>8.78</td>\n",
              "      <td>4.11</td>\n",
              "      <td>1.14</td>\n",
              "      <td>23.969999</td>\n",
              "      <td>188.050003</td>\n",
              "      <td>12.60</td>\n",
              "      <td>396.429993</td>\n",
              "      <td>1.75</td>\n",
              "      <td>85.230003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>0.04</td>\n",
              "      <td>99.930000</td>\n",
              "      <td>27.670000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.39</td>\n",
              "      <td>8.78</td>\n",
              "      <td>3.37</td>\n",
              "      <td>1.13</td>\n",
              "      <td>23.990000</td>\n",
              "      <td>187.429993</td>\n",
              "      <td>12.60</td>\n",
              "      <td>396.720001</td>\n",
              "      <td>1.74</td>\n",
              "      <td>85.330002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>0.11</td>\n",
              "      <td>99.830002</td>\n",
              "      <td>27.590000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.39</td>\n",
              "      <td>8.78</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1.14</td>\n",
              "      <td>23.969999</td>\n",
              "      <td>188.059998</td>\n",
              "      <td>12.60</td>\n",
              "      <td>396.500000</td>\n",
              "      <td>1.74</td>\n",
              "      <td>85.230003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>0.09</td>\n",
              "      <td>99.860001</td>\n",
              "      <td>27.580000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.39</td>\n",
              "      <td>8.78</td>\n",
              "      <td>3.98</td>\n",
              "      <td>1.14</td>\n",
              "      <td>23.980000</td>\n",
              "      <td>187.649994</td>\n",
              "      <td>12.60</td>\n",
              "      <td>396.519989</td>\n",
              "      <td>1.74</td>\n",
              "      <td>85.250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM         ZN      INDUS  CHAS  ...  PTRATIO           B  LSTAT      price\n",
              "0     0.45  99.470001  27.230000   1.0  ...    12.62  395.440002   1.78  84.790001\n",
              "1     0.08  99.849998  27.600000   1.0  ...    12.60  396.559998   1.74  85.269997\n",
              "2     0.15  99.669998  27.490000   1.0  ...    12.61  396.200012   1.76  85.120003\n",
              "3     0.26  99.510002  27.350000   1.0  ...    12.62  395.290009   1.79  84.860001\n",
              "4     0.20  99.489998  27.459999   1.0  ...    12.61  395.839996   1.77  85.029999\n",
              "...    ...        ...        ...   ...  ...      ...         ...    ...        ...\n",
              "4995  0.06  99.889999  27.620001   1.0  ...    12.60  396.640015   1.74  85.290001\n",
              "4996  0.10  99.849998  27.590000   1.0  ...    12.60  396.429993   1.75  85.230003\n",
              "4997  0.04  99.930000  27.670000   1.0  ...    12.60  396.720001   1.74  85.330002\n",
              "4998  0.11  99.830002  27.590000   1.0  ...    12.60  396.500000   1.74  85.230003\n",
              "4999  0.09  99.860001  27.580000   1.0  ...    12.60  396.519989   1.74  85.250000\n",
              "\n",
              "[5000 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flVYSX5dmcsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}