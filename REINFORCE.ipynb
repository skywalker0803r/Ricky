{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REINFORCE.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skywalker0803r/Ricky/blob/master/REINFORCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u8SzL5cvnY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install mxnet\n",
        "import sys\n",
        "import mxnet  \n",
        "import gym\n",
        "import numpy as np  \n",
        "from mxnet import nd,autograd,init\n",
        "from mxnet.gluon import nn,trainer\n",
        "import matplotlib.pyplot as plt\n",
        "from mxnet.gluon import Trainer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKCIv71r0-Wo",
        "colab_type": "text"
      },
      "source": [
        "# PolicyNetwork"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lRLrhZyvvIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PolicyNetwork(nn.Block):\n",
        "  def __init__(self,num_actions):\n",
        "    super(PolicyNetwork,self).__init__()\n",
        "    self.linear1 = nn.Dense(128)\n",
        "    self.linear2 = nn.Dense(num_actions)\n",
        "  def forward(self,x):\n",
        "    x = nd.array(x)\n",
        "    x = nd.relu(self.linear1(x))\n",
        "    x = nd.softmax(self.linear2(x))\n",
        "    return x\n",
        "  def get_action(self,x):\n",
        "    probs = self.forward(x)\n",
        "    action,log_prob = nd.random.multinomial(probs,get_prob=True)\n",
        "    return action,log_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3qdaFhHymCZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "90219767-57da-44b0-aae4-d1557ced83d3"
      },
      "source": [
        "#test\n",
        "p = PolicyNetwork(2)\n",
        "p.initialize(init=init.Xavier())\n",
        "s1 = np.array([1,2,3,4])\n",
        "s2 = np.array([2,3,4,5])\n",
        "batch_state = np.stack([s1,s2])\n",
        "print('===============')\n",
        "print(p.get_action([s1]))\n",
        "print('===============')\n",
        "print(p.get_action(batch_state))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===============\n",
            "(\n",
            "[1]\n",
            "<NDArray 1 @cpu(0)>, \n",
            "[-0.44218192]\n",
            "<NDArray 1 @cpu(0)>)\n",
            "===============\n",
            "(\n",
            "[1 1]\n",
            "<NDArray 2 @cpu(0)>, \n",
            "[-0.44218192 -0.36352086]\n",
            "<NDArray 2 @cpu(0)>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8j0CYet4aw0",
        "colab_type": "text"
      },
      "source": [
        "# discounted_rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kjbq0GD4a4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_returns(rewards,discount_factor=0.9):       \n",
        "  returns=[]\n",
        "  curr_sum = 0.\n",
        "  for r in reversed(rewards):\n",
        "      curr_sum = r + discount_factor*curr_sum\n",
        "      returns.append(curr_sum)\n",
        "  returns.reverse()\n",
        "  normalized_returns = nd.array(returns) - nd.mean(nd.array(returns))\n",
        "  return normalized_returns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnIiUKuC5pfK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4a5e15a7-c691-4697-bb2d-03ce92bf4835"
      },
      "source": [
        "#test\n",
        "get_returns([0,0,0,0,1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[-0.16292    -0.09002    -0.00901997  0.08098     0.18098003]\n",
              "<NDArray 5 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_Sotq9G4S56",
        "colab_type": "text"
      },
      "source": [
        "# update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCx2TtLj4TCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_gradient(rewards,log_probs):\n",
        "\n",
        "  policy_gradient = [] \n",
        "  \n",
        "  for log_prob,Gt in zip(log_probs,rewards):\n",
        "    policy_gradient.append(log_prob*(-Gt))\n",
        "  \n",
        "  return policy_gradient"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV5VFcv_1Knl",
        "colab_type": "text"
      },
      "source": [
        "# main loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oWV_Euh8BSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "policy_net = PolicyNetwork(env.action_space.n)\n",
        "policy_net.initialize(init=init.Xavier())\n",
        "trainer = Trainer(policy_net.collect_params(),'adam',{'learning_rate':0.001})\n",
        "\n",
        "max_episode_num = 5000\n",
        "max_steps = 10000\n",
        "numsteps = []\n",
        "avg_numsteps = []\n",
        "all_rewards = []\n",
        "\n",
        "for episode in range(max_episode_num):\n",
        "  state = env.reset()\n",
        "  log_probs = []\n",
        "  rewards = []\n",
        "  with autograd.record():\n",
        "    for t in range(max_steps):\n",
        "      state = nd.array(np.expand_dims(state, 0))\n",
        "      action, log_prob = policy_net.get_action(state)\n",
        "      state, reward, done, _ = env.step(action.asnumpy()[0])\n",
        "      log_probs.append(log_prob)\n",
        "      rewards.append(reward)\n",
        "      if done:\n",
        "        break\n",
        "      \n",
        "    # reverse accumulate and normalize rewards\n",
        "    R = 0\n",
        "    for i in range(len(rewards)-1, -1, -1):\n",
        "      R = rewards[i] + 0.9 * R\n",
        "      rewards[i] = R\n",
        "    rewards = np.array(rewards)\n",
        "    rewards -= rewards.mean()\n",
        "    rewards /= rewards.std() + np.finfo(rewards.dtype).eps\n",
        "      \n",
        "    # compute loss and gradient\n",
        "    policy_gradient = []\n",
        "    for log_prob,Gt in zip(log_probs,rewards):\n",
        "      policy_gradient.append(log_prob*(-Gt))\n",
        "    autograd.backward(policy_gradient)\n",
        "  trainer.step(t)\n",
        "  \n",
        "  print(episode,t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzDB9BVU9gKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}